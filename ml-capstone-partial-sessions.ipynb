{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Udacity ML Engineering Nanodegree - Capstone Project\n",
    "\n",
    "# Identifying attacker application sessions through supervised learning over known attacker sessions\n",
    "## Project Domain\n",
    "This project bears on the domain of security. I work for a company that publishes expensive content on the web, making it available to subscribers only. We frequently discover cases in which attackers have stolen credentials from our customers and use them to perform unauthorized content downloads. Detecting such activity quickly, without human analysis, has always been difficult given the large amounts of data involved.\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "Given a selection of log data containing records that describe different usage activities, determine whether a given session is likely to describe unauthorized content access. This project will use known attacker data as labeled examples supporting a supervised learning approach. I will use a supervised learning approach on labeled input data that includes both attacker sessions and “innocent” sessions, hoping to produce a model that can predict a high percentage of attack sessions with very low false positives. A successful proof of concept would identify 70% of attacker sessions with no more than 5% false positives.\n",
    "\n",
    "## Datasets and Inputs\n",
    "\n",
    "- known-attacker.txt–afilecontaininglogentriescollectedfromsessions manually identified as belonging to known attackers. Covering a span of around two years, this file contains ~60K attacker events.\n",
    "\n",
    "- mixed1-3.txt – files containing an unfiltered selection of time intervals known to contain attacker activities. These files contain a total of ~200K events, of which less than 0.5% are attacker events.\n",
    "\n",
    "The events in each file are recorded as tab-separated rows in the following columns:\n",
    "\n",
    "SessionNo LogTime CustID GroupID ProfID Act BadActor\n",
    "\n",
    "The ‘SessionNo’ column groups activities into sets of consecutive user actions, each of which is identified with an ‘Act’ column that identified what the user did, e.g., logged in, performed a search, downloaded content. The CustID, GroupID, and ProfID columns identify unique customer organizations, and are not expected to be useful in the learning exercise per se.\n",
    "\n",
    "Each file has been labeled manually with a notation whether each event belongs to an attacker session. The ‘BadActor’ column indicates whether the given event belongs to an attacker session.\n",
    "\n",
    "Given the low ratio of attacker to innocent events in the mixed files, I expect it will be necessary to augment the data to improve training results. [2016, Buczakak and Guven] suggests dropping negative rows or duplicating positive rows; I plan to augment training data using the long history of attacker-only rows in known-attacker.txt.\n",
    "\n",
    "## Solution Statement - further experiments\n",
    "\n",
    "After completing the sesssion-times experiment, I wanted to evaluate some approaches for measuring performance of the model against partial sessions. Partial-session processing is useful for live incident response; I want to be able to identify an attack in progress and block it, so I can't wait for a session to complete. In this notebook I explore several kinds of partial sessions - first N transactions, last N transactions, and first N% transactions. \n",
    "\n",
    "In addition, I wanted to improve the software I'm using to run these experiments. I wanted to be able to run many training jobs at the same time and start many endpoints consecutively; in addition, I wanted to get the Sagemaker boilerplate code out of the notebook so it's easy to see what's going on in a given experiment.\n",
    "\n",
    "## Prior research\n",
    "As far as I can see, the problem of identifying attackers from logs of activities at the business- use-case level has not been thoroughly researched. I did find numerous papers on analysis of net flows and HTTP logs that can be read for useful parallel techniques.\n",
    "\n",
    "Pietraszek, Tadeusz and Axel Tanner. “Data mining and machine learning - Towards reducing false positives in intrusion detection.” Inf. Sec. Techn. Report 10 (2005): 169-183. Uses machine learning to identify candidate alerts from an IDS for human labeling. Labels are used in supervised learning to refine selection of alerts.\n",
    "\n",
    "Buczak, Anna L. and Erhan Guven. “A Survey of Data Mining and Machine Learning Methods for Cyber Security Intrusion Detection.” IEEE Communications Surveys & Tutorials 18 (2016): 1153-1176. In the domain of IP net flow analysis, establishes terminology and reviews a broad selection of techniques for modeling attacks based on collections of internet packets.\n",
    "\n",
    "Sperotto, Anna et al. “An Overview of IP Flow-Based Intrusion Detection.” IEEE Communications Surveys & Tutorials 12 (2010): 343-356. In-depth discussion of net flow analysis, distinguishing parts of the net-flow modeling problem rather than how to analyze collections of successfully-captured packets.\n",
    "\n",
    "Moh, Melody et al. “Detecting Web Attacks Using Multi-stage Log Analysis.” 2016 IEEE 6th International Conference on Advanced Computing (IACC) (2016): 733-738. Overview of an approach for managing high volumes of HTTP logs and analyzing for presence of SQL injection attackes. Uses Bayes net classification in WEKA, produces an enriched analyst workstation environment in Kibana.\n",
    "\n",
    "## Benchmark Model & Evaluation Metrics\n",
    "The total volume of known attacker traffic is extremely low, less than .05% for a given victim. My approach to computing a baseline is to assume that *P(attack)* for a given row is 0. I will compare the confusion matrix for this baseline to the one for predictions from my ML model.\n",
    "\n",
    "## Project Design\n",
    "I will execute the following plan:\n",
    "\n",
    "1. Import the data into Jupyter & SageMaker in order to study it in place 2. Choose an approach for feature engineering:\n",
    "\n",
    "    a. Can I engineer a session row that contains enough information to produce a useful result in one of the algorithms I’ve already used, like XGBoost?\n",
    "\n",
    "    b. Do I need to use LSTM or convolution or some other learning algorithm with a memory that can learn sequences?\n",
    "\n",
    "3. Produce a repeatable process that can convert our raw log file into a dataset that my chosen algorithm can process\n",
    "\n",
    "4. Train a model, and execute the model against labelled data in batch transform mode 5. Use the result to calculate true and false positives and negatives. Given the small population of positive results – attacker session are a small fraction of total traffic – precision and recall are the most important metrics.\n",
    "\n",
    "    a. High precision means that my false positive rate is low. It’s critical not to misidentify innocent traffic as malicious.\n",
    "\n",
    "    b. High recall is the next most important metric. I need to identify as great a fraction of actual attack traffic as possible.\n",
    "\n",
    "The training set includes the following files:\n",
    "\n",
    "|File       |Rows                             |Contents|\n",
    "------------|-------------------------------------|----|\n",
    "|mixed-1.txt|119474|raw transactions|\n",
    "|mixed-2.txt|43608|raw transactions|\n",
    "|mixed-3.txt|30844|raw transactions|\n",
    "|known-attacker.txt|61917|raw transactions for a known attacker|\n",
    "\n",
    "All files include a 'BadActor' column that labels a transaction as belonging to a known attacker or not. The 'mixed-' files consist of whole hours of activity in which there is an attack, while known-attacker.txt contains all attacks for the known attacker over the last two years. In all, we have 193926 transactions from \"innocent\" sessions and 61917 LT transactions. Approximately 32% of transactions are labeled BadActor = 1, giving us a reasonable proportion in both classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence testing phase 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm up\n",
    "Import standard libraries and prepare the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘out’: File exists\n"
     ]
    }
   ],
   "source": [
    "# S3 bucket name\n",
    "bucket = 'sk-mlai-harvesting'\n",
    "\n",
    "# common column names\n",
    "bad_col='BadActor'\n",
    "sess_col='SessionNo'\n",
    "txn_col='Act'\n",
    "logtime_col = 'LogTime'\n",
    "\n",
    "# paths\n",
    "csv_path = \"out\"\n",
    "!mkdir out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: data/mixed-1.txt Rows: 119474\n",
      "File: data/mixed-2.txt Rows: 43608\n",
      "File: data/mixed-3.txt Rows: 30844\n",
      "File: data/known-attacker.txt Rows: 61917\n",
      "Total mixed transaction rows: 255843\n"
     ]
    }
   ],
   "source": [
    "mixed = []\n",
    "for i in range(3):\n",
    "    name = 'data/mixed-{}.txt'.format(i+1)\n",
    "    m = pd.read_csv(name,sep='\\t')\n",
    "    print( \"File: {} Rows: {}\".format( name, len(m)))\n",
    "    mixed.append(m)\n",
    "\n",
    "name = 'data/known-attacker.txt'\n",
    "known = pd.read_csv(name ,sep='\\t')\n",
    "print( \"File: {} Rows: {}\".format( name, len(known)))\n",
    "\n",
    "# Load data for extra day of test data - need to have full set of transaction types\n",
    "name = 'data/single.txt'\n",
    "single = pd.read_csv(name ,sep='\\t')\n",
    "single[txn_col]= single[txn_col].astype(str)\n",
    "\n",
    "mixed.append(known)\n",
    "\n",
    "txn = pd.concat(mixed)\n",
    "print( \"Total mixed transaction rows: {}\".format(len(txn)))\n",
    "\n",
    "txn[logtime_col] = pd.to_datetime(txn[logtime_col])\n",
    "txn[txn_col]= txn[txn_col].astype(str)\n",
    "# txn[txn[bad_col]==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to compute the full list of transaction types appearing in all of our data in order to make sure that our columns line up later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data conversion and feature engineering\n",
    "In real life, a session consists of a series of rows of transactions of different types, and each transaction type records a variable number of additional metadata attributes describing a logged event, for a total of over 30 columns of extracted data. In addition, our tagging process has given each row a BadActor label.\n",
    "\n",
    "|sessionno|txn id|BadActor|parm1|parm2|...|\n",
    "|---------|------|--------|-----|-----|---|\n",
    "|1240|111|0|query string|...|...|\n",
    "|1240|112|0|meta|...|...|\n",
    "|2993|301|1|meta|...|...|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We drop most of this information, including the temporal sequence of the log entries, and convert each session into a single row of data. Almost all of the columns go away, replaced by counts of transaction types in the session.\n",
    "\n",
    "|sessionno|BadActor|111|112|113|...|301|302|...|\n",
    "|---------|--------|---|---|---|---|---|---|---|\n",
    "|1240|0|1|1|0|...|0|0|...|\n",
    "|2993|1|0|0|0|...|1|0|...|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['111', '112', '114', '115', '116', '117', '118', '119', '121',\n",
       "       '123', '124', '125', '126', '127', '135', '201', '215', '216',\n",
       "       '217', '219', '311', '312', '315', '316', '317', '401', '402',\n",
       "       '403', '404', '406', '407', '410', '411', '511', '513', '601',\n",
       "       '607'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'Innocent' log entries\n",
    "txns = pd.DataFrame(np.sort(txn['Act'].unique()))\n",
    "\n",
    "# Harvesting log entries\n",
    "known_txns = pd.DataFrame(np.sort(known['Act'].unique()))\n",
    "\n",
    "all_txns=pd.concat([txn,single])\n",
    "\n",
    "all_txn_types = np.sort(all_txns[txn_col].unique())\n",
    "all_txn_types\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Truncating sessions\n",
    "Before we flatten the sessions, we're going to truncate them. This may be a better match for the real world, in which at best we will be able to scan sliding windows of transactions with a scaling resumption that we may not scan every event.\n",
    "\n",
    "We'll try several approaches at once:\n",
    "- dropping out D% of transactions from every session\n",
    "- taking only the first N sessions from every session\n",
    "- taking on the last N transactions from every session\n",
    "- taking N consecutive transactions from the middle of every session. \n",
    "- choosing N transactions as above, but dropping every session without at least N transactions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas.core.groupby.generic.DataFrameGroupBy object at 0x7f0cbabba630>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def drop_column_groups( txn_c ):\n",
    "    txn_c.columns = txn_c.columns.droplevel(0)  \n",
    "    txn_c.rename_axis(None, axis=1).reset_index()\n",
    "    return txn_c\n",
    "\n",
    "def get_session_groups( txn ):\n",
    "    txn_g = txn.groupby(sess_col)\n",
    "    return txn_g\n",
    "\n",
    "txng = get_session_groups(txn)\n",
    "txng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def drop_pct( df, n = .1 ):\n",
    "    return df.sample(frac= 1-n)\n",
    "\n",
    "def first_n( df, n = 5):\n",
    "    return df.head( n )\n",
    "    \n",
    "def last_n( df, n = 5):\n",
    "    return df.tail( n )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grappl( fun, parm ):\n",
    "    return lambda df: df.apply(fun, parm).drop(sess_col, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_missing_columns( df_target, source_cols):\n",
    "    '''\n",
    "    If there are columns in the list 'source' that are not in the DataFrame 'target',\n",
    "    add new columns to 'target' that are populated with 0.0.\n",
    "    '''\n",
    "    df = df_target\n",
    "    target_cols = df_target.columns\n",
    "    missing_cols = set(source_cols) - set(target_cols)\n",
    "    \n",
    "    print( \"Missing columns: {}\".format(missing_cols)) \n",
    "    if 0 < len(missing_cols):\n",
    "        new_cols = dict([(col,0.0) for col in missing_cols])\n",
    "        df = df_target.assign(**new_cols)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_txns( txn_log ):\n",
    "    '''\n",
    "    On a flat list of sessions, run a pivot table on transaction type counts by session, and eliminate extraneous columns.\n",
    "    Flatten the pivot table and simplify the index.\n",
    "    '''\n",
    "    txn_narrow = txn_log[[sess_col, txn_col,bad_col]]\n",
    "    txn_pivot = pd.pivot_table(txn_narrow, index=[sess_col,bad_col], columns = [txn_col],aggfunc=[len]).fillna(0)\n",
    "    txn_pivot.columns = txn_pivot.columns.droplevel(0)           # the pivot table has a two-level index\n",
    "    txn_flat = txn_pivot.rename_axis(None, axis=1).reset_index() # these two lines get rid of it so we have a simple table\n",
    "\n",
    "    txn_full = add_missing_columns(txn_flat, all_txn_types)\n",
    "    txn_xs = txn_full.drop(columns=[sess_col,bad_col])\n",
    "    txn_xs = txn_xs.reindex(sorted(txn_xs.columns), axis=1)\n",
    "\n",
    "    txn_sort = pd.concat([txn_full[[sess_col,bad_col]], txn_xs],sort=True,axis=1)\n",
    "    \n",
    "    return txn_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_groups( txn_log ):\n",
    "    '''\n",
    "    On a set of session groups, run a pivot table on transaction type counts by session, and eliminate extraneous columns.\n",
    "    Flatten all groups into one table and simplify the index.\n",
    "    '''\n",
    "    txn_narrow = txn_log[[txn_col,bad_col]] # for groups, don't need to drop the session column because it's already an index column.\n",
    "    txn_pivot = pd.pivot_table(txn_narrow, index=[sess_col,bad_col], columns = [txn_col],aggfunc=[len]).fillna(0)\n",
    "    txn_pivot.columns = txn_pivot.columns.droplevel(0)           # the pivot table has a two-level index\n",
    "    txn_flat = txn_pivot.rename_axis(None, axis=1).reset_index() # these two lines get rid of it so we have a simple table\n",
    "    txn_full = add_missing_columns(txn_flat, all_txn_types)\n",
    "    txn_xs = txn_full.drop(columns=[sess_col,bad_col])\n",
    "\n",
    "    txn_xs = txn_xs.reindex(sorted(txn_xs.columns), axis=1)\n",
    "#     txn_xs = txn_xs.drop('index',axis=1)\n",
    "\n",
    "    txn_sort = pd.concat([txn_full[[sess_col,bad_col]], txn_xs],sort=True,axis=1)\n",
    "\n",
    "    return txn_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('drop0', <function __main__.drop_pct(df, n=0.1)>, 0.0),\n",
       " ('drop10', <function __main__.drop_pct(df, n=0.1)>, 0.2),\n",
       " ('first5', <function __main__.first_n(df, n=5)>, 5),\n",
       " ('first10', <function __main__.first_n(df, n=5)>, 10),\n",
       " ('last5', <function __main__.last_n(df, n=5)>, 5),\n",
       " ('last10', <function __main__.last_n(df, n=5)>, 10)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs = [['drop0', [drop_pct, 0.0]], # control - keep the whole session, just to make sure that everything is working\n",
    "    ['drop10', [drop_pct, .2]], \n",
    "    ['first5', [first_n, 5]],\n",
    "    ['first10', [first_n, 10]],\n",
    "    ['last5', [last_n, 5]],\n",
    "    ['last10', [last_n, 10]]\n",
    "       ]\n",
    "job_names = [job[0] for job in jobs]\n",
    "[(name, func, parm) for [name, [func, parm]] in jobs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_jobs(df, jobs):\n",
    "    groups = [[name, grappl(fun, parm)(df)] for [name, [fun, parm]] in jobs]\n",
    "\n",
    "    flats = [[name,flatten_groups( txn )] for [name,txn] in groups] #.reset_index()\n",
    "    return flats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing columns: {'312'}\n",
      "Missing columns: {'312'}\n",
      "Missing columns: {'607', '312', '601', '402'}\n",
      "Missing columns: {'607', '312', '601', '402'}\n",
      "Missing columns: {'312', '402'}\n",
      "Missing columns: {'312', '402'}\n",
      "CPU times: user 3min 31s, sys: 835 ms, total: 3min 32s\n",
      "Wall time: 3min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "gs = prep_jobs(txng, jobs )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SessionNo</td>\n",
       "      <td>SessionNo</td>\n",
       "      <td>SessionNo</td>\n",
       "      <td>SessionNo</td>\n",
       "      <td>SessionNo</td>\n",
       "      <td>SessionNo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BadActor</td>\n",
       "      <td>BadActor</td>\n",
       "      <td>BadActor</td>\n",
       "      <td>BadActor</td>\n",
       "      <td>BadActor</td>\n",
       "      <td>BadActor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>111</td>\n",
       "      <td>111</td>\n",
       "      <td>111</td>\n",
       "      <td>111</td>\n",
       "      <td>111</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>112</td>\n",
       "      <td>112</td>\n",
       "      <td>112</td>\n",
       "      <td>112</td>\n",
       "      <td>112</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>114</td>\n",
       "      <td>114</td>\n",
       "      <td>114</td>\n",
       "      <td>114</td>\n",
       "      <td>114</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>116</td>\n",
       "      <td>116</td>\n",
       "      <td>116</td>\n",
       "      <td>116</td>\n",
       "      <td>116</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>118</td>\n",
       "      <td>118</td>\n",
       "      <td>118</td>\n",
       "      <td>118</td>\n",
       "      <td>118</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>119</td>\n",
       "      <td>119</td>\n",
       "      <td>119</td>\n",
       "      <td>119</td>\n",
       "      <td>119</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>121</td>\n",
       "      <td>121</td>\n",
       "      <td>121</td>\n",
       "      <td>121</td>\n",
       "      <td>121</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>123</td>\n",
       "      <td>123</td>\n",
       "      <td>123</td>\n",
       "      <td>123</td>\n",
       "      <td>123</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>124</td>\n",
       "      <td>124</td>\n",
       "      <td>124</td>\n",
       "      <td>124</td>\n",
       "      <td>124</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>125</td>\n",
       "      <td>125</td>\n",
       "      <td>125</td>\n",
       "      <td>125</td>\n",
       "      <td>125</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>126</td>\n",
       "      <td>126</td>\n",
       "      <td>126</td>\n",
       "      <td>126</td>\n",
       "      <td>126</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>127</td>\n",
       "      <td>127</td>\n",
       "      <td>127</td>\n",
       "      <td>127</td>\n",
       "      <td>127</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>135</td>\n",
       "      <td>135</td>\n",
       "      <td>135</td>\n",
       "      <td>135</td>\n",
       "      <td>135</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>215</td>\n",
       "      <td>215</td>\n",
       "      <td>215</td>\n",
       "      <td>215</td>\n",
       "      <td>215</td>\n",
       "      <td>215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>216</td>\n",
       "      <td>216</td>\n",
       "      <td>216</td>\n",
       "      <td>216</td>\n",
       "      <td>216</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>217</td>\n",
       "      <td>217</td>\n",
       "      <td>217</td>\n",
       "      <td>217</td>\n",
       "      <td>217</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>219</td>\n",
       "      <td>219</td>\n",
       "      <td>219</td>\n",
       "      <td>219</td>\n",
       "      <td>219</td>\n",
       "      <td>219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>311</td>\n",
       "      <td>311</td>\n",
       "      <td>311</td>\n",
       "      <td>311</td>\n",
       "      <td>311</td>\n",
       "      <td>311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>312</td>\n",
       "      <td>312</td>\n",
       "      <td>312</td>\n",
       "      <td>312</td>\n",
       "      <td>312</td>\n",
       "      <td>312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>315</td>\n",
       "      <td>315</td>\n",
       "      <td>315</td>\n",
       "      <td>315</td>\n",
       "      <td>315</td>\n",
       "      <td>315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>316</td>\n",
       "      <td>316</td>\n",
       "      <td>316</td>\n",
       "      <td>316</td>\n",
       "      <td>316</td>\n",
       "      <td>316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>317</td>\n",
       "      <td>317</td>\n",
       "      <td>317</td>\n",
       "      <td>317</td>\n",
       "      <td>317</td>\n",
       "      <td>317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>401</td>\n",
       "      <td>401</td>\n",
       "      <td>401</td>\n",
       "      <td>401</td>\n",
       "      <td>401</td>\n",
       "      <td>401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>402</td>\n",
       "      <td>402</td>\n",
       "      <td>402</td>\n",
       "      <td>402</td>\n",
       "      <td>402</td>\n",
       "      <td>402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>403</td>\n",
       "      <td>403</td>\n",
       "      <td>403</td>\n",
       "      <td>403</td>\n",
       "      <td>403</td>\n",
       "      <td>403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>404</td>\n",
       "      <td>404</td>\n",
       "      <td>404</td>\n",
       "      <td>404</td>\n",
       "      <td>404</td>\n",
       "      <td>404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>406</td>\n",
       "      <td>406</td>\n",
       "      <td>406</td>\n",
       "      <td>406</td>\n",
       "      <td>406</td>\n",
       "      <td>406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>407</td>\n",
       "      <td>407</td>\n",
       "      <td>407</td>\n",
       "      <td>407</td>\n",
       "      <td>407</td>\n",
       "      <td>407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>410</td>\n",
       "      <td>410</td>\n",
       "      <td>410</td>\n",
       "      <td>410</td>\n",
       "      <td>410</td>\n",
       "      <td>410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>411</td>\n",
       "      <td>411</td>\n",
       "      <td>411</td>\n",
       "      <td>411</td>\n",
       "      <td>411</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>511</td>\n",
       "      <td>511</td>\n",
       "      <td>511</td>\n",
       "      <td>511</td>\n",
       "      <td>511</td>\n",
       "      <td>511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>513</td>\n",
       "      <td>513</td>\n",
       "      <td>513</td>\n",
       "      <td>513</td>\n",
       "      <td>513</td>\n",
       "      <td>513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>601</td>\n",
       "      <td>601</td>\n",
       "      <td>601</td>\n",
       "      <td>601</td>\n",
       "      <td>601</td>\n",
       "      <td>601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>607</td>\n",
       "      <td>607</td>\n",
       "      <td>607</td>\n",
       "      <td>607</td>\n",
       "      <td>607</td>\n",
       "      <td>607</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0          1          2          3          4          5\n",
       "0   SessionNo  SessionNo  SessionNo  SessionNo  SessionNo  SessionNo\n",
       "1    BadActor   BadActor   BadActor   BadActor   BadActor   BadActor\n",
       "2         111        111        111        111        111        111\n",
       "3         112        112        112        112        112        112\n",
       "4         114        114        114        114        114        114\n",
       "5         115        115        115        115        115        115\n",
       "6         116        116        116        116        116        116\n",
       "7         117        117        117        117        117        117\n",
       "8         118        118        118        118        118        118\n",
       "9         119        119        119        119        119        119\n",
       "10        121        121        121        121        121        121\n",
       "11        123        123        123        123        123        123\n",
       "12        124        124        124        124        124        124\n",
       "13        125        125        125        125        125        125\n",
       "14        126        126        126        126        126        126\n",
       "15        127        127        127        127        127        127\n",
       "16        135        135        135        135        135        135\n",
       "17        201        201        201        201        201        201\n",
       "18        215        215        215        215        215        215\n",
       "19        216        216        216        216        216        216\n",
       "20        217        217        217        217        217        217\n",
       "21        219        219        219        219        219        219\n",
       "22        311        311        311        311        311        311\n",
       "23        312        312        312        312        312        312\n",
       "24        315        315        315        315        315        315\n",
       "25        316        316        316        316        316        316\n",
       "26        317        317        317        317        317        317\n",
       "27        401        401        401        401        401        401\n",
       "28        402        402        402        402        402        402\n",
       "29        403        403        403        403        403        403\n",
       "30        404        404        404        404        404        404\n",
       "31        406        406        406        406        406        406\n",
       "32        407        407        407        407        407        407\n",
       "33        410        410        410        410        410        410\n",
       "34        411        411        411        411        411        411\n",
       "35        511        511        511        511        511        511\n",
       "36        513        513        513        513        513        513\n",
       "37        601        601        601        601        601        601\n",
       "38        607        607        607        607        607        607"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([(g[1].columns) for g in gs]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('drop0', 24114, 39),\n",
       " ('drop10', 24114, 39),\n",
       " ('first5', 24112, 39),\n",
       " ('first10', 24112, 39),\n",
       " ('last5', 24112, 39),\n",
       " ('last10', 24113, 39)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txngs= gs\n",
    "[(name,len(df), len(df.columns)) for [name,df] in txngs]\n",
    "# gs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Producing pools of training and testing data\n",
    "\n",
    "In order to support simultaneous execution of multiple jobs, this notebook introduces a new scheme for piping data through to models.\n",
    "\n",
    "The normal flow runs as follows:\n",
    "\n",
    "input data(s3) -> df's on notebook instance -> train.csv, test.csv, validate.csv on notebook instance -> s3/out/ -> Sagemaker instances\n",
    "\n",
    "Hardcoding these filenames is fine for playing around in a notebook, but it limits us to one job at a time.\n",
    "\n",
    "In this approach, every job has a base Name. This name will carry through from S3 into the Sagemaker instances.\n",
    "Training data files will reside in `<s3bucket>/<key>/out/Name`.\n",
    "\n",
    "As before, in each `Name` subfolder, we will divide the combined good and bad data pools as follows:\n",
    "- a training set that the model iterates over during the learning process\n",
    "- a test set that is used to evaluate the model during training\n",
    "- a validation set that is kept separate to test the model after training is complete. We need separate test and validate pools in order to make sure that we're not overfitting the model to a single set of test data.\n",
    "\n",
    "All of these functions are in the FrameSplitter class. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'lib.JupHelper.JupHelper' from '/home/ec2-user/SageMaker/ml-udacity/lib/JupHelper/JupHelper.py'>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import lib.JupHelper.JupHelper as jh\n",
    "reload(jh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib.JupHelper.JupHelper as jh\n",
    "\n",
    "csv = jh.FrameSplitter( bad_col, [sess_col]) # FrameSplitter only holds onto the definition of the y_col and the x_cols - everything else is passed in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_csvs = csv.make_all_csvs( gs )           # Drives the whole conversion process - look in the class for other helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvs = csv.get_all_csv_names(gs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload to S3 and move into SageMaker\n",
    "\n",
    "Move all of the current csv's up into S3 for SageMaker, then start configuring jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'lib.JupHelper.JupHelper' from '/home/ec2-user/SageMaker/ml-udacity/lib/JupHelper/JupHelper.py'>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import lib.JupHelper.JupHelper as jh\n",
    "reload(jh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "import boto3\n",
    "from time import gmtime, strftime\n",
    "import time\n",
    "\n",
    "class XgbHyper:\n",
    "    def __init__(self):\n",
    "        self.params = {\n",
    "            \"max_depth\":\"5\",\n",
    "            \"eta\":\"0.2\",\n",
    "            \"gamma\":\"4\",\n",
    "            \"min_child_weight\":\"6\",\n",
    "            \"subsample\":\"0.7\",\n",
    "            \"silent\":\"0\",\n",
    "            \"objective\":\"binary:logistic\",\n",
    "            \"num_round\":\"50\"\n",
    "        }\n",
    "\n",
    "class TrainRunner:\n",
    "    def __init__( self, prefix, region='us-east-1' ):\n",
    "        '''\n",
    "        Wrapper object to run multiple \n",
    "        '''\n",
    "        self.prefix = prefix\n",
    "        self.region = region\n",
    "\n",
    "        self.running_jobs = []\n",
    "        self.models = []\n",
    "        self.sg_client = boto3.client('sagemaker', region_name=region )\n",
    "        # sagemaker session, role\n",
    "        self.sagemaker_session = sagemaker.Session()\n",
    "        self.role = sagemaker.get_execution_role()\n",
    "    \n",
    "    def start_jobs(self, jobs, bucket ):\n",
    "        for (name, csvs) in jobs:\n",
    "            job_name = self.make_job_name( name )\n",
    "            print( job_name )\n",
    "            self.start_training_job( bucket, job_name, csvs )\n",
    "            self.running_jobs.append([name,job_name])\n",
    "        return self.running_jobs\n",
    "    \n",
    "    def clear_jobs(self):\n",
    "        self.running_jobs = []\n",
    "        self.models = []\n",
    "        \n",
    "        #delete endpoints\n",
    "        #delete configs\n",
    "    \n",
    "    def start_training_job(self, bucket, job_name, s3_inputs, image='xgboost', instance = [1, \"ml.m4.4xlarge\", 5 ], hyper=XgbHyper(), verbose=True):\n",
    "        self.image = image\n",
    "        self.s3_input = s3_inputs\n",
    "        self.instance = instance\n",
    "        self.hyper = hyper\n",
    "        self.bucket = bucket\n",
    "\n",
    "        self.container = get_image_uri( self.region, 'xgboost' )\n",
    "        job_config = self.make_job_config( job_name, s3_inputs[0], s3_inputs[1], s3_inputs[2])\n",
    "        res = self.launch_training_job( job_config )\n",
    "        if verbose:\n",
    "            print( \"Started {}. Response: {}\".format( job_name, res))\n",
    "            \n",
    "        return res\n",
    "    \n",
    "    def make_s3_url(self, file):\n",
    "        return \"s3://{}/{}\".format(self.bucket, file)\n",
    "    \n",
    "    def get_job_status(self, job):\n",
    "        [name, job_name] = job\n",
    "        return self.sg_client.describe_training_job(TrainingJobName=job_name)['TrainingJobStatus']\n",
    "    \n",
    "    def check_jobs_still_running( self ):\n",
    "        still_running = False\n",
    "        for job in self.running_jobs:\n",
    "            status = self.get_job_status( job )\n",
    "            if status !='Completed' and status !='Failed':\n",
    "                still_running = True\n",
    "                break\n",
    "        \n",
    "        return still_running\n",
    "    \n",
    "    def map_jobs( self, func ):\n",
    "        for job in self.running_jobs:\n",
    "            func( job )\n",
    "    \n",
    "    def print_job_status( self, job ):\n",
    "        [name, job_name] = job\n",
    "        \n",
    "        print(\"{}: {}\".format(job, self.get_job_status(job) )  )\n",
    "        \n",
    "    def trace_jobs( self ):\n",
    "        self.map_jobs( self.print_job_status )\n",
    "                    \n",
    "    def make_job_name( self, job_name ):\n",
    "        name = \"{}-{}-{}\".format( self.prefix, job_name, strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime()))\n",
    "        return name\n",
    "            \n",
    "    def launch_training_job( self, job_config ):\n",
    "        self.sg_client.create_training_job( **job_config )\n",
    "    \n",
    "    def wait_for_jobs( self ):\n",
    "        while True:\n",
    "            print(\"Checking job statuses:\")\n",
    "            self.trace_jobs()\n",
    "            if not self.check_jobs_still_running():\n",
    "                break\n",
    "            time.sleep(15)\n",
    "\n",
    "    def get_model_name(self, name):\n",
    "        return name + \"-model\"\n",
    "    \n",
    "    def get_endpoint_config_name(self, name):\n",
    "        return name + \"-config\"\n",
    "     \n",
    "    def get_endpoint_name(self, name):\n",
    "        return name + \"-endpoint\"\n",
    "\n",
    "    def create_models( self ):\n",
    "        for [name, job_name] in self.running_jobs:\n",
    "            info = self.sg_client.describe_training_job(TrainingJobName=job_name)\n",
    "            model_data = info['ModelArtifacts']['S3ModelArtifacts']\n",
    "            primary_container = {\n",
    "                'Image': self.container,\n",
    "                'ModelDataUrl': model_data\n",
    "            }\n",
    "            res = self.sg_client.create_model(\n",
    "                ModelName = self. get_model_name(job_name),\n",
    "                ExecutionRoleArn = self.role,\n",
    "                PrimaryContainer = primary_container)\n",
    "            print(res['ModelArn'])\n",
    "            self.models.append( res )\n",
    "            \n",
    "    def create_endpoint_configs(self, job_name ):\n",
    "        endpoint_config_name = self.get_endpoint_config_name(job_name)\n",
    "        model_name = self.get_model_name( job_name )\n",
    "        print(endpoint_config_name)\n",
    "        res = self.sg_client.create_endpoint_config(\n",
    "            EndpointConfigName = endpoint_config_name,\n",
    "            ProductionVariants=[{\n",
    "                'InstanceType':'ml.m4.xlarge',\n",
    "                'InitialVariantWeight':1,\n",
    "                'InitialInstanceCount':1,\n",
    "                'ModelName':model_name,\n",
    "                'VariantName':'AllTraffic'}])\n",
    "        print(\"Endpoint Config Arn: \" + res['EndpointConfigArn'])\n",
    "        \n",
    "    def create_endpoints(self):\n",
    "        for [name, job_name] in self.running_jobs:\n",
    "            self.create_endpoint_configs(job_name)\n",
    "            endpoint_name = self.get_endpoint_name(job_name)\n",
    "            print(endpoint_name)\n",
    "            res = self.sg_client.create_endpoint(\n",
    "                EndpointName=endpoint_name,\n",
    "                EndpointConfigName=self.get_endpoint_config_name( job_name ))\n",
    "            print(res['EndpointArn'])\n",
    "\n",
    "    def wait_for_endpoints(self):\n",
    "        still_creating = True\n",
    "            \n",
    "        while still_creating:\n",
    "            still_creating = True\n",
    "\n",
    "            print(\"Checking endpoint statuses:\")\n",
    "\n",
    "            for [name, job_name] in self.running_jobs:\n",
    "                resp = self.sg_client.describe_endpoint(EndpointName=self.get_endpoint_name( job_name ))\n",
    "                status = resp['EndpointStatus']\n",
    "                print( \"Endpoint {}: {}\".format(self.get_endpoint_name( job_name ), status ))\n",
    "                if status == 'InService':\n",
    "                    still_creating = False\n",
    "                    \n",
    "            time.sleep(30)\n",
    "        \n",
    "        print( \"All endpoints created.\")\n",
    "        \n",
    "    def test_model(self, job_name, csv):\n",
    "        '''\n",
    "        job_name - name of a job that has run all the way through to an endpoint\n",
    "        csv - a csv with the same y_col and x_col structure as the training data\n",
    "        returns - a single-column dataframe with the predictions from the model.\n",
    "        '''\n",
    "        endpoint_name = self.get_endpoint_name( job_name )\n",
    "        with open(csv, 'r') as f:\n",
    "            payload = f.read().strip()\n",
    "        response = self.sg_client.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                   ContentType='text/csv', \n",
    "                                   Body=payload)\n",
    "        result = response['Body'].read()\n",
    "        result = result.decode(\"utf-8\")\n",
    "        result = result.split(',')\n",
    "        result = [round(float(i)) for i in result]\n",
    "        return pd.DataFrame( result )\n",
    "            \n",
    "    def make_job_config(self, job_name, train, test, val ):\n",
    "        return {\n",
    "        \"AlgorithmSpecification\": {\n",
    "            \"TrainingImage\": self.container,\n",
    "            \"TrainingInputMode\": \"File\"\n",
    "        },\n",
    "        \"RoleArn\": self.role,\n",
    "        \"OutputDataConfig\": {\n",
    "            \"S3OutputPath\": os.path.join(\"s3://\", self.bucket, \"out\", \"xgb-class\") \n",
    "        },\n",
    "        \"ResourceConfig\": {\n",
    "            \"InstanceCount\": self.instance[0],\n",
    "            \"InstanceType\": self.instance[1],\n",
    "            \"VolumeSizeInGB\": self.instance[2]\n",
    "        },\n",
    "        \"TrainingJobName\": job_name,\n",
    "        \"HyperParameters\": self.hyper.params,        \n",
    "        \"StoppingCondition\": {\n",
    "            \"MaxRuntimeInSeconds\": 3600\n",
    "        },\n",
    "        \"InputDataConfig\": [\n",
    "            {\n",
    "                \"ChannelName\": \"train\",\n",
    "                \"DataSource\": {\n",
    "                    \"S3DataSource\": {\n",
    "                        \"S3DataType\": \"S3Prefix\",\n",
    "                        \"S3Uri\": self.make_s3_url( train ), # \"s3://sagemaker-mlai-harvesting/out/train.csv\" , \n",
    "                        \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                    }\n",
    "                },\n",
    "                \"ContentType\": \"text/csv\",\n",
    "                \"CompressionType\": \"None\"\n",
    "            },\n",
    "            {\n",
    "                \"ChannelName\": \"test\",\n",
    "                \"DataSource\": {\n",
    "                    \"S3DataSource\": {\n",
    "                        \"S3DataType\": \"S3Prefix\",\n",
    "                        \"S3Uri\": self.make_s3_url( test ), # \"s3://sagemaker-mlai-harvesting/out/test.csv\" , \n",
    "                        \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                    }\n",
    "                },\n",
    "                \"ContentType\": \"text/csv\",\n",
    "                \"CompressionType\": \"None\"\n",
    "            },\n",
    "            {\n",
    "                \"ChannelName\": \"validation\",\n",
    "                \"DataSource\": {\n",
    "                    \"S3DataSource\": {\n",
    "                        \"S3DataType\": \"S3Prefix\",\n",
    "                        \"S3Uri\": self.make_s3_url( val ), # \"s3://sagemaker-mlai-harvesting/out/validate.csv\" ,\n",
    "                        \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                    }\n",
    "                },\n",
    "                \"ContentType\": \"text/csv\",\n",
    "                \"CompressionType\": \"None\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "class SageHelper:\n",
    "    def __init__(self, bucket, image, region='us-east-1'):\n",
    "        self.bucket = bucket\n",
    "        self.image = image\n",
    "        self.container = get_image_uri(region, self.image)\n",
    "        self.s3_client = boto3.client('s3')\n",
    "    \n",
    "    def s3_upload(self, file, s3_path=\"\", verbose = True):\n",
    "        target = os.path.join(s3_path, file) \n",
    "        if verbose:\n",
    "            print( \"Uploading {} to s3://{}/{}\".format(file, bucket, target))\n",
    "        response = self.s3_client.upload_file( file, bucket, target )\n",
    "        if verbose:\n",
    "            print( response )\n",
    "        return response\n",
    "    \n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sh = SageHelper( bucket, 'xgboost' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading files for drop0\n",
      "Uploading out/drop0_train.csv to s3://sk-mlai-harvesting/out/drop0_train.csv\n",
      "None\n",
      "Uploading out/drop0_test.csv to s3://sk-mlai-harvesting/out/drop0_test.csv\n",
      "None\n",
      "Uploading out/drop0_validate.csv to s3://sk-mlai-harvesting/out/drop0_validate.csv\n",
      "None\n",
      "Uploading files for drop10\n",
      "Uploading out/drop10_train.csv to s3://sk-mlai-harvesting/out/drop10_train.csv\n",
      "None\n",
      "Uploading out/drop10_test.csv to s3://sk-mlai-harvesting/out/drop10_test.csv\n",
      "None\n",
      "Uploading out/drop10_validate.csv to s3://sk-mlai-harvesting/out/drop10_validate.csv\n",
      "None\n",
      "Uploading files for first5\n",
      "Uploading out/first5_train.csv to s3://sk-mlai-harvesting/out/first5_train.csv\n",
      "None\n",
      "Uploading out/first5_test.csv to s3://sk-mlai-harvesting/out/first5_test.csv\n",
      "None\n",
      "Uploading out/first5_validate.csv to s3://sk-mlai-harvesting/out/first5_validate.csv\n",
      "None\n",
      "Uploading files for first10\n",
      "Uploading out/first10_train.csv to s3://sk-mlai-harvesting/out/first10_train.csv\n",
      "None\n",
      "Uploading out/first10_test.csv to s3://sk-mlai-harvesting/out/first10_test.csv\n",
      "None\n",
      "Uploading out/first10_validate.csv to s3://sk-mlai-harvesting/out/first10_validate.csv\n",
      "None\n",
      "Uploading files for last5\n",
      "Uploading out/last5_train.csv to s3://sk-mlai-harvesting/out/last5_train.csv\n",
      "None\n",
      "Uploading out/last5_test.csv to s3://sk-mlai-harvesting/out/last5_test.csv\n",
      "None\n",
      "Uploading out/last5_validate.csv to s3://sk-mlai-harvesting/out/last5_validate.csv\n",
      "None\n",
      "Uploading files for last10\n",
      "Uploading out/last10_train.csv to s3://sk-mlai-harvesting/out/last10_train.csv\n",
      "None\n",
      "Uploading out/last10_test.csv to s3://sk-mlai-harvesting/out/last10_test.csv\n",
      "None\n",
      "Uploading out/last10_validate.csv to s3://sk-mlai-harvesting/out/last10_validate.csv\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for (job, csvs ) in job_csvs:\n",
    "    print( \"Uploading files for {}\".format(job))\n",
    "    for file in csvs:\n",
    "        sh.s3_upload( file )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare and train a model\n",
    "Boilerplate code mostly copied from Amazon sample code at https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/xgboost_abalone/xgboost_abalone.ipynb, with ample room for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr= TrainRunner(\"harvest-xgb-bc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "harvest-xgb-bc-drop0-2019-07-17-02-54-00\n",
      "Started harvest-xgb-bc-drop0-2019-07-17-02-54-00. Response: None\n",
      "harvest-xgb-bc-drop10-2019-07-17-02-54-00\n",
      "Started harvest-xgb-bc-drop10-2019-07-17-02-54-00. Response: None\n",
      "harvest-xgb-bc-first5-2019-07-17-02-54-01\n",
      "Started harvest-xgb-bc-first5-2019-07-17-02-54-01. Response: None\n",
      "harvest-xgb-bc-first10-2019-07-17-02-54-06\n",
      "Started harvest-xgb-bc-first10-2019-07-17-02-54-06. Response: None\n",
      "harvest-xgb-bc-last5-2019-07-17-02-54-08\n",
      "Started harvest-xgb-bc-last5-2019-07-17-02-54-08. Response: None\n",
      "harvest-xgb-bc-last10-2019-07-17-02-54-11\n",
      "Started harvest-xgb-bc-last10-2019-07-17-02-54-11. Response: None\n",
      "Checking job statuses:\n",
      "['drop0', 'harvest-xgb-bc-drop0-2019-07-17-02-54-00']: InProgress\n",
      "['drop10', 'harvest-xgb-bc-drop10-2019-07-17-02-54-00']: InProgress\n",
      "['first5', 'harvest-xgb-bc-first5-2019-07-17-02-54-01']: InProgress\n",
      "['first10', 'harvest-xgb-bc-first10-2019-07-17-02-54-06']: InProgress\n",
      "['last5', 'harvest-xgb-bc-last5-2019-07-17-02-54-08']: InProgress\n",
      "['last10', 'harvest-xgb-bc-last10-2019-07-17-02-54-11']: InProgress\n",
      "Checking job statuses:\n",
      "['drop0', 'harvest-xgb-bc-drop0-2019-07-17-02-54-00']: InProgress\n",
      "['drop10', 'harvest-xgb-bc-drop10-2019-07-17-02-54-00']: InProgress\n",
      "['first5', 'harvest-xgb-bc-first5-2019-07-17-02-54-01']: InProgress\n",
      "['first10', 'harvest-xgb-bc-first10-2019-07-17-02-54-06']: InProgress\n",
      "['last5', 'harvest-xgb-bc-last5-2019-07-17-02-54-08']: InProgress\n",
      "['last10', 'harvest-xgb-bc-last10-2019-07-17-02-54-11']: InProgress\n",
      "Checking job statuses:\n",
      "['drop0', 'harvest-xgb-bc-drop0-2019-07-17-02-54-00']: InProgress\n",
      "['drop10', 'harvest-xgb-bc-drop10-2019-07-17-02-54-00']: InProgress\n",
      "['first5', 'harvest-xgb-bc-first5-2019-07-17-02-54-01']: InProgress\n",
      "['first10', 'harvest-xgb-bc-first10-2019-07-17-02-54-06']: InProgress\n",
      "['last5', 'harvest-xgb-bc-last5-2019-07-17-02-54-08']: InProgress\n",
      "['last10', 'harvest-xgb-bc-last10-2019-07-17-02-54-11']: InProgress\n",
      "Checking job statuses:\n",
      "['drop0', 'harvest-xgb-bc-drop0-2019-07-17-02-54-00']: InProgress\n",
      "['drop10', 'harvest-xgb-bc-drop10-2019-07-17-02-54-00']: InProgress\n",
      "['first5', 'harvest-xgb-bc-first5-2019-07-17-02-54-01']: InProgress\n",
      "['first10', 'harvest-xgb-bc-first10-2019-07-17-02-54-06']: InProgress\n",
      "['last5', 'harvest-xgb-bc-last5-2019-07-17-02-54-08']: InProgress\n",
      "['last10', 'harvest-xgb-bc-last10-2019-07-17-02-54-11']: InProgress\n",
      "Checking job statuses:\n",
      "['drop0', 'harvest-xgb-bc-drop0-2019-07-17-02-54-00']: InProgress\n",
      "['drop10', 'harvest-xgb-bc-drop10-2019-07-17-02-54-00']: InProgress\n",
      "['first5', 'harvest-xgb-bc-first5-2019-07-17-02-54-01']: InProgress\n",
      "['first10', 'harvest-xgb-bc-first10-2019-07-17-02-54-06']: InProgress\n",
      "['last5', 'harvest-xgb-bc-last5-2019-07-17-02-54-08']: InProgress\n",
      "['last10', 'harvest-xgb-bc-last10-2019-07-17-02-54-11']: InProgress\n",
      "Checking job statuses:\n",
      "['drop0', 'harvest-xgb-bc-drop0-2019-07-17-02-54-00']: InProgress\n",
      "['drop10', 'harvest-xgb-bc-drop10-2019-07-17-02-54-00']: InProgress\n",
      "['first5', 'harvest-xgb-bc-first5-2019-07-17-02-54-01']: InProgress\n",
      "['first10', 'harvest-xgb-bc-first10-2019-07-17-02-54-06']: InProgress\n",
      "['last5', 'harvest-xgb-bc-last5-2019-07-17-02-54-08']: InProgress\n",
      "['last10', 'harvest-xgb-bc-last10-2019-07-17-02-54-11']: InProgress\n",
      "Checking job statuses:\n",
      "['drop0', 'harvest-xgb-bc-drop0-2019-07-17-02-54-00']: InProgress\n",
      "['drop10', 'harvest-xgb-bc-drop10-2019-07-17-02-54-00']: InProgress\n",
      "['first5', 'harvest-xgb-bc-first5-2019-07-17-02-54-01']: InProgress\n",
      "['first10', 'harvest-xgb-bc-first10-2019-07-17-02-54-06']: InProgress\n",
      "['last5', 'harvest-xgb-bc-last5-2019-07-17-02-54-08']: InProgress\n",
      "['last10', 'harvest-xgb-bc-last10-2019-07-17-02-54-11']: InProgress\n",
      "Checking job statuses:\n",
      "['drop0', 'harvest-xgb-bc-drop0-2019-07-17-02-54-00']: InProgress\n",
      "['drop10', 'harvest-xgb-bc-drop10-2019-07-17-02-54-00']: InProgress\n",
      "['first5', 'harvest-xgb-bc-first5-2019-07-17-02-54-01']: InProgress\n",
      "['first10', 'harvest-xgb-bc-first10-2019-07-17-02-54-06']: InProgress\n",
      "['last5', 'harvest-xgb-bc-last5-2019-07-17-02-54-08']: InProgress\n",
      "['last10', 'harvest-xgb-bc-last10-2019-07-17-02-54-11']: InProgress\n",
      "Checking job statuses:\n",
      "['drop0', 'harvest-xgb-bc-drop0-2019-07-17-02-54-00']: InProgress\n",
      "['drop10', 'harvest-xgb-bc-drop10-2019-07-17-02-54-00']: InProgress\n",
      "['first5', 'harvest-xgb-bc-first5-2019-07-17-02-54-01']: InProgress\n",
      "['first10', 'harvest-xgb-bc-first10-2019-07-17-02-54-06']: InProgress\n",
      "['last5', 'harvest-xgb-bc-last5-2019-07-17-02-54-08']: InProgress\n",
      "['last10', 'harvest-xgb-bc-last10-2019-07-17-02-54-11']: InProgress\n",
      "Checking job statuses:\n",
      "['drop0', 'harvest-xgb-bc-drop0-2019-07-17-02-54-00']: InProgress\n",
      "['drop10', 'harvest-xgb-bc-drop10-2019-07-17-02-54-00']: InProgress\n",
      "['first5', 'harvest-xgb-bc-first5-2019-07-17-02-54-01']: InProgress\n",
      "['first10', 'harvest-xgb-bc-first10-2019-07-17-02-54-06']: InProgress\n",
      "['last5', 'harvest-xgb-bc-last5-2019-07-17-02-54-08']: InProgress\n",
      "['last10', 'harvest-xgb-bc-last10-2019-07-17-02-54-11']: InProgress\n",
      "Checking job statuses:\n",
      "['drop0', 'harvest-xgb-bc-drop0-2019-07-17-02-54-00']: Completed\n",
      "['drop10', 'harvest-xgb-bc-drop10-2019-07-17-02-54-00']: InProgress\n",
      "['first5', 'harvest-xgb-bc-first5-2019-07-17-02-54-01']: Completed\n",
      "['first10', 'harvest-xgb-bc-first10-2019-07-17-02-54-06']: Completed\n",
      "['last5', 'harvest-xgb-bc-last5-2019-07-17-02-54-08']: Completed\n",
      "['last10', 'harvest-xgb-bc-last10-2019-07-17-02-54-11']: InProgress\n",
      "Checking job statuses:\n",
      "['drop0', 'harvest-xgb-bc-drop0-2019-07-17-02-54-00']: Completed\n",
      "['drop10', 'harvest-xgb-bc-drop10-2019-07-17-02-54-00']: Completed\n",
      "['first5', 'harvest-xgb-bc-first5-2019-07-17-02-54-01']: Completed\n",
      "['first10', 'harvest-xgb-bc-first10-2019-07-17-02-54-06']: Completed\n",
      "['last5', 'harvest-xgb-bc-last5-2019-07-17-02-54-08']: Completed\n",
      "['last10', 'harvest-xgb-bc-last10-2019-07-17-02-54-11']: InProgress\n",
      "Checking job statuses:\n",
      "['drop0', 'harvest-xgb-bc-drop0-2019-07-17-02-54-00']: Completed\n",
      "['drop10', 'harvest-xgb-bc-drop10-2019-07-17-02-54-00']: Completed\n",
      "['first5', 'harvest-xgb-bc-first5-2019-07-17-02-54-01']: Completed\n",
      "['first10', 'harvest-xgb-bc-first10-2019-07-17-02-54-06']: Completed\n",
      "['last5', 'harvest-xgb-bc-last5-2019-07-17-02-54-08']: Completed\n",
      "['last10', 'harvest-xgb-bc-last10-2019-07-17-02-54-11']: InProgress\n",
      "Checking job statuses:\n",
      "['drop0', 'harvest-xgb-bc-drop0-2019-07-17-02-54-00']: Completed\n",
      "['drop10', 'harvest-xgb-bc-drop10-2019-07-17-02-54-00']: Completed\n",
      "['first5', 'harvest-xgb-bc-first5-2019-07-17-02-54-01']: Completed\n",
      "['first10', 'harvest-xgb-bc-first10-2019-07-17-02-54-06']: Completed\n",
      "['last5', 'harvest-xgb-bc-last5-2019-07-17-02-54-08']: Completed\n",
      "['last10', 'harvest-xgb-bc-last10-2019-07-17-02-54-11']: Completed\n",
      "CPU times: user 722 ms, sys: 12.3 ms, total: 735 ms\n",
      "Wall time: 3min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "running_jobs = tr.start_jobs(job_csvs, bucket)\n",
    "\n",
    "tr.wait_for_jobs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launching endpoints for trained models.\n",
    "\n",
    "In a straight-through pipeline, launch endpoints, run tests, collect output, and shut endpoints down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:sagemaker:us-east-1:617644144259:model/harvest-xgb-bc-drop0-2019-07-17-02-54-00-model\n",
      "arn:aws:sagemaker:us-east-1:617644144259:model/harvest-xgb-bc-drop10-2019-07-17-02-54-00-model\n",
      "arn:aws:sagemaker:us-east-1:617644144259:model/harvest-xgb-bc-first5-2019-07-17-02-54-01-model\n",
      "arn:aws:sagemaker:us-east-1:617644144259:model/harvest-xgb-bc-first10-2019-07-17-02-54-06-model\n",
      "arn:aws:sagemaker:us-east-1:617644144259:model/harvest-xgb-bc-last5-2019-07-17-02-54-08-model\n",
      "arn:aws:sagemaker:us-east-1:617644144259:model/harvest-xgb-bc-last10-2019-07-17-02-54-11-model\n"
     ]
    }
   ],
   "source": [
    "tr.create_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "harvest-xgb-bc-drop0-2019-07-17-02-54-00-config\n",
      "Endpoint Config Arn: arn:aws:sagemaker:us-east-1:617644144259:endpoint-config/harvest-xgb-bc-drop0-2019-07-17-02-54-00-config\n",
      "harvest-xgb-bc-drop0-2019-07-17-02-54-00-endpoint\n",
      "arn:aws:sagemaker:us-east-1:617644144259:endpoint/harvest-xgb-bc-drop0-2019-07-17-02-54-00-endpoint\n",
      "harvest-xgb-bc-drop10-2019-07-17-02-54-00-config\n",
      "Endpoint Config Arn: arn:aws:sagemaker:us-east-1:617644144259:endpoint-config/harvest-xgb-bc-drop10-2019-07-17-02-54-00-config\n",
      "harvest-xgb-bc-drop10-2019-07-17-02-54-00-endpoint\n",
      "arn:aws:sagemaker:us-east-1:617644144259:endpoint/harvest-xgb-bc-drop10-2019-07-17-02-54-00-endpoint\n",
      "harvest-xgb-bc-first5-2019-07-17-02-54-01-config\n",
      "Endpoint Config Arn: arn:aws:sagemaker:us-east-1:617644144259:endpoint-config/harvest-xgb-bc-first5-2019-07-17-02-54-01-config\n",
      "harvest-xgb-bc-first5-2019-07-17-02-54-01-endpoint\n",
      "arn:aws:sagemaker:us-east-1:617644144259:endpoint/harvest-xgb-bc-first5-2019-07-17-02-54-01-endpoint\n",
      "harvest-xgb-bc-first10-2019-07-17-02-54-06-config\n",
      "Endpoint Config Arn: arn:aws:sagemaker:us-east-1:617644144259:endpoint-config/harvest-xgb-bc-first10-2019-07-17-02-54-06-config\n",
      "harvest-xgb-bc-first10-2019-07-17-02-54-06-endpoint\n",
      "arn:aws:sagemaker:us-east-1:617644144259:endpoint/harvest-xgb-bc-first10-2019-07-17-02-54-06-endpoint\n",
      "harvest-xgb-bc-last5-2019-07-17-02-54-08-config\n",
      "Endpoint Config Arn: arn:aws:sagemaker:us-east-1:617644144259:endpoint-config/harvest-xgb-bc-last5-2019-07-17-02-54-08-config\n",
      "harvest-xgb-bc-last5-2019-07-17-02-54-08-endpoint\n",
      "arn:aws:sagemaker:us-east-1:617644144259:endpoint/harvest-xgb-bc-last5-2019-07-17-02-54-08-endpoint\n",
      "harvest-xgb-bc-last10-2019-07-17-02-54-11-config\n",
      "Endpoint Config Arn: arn:aws:sagemaker:us-east-1:617644144259:endpoint-config/harvest-xgb-bc-last10-2019-07-17-02-54-11-config\n",
      "harvest-xgb-bc-last10-2019-07-17-02-54-11-endpoint\n",
      "arn:aws:sagemaker:us-east-1:617644144259:endpoint/harvest-xgb-bc-last10-2019-07-17-02-54-11-endpoint\n"
     ]
    }
   ],
   "source": [
    "tr.create_endpoints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking endpoint statuses:\n",
      "Endpoint harvest-xgb-bc-drop0-2019-07-17-02-54-00-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-drop10-2019-07-17-02-54-00-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-first5-2019-07-17-02-54-01-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-first10-2019-07-17-02-54-06-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-last5-2019-07-17-02-54-08-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-last10-2019-07-17-02-54-11-endpoint: Creating\n",
      "Checking endpoint statuses:\n",
      "Endpoint harvest-xgb-bc-drop0-2019-07-17-02-54-00-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-drop10-2019-07-17-02-54-00-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-first5-2019-07-17-02-54-01-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-first10-2019-07-17-02-54-06-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-last5-2019-07-17-02-54-08-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-last10-2019-07-17-02-54-11-endpoint: Creating\n",
      "Checking endpoint statuses:\n",
      "Endpoint harvest-xgb-bc-drop0-2019-07-17-02-54-00-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-drop10-2019-07-17-02-54-00-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-first5-2019-07-17-02-54-01-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-first10-2019-07-17-02-54-06-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-last5-2019-07-17-02-54-08-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-last10-2019-07-17-02-54-11-endpoint: Creating\n",
      "Checking endpoint statuses:\n",
      "Endpoint harvest-xgb-bc-drop0-2019-07-17-02-54-00-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-drop10-2019-07-17-02-54-00-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-first5-2019-07-17-02-54-01-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-first10-2019-07-17-02-54-06-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-last5-2019-07-17-02-54-08-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-last10-2019-07-17-02-54-11-endpoint: Creating\n",
      "Checking endpoint statuses:\n",
      "Endpoint harvest-xgb-bc-drop0-2019-07-17-02-54-00-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-drop10-2019-07-17-02-54-00-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-first5-2019-07-17-02-54-01-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-first10-2019-07-17-02-54-06-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-last5-2019-07-17-02-54-08-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-last10-2019-07-17-02-54-11-endpoint: Creating\n",
      "Checking endpoint statuses:\n",
      "Endpoint harvest-xgb-bc-drop0-2019-07-17-02-54-00-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-drop10-2019-07-17-02-54-00-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-first5-2019-07-17-02-54-01-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-first10-2019-07-17-02-54-06-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-last5-2019-07-17-02-54-08-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-last10-2019-07-17-02-54-11-endpoint: Creating\n",
      "Checking endpoint statuses:\n",
      "Endpoint harvest-xgb-bc-drop0-2019-07-17-02-54-00-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-drop10-2019-07-17-02-54-00-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-first5-2019-07-17-02-54-01-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-first10-2019-07-17-02-54-06-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-last5-2019-07-17-02-54-08-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-last10-2019-07-17-02-54-11-endpoint: Creating\n",
      "Checking endpoint statuses:\n",
      "Endpoint harvest-xgb-bc-drop0-2019-07-17-02-54-00-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-drop10-2019-07-17-02-54-00-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-first5-2019-07-17-02-54-01-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-first10-2019-07-17-02-54-06-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-last5-2019-07-17-02-54-08-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-last10-2019-07-17-02-54-11-endpoint: Creating\n",
      "Checking endpoint statuses:\n",
      "Endpoint harvest-xgb-bc-drop0-2019-07-17-02-54-00-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-drop10-2019-07-17-02-54-00-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-first5-2019-07-17-02-54-01-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-first10-2019-07-17-02-54-06-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-last5-2019-07-17-02-54-08-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-last10-2019-07-17-02-54-11-endpoint: Creating\n",
      "Checking endpoint statuses:\n",
      "Endpoint harvest-xgb-bc-drop0-2019-07-17-02-54-00-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-drop10-2019-07-17-02-54-00-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-first5-2019-07-17-02-54-01-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-first10-2019-07-17-02-54-06-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-last5-2019-07-17-02-54-08-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-last10-2019-07-17-02-54-11-endpoint: Creating\n",
      "Checking endpoint statuses:\n",
      "Endpoint harvest-xgb-bc-drop0-2019-07-17-02-54-00-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-drop10-2019-07-17-02-54-00-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-first5-2019-07-17-02-54-01-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-first10-2019-07-17-02-54-06-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-last5-2019-07-17-02-54-08-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-last10-2019-07-17-02-54-11-endpoint: Creating\n",
      "Checking endpoint statuses:\n",
      "Endpoint harvest-xgb-bc-drop0-2019-07-17-02-54-00-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-drop10-2019-07-17-02-54-00-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-first5-2019-07-17-02-54-01-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-first10-2019-07-17-02-54-06-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-last5-2019-07-17-02-54-08-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-last10-2019-07-17-02-54-11-endpoint: Creating\n",
      "Checking endpoint statuses:\n",
      "Endpoint harvest-xgb-bc-drop0-2019-07-17-02-54-00-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-drop10-2019-07-17-02-54-00-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-first5-2019-07-17-02-54-01-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-first10-2019-07-17-02-54-06-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-last5-2019-07-17-02-54-08-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-last10-2019-07-17-02-54-11-endpoint: Creating\n",
      "Checking endpoint statuses:\n",
      "Endpoint harvest-xgb-bc-drop0-2019-07-17-02-54-00-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-drop10-2019-07-17-02-54-00-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-first5-2019-07-17-02-54-01-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-first10-2019-07-17-02-54-06-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-last5-2019-07-17-02-54-08-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-last10-2019-07-17-02-54-11-endpoint: Creating\n",
      "Checking endpoint statuses:\n",
      "Endpoint harvest-xgb-bc-drop0-2019-07-17-02-54-00-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-drop10-2019-07-17-02-54-00-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-first5-2019-07-17-02-54-01-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-first10-2019-07-17-02-54-06-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-last5-2019-07-17-02-54-08-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-last10-2019-07-17-02-54-11-endpoint: Creating\n",
      "Checking endpoint statuses:\n",
      "Endpoint harvest-xgb-bc-drop0-2019-07-17-02-54-00-endpoint: Creating\n",
      "Endpoint harvest-xgb-bc-drop10-2019-07-17-02-54-00-endpoint: InService\n",
      "Endpoint harvest-xgb-bc-first5-2019-07-17-02-54-01-endpoint: InService\n",
      "Endpoint harvest-xgb-bc-first10-2019-07-17-02-54-06-endpoint: InService\n",
      "Endpoint harvest-xgb-bc-last5-2019-07-17-02-54-08-endpoint: InService\n",
      "Endpoint harvest-xgb-bc-last10-2019-07-17-02-54-11-endpoint: Creating\n",
      "All endpoints created.\n"
     ]
    }
   ],
   "source": [
    "tr.wait_for_endpoints()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model\n",
    "Currently, we launch an endpoint to test the model. This endpoint includes a simple web service that takes POST request with rows of or model's X values - columns other than BadActor - and returns a corresponding list of Y values - BadActor predictions.\n",
    "\n",
    "The endpoint approach is most suitable to interactive use, such as possibly using the model to blacklist a harvesting session as soon as it is identified. For offline analysis, this should be reconfigured to run batch transform jobs instead, which are cheaper to run and more streamlined to invoke."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the confusion metrics\n",
    "\n",
    "Test all of the experiments against a new dataset and compute the confusion matrix values. A confusion matrix describes the proportions of true and false positives and negatives, together with some derived metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing columns: {'404', '311', '601'}\n"
     ]
    }
   ],
   "source": [
    "region = 'us-east-1'\n",
    "sg_client = boto3.client('runtime.sagemaker', region_name=region)\n",
    "\n",
    "import json\n",
    "from itertools import islice\n",
    "import math\n",
    "import struct\n",
    "\n",
    "payload_csv = \"out/single.csv\"\n",
    "\n",
    "# Use our remaining data to test our model\n",
    "# test_mixed = []\n",
    "# for i in range(3):\n",
    "#     name = 'data/mixed-{}.txt'.format(i+1)\n",
    "#     m = pd.read_csv(name,sep='\\t')\n",
    "#     print( \"File: {} Rows: {}\".format( name, len(m)))\n",
    "#     test_mixed.append(m)\n",
    "\n",
    "# df = pd.concat(test_mixed)\n",
    "\n",
    "df = pd.read_csv( \"data/single.txt\", sep=\"\\t\" )\n",
    "df[txn_col]=df[txn_col].astype(str)\n",
    "flat = flatten_txns( df ).drop( [sess_col], axis=1)\n",
    "label = pd.DataFrame(flat[bad_col])\n",
    "label.columns = [bad_col]\n",
    "flat = flat.drop(bad_col,axis=1)\n",
    "    \n",
    "flat.to_csv( path_or_buf=payload_csv,  header = None, index = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BadActor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2671</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4358</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4559</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5422</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5742</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5941</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6222</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6633</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7271</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      BadActor\n",
       "303          1\n",
       "2671         1\n",
       "4358         1\n",
       "4559         1\n",
       "5422         1\n",
       "5742         1\n",
       "5941         1\n",
       "6222         1\n",
       "6633         1\n",
       "7271         1"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label[label[bad_col]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(endpoint_name, csv):\n",
    "    '''\n",
    "    job_name - name of a job that has run all the way through to an endpoint\n",
    "    csv - a csv with the same y_col and x_col structure as the training data\n",
    "    returns - a single-column dataframe with the predictions from the model.\n",
    "    '''\n",
    "#     endpoint_name = tr.get_endpoint_name( job_name )\n",
    "    with open(csv, 'r') as f:\n",
    "        payload = f.read().strip()\n",
    "    response = sg_client.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                               ContentType='text/csv', \n",
    "                               Body=payload)\n",
    "    result = response['Body'].read()\n",
    "    result = result.decode(\"utf-8\")\n",
    "    result = result.split(',')\n",
    "    result = [round(float(i)) for i in result]\n",
    "    return pd.DataFrame( result )\n",
    "\n",
    "def compute_confusion(reference, test):\n",
    "    '''\n",
    "    reference - single-column dataframe of expected y-values - labels\n",
    "    test - single-column dataframe of computed y-values for comparison\n",
    "    '''\n",
    "    comp = pd.concat( [reference, test], axis = 1)\n",
    "    comp.columns =[\"label\",'prediction']\n",
    "    label_positive = comp['label'] == 1\n",
    "    predict_positive = comp['prediction'] == 1\n",
    "    tp = len( comp[label_positive & predict_positive])\n",
    "    fp = len( comp[~label_positive & predict_positive])\n",
    "    tn = len( comp[~label_positive & ~predict_positive])\n",
    "    fn = len( comp[label_positive & ~predict_positive])\n",
    "    m = len(comp)\n",
    "\n",
    "    accuracy = (tp+tn)/m\n",
    "    precision = tp/(tp+fp+0.000000001) # avoid division by 0\n",
    "    recall = tp/(tp+fn + 0.000000001)\n",
    "\n",
    "    print(\"accuracy: {} precision: {} recall {}\".format(accuracy, precision,recall))\n",
    "    return (accuracy, precision, recall, tp,fp,tn,fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9989184512221502 precision: 0.0 recall 0.0\n",
      "(0.9989184512221502, 0.0, 0.0, 0, 0, 9236, 10)\n",
      "accuracy: 0.9974042829331603 precision: 0.06249999999609375 recall 0.09999999999\n",
      "(0.9974042829331603, 0.06249999999609375, 0.09999999999, 1, 15, 9221, 9)\n",
      "accuracy: 0.9988102963443651 precision: 0.0 recall 0.0\n",
      "(0.9988102963443651, 0.0, 0.0, 0, 1, 9235, 10)\n",
      "accuracy: 0.9988102963443651 precision: 0.3333333332222222 recall 0.09999999999\n",
      "(0.9988102963443651, 0.3333333332222222, 0.09999999999, 1, 2, 9234, 9)\n",
      "accuracy: 0.9539260220635951 precision: 0.002392344497601932 recall 0.09999999999\n",
      "(0.9539260220635951, 0.002392344497601932, 0.09999999999, 1, 417, 8819, 9)\n",
      "accuracy: 0.9964308890330954 precision: 0.0399999999984 recall 0.09999999999\n",
      "(0.9964308890330954, 0.0399999999984, 0.09999999999, 1, 24, 9212, 9)\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "endpoints = [tr.get_endpoint_name( training_job) for (name, training_job) in tr.running_jobs]\n",
    "for endpoint in endpoints:\n",
    "    result = test_model( endpoint, payload_csv)\n",
    "    results.append(result)\n",
    "    print(compute_confusion(label, result) )\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BadActor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2306</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3426</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3909</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4560</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6398</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6633</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7379</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8695</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      BadActor\n",
       "337          1\n",
       "495          1\n",
       "2306         1\n",
       "3426         1\n",
       "3909         1\n",
       "4560         1\n",
       "6398         1\n",
       "6633         1\n",
       "7379         1\n",
       "8695         1"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# results[0].columns = [bad_col]\n",
    "# results[0]==label\n",
    "# results[0][results[0][bad_col]==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In normal traffic, almost no sessions are malicious. Compute a baseline on the assumption there are no Bad Actor rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9989184512221502 precision: 0.0 recall 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9989184512221502, 0.0, 0.0, 0, 0, 9236, 10)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline = label.copy()\n",
    "baseline.values[:] = 0\n",
    "compute_confusion(label, baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "In a series of five related experiments, I explored an approach for distinguishing malicious sessions from innocent sessions and demonstrated its effectiveness.\n",
    "\n",
    "The initial bag-of-transactions approach was extremely successful, with high accuracy and recall of almost 85%.\n",
    "Adding session time features slightly improved recall at a tiny cost in precision.\n",
    "\n",
    "Finally, using partial sessions did not greatly impair the performance of the algorithm, showing that the algorithm may be resilient enough for use in live traffic.\n",
    "\n",
    "All experiments performed much better than the baseline assumption that there is no attack traffic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remaining work\n",
    "\n",
    "1. Fix wait_for_endpoints.\n",
    "1. Move TrainingHelper and the other helpers into the JupyterHelper package\n",
    "1. Final cleanup so that there's a single panel that runs everything start to finish.\n",
    "    - consider creating a DataHelper class to move some of that stuff out\n",
    "1. Finish writing the batch test driver so that it prints a comparison of the results of the experiments\n",
    "1. Go back and add back the session-timing results that we seem to have dropped out of this architecture\n",
    "    - this might mean having to have a separate notebook\n",
    "    - consider combining the flatten method with the grouping approach, or putting an 'if' so that flatten only drops the sess_col when it's there.\n",
    "1. Review the rubric, correct, and submit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
