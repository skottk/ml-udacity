{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harvesting @ MLAI Training - First Round Overview\n",
    "\n",
    "This experiment trains an XGB model to distinguish harvesting sessions from ordinary sessions. In this model, we use a simplified representation of user sessions based on the common bag-of-words representation. This representation discards sequences and timing, reducing a user session to a row of counts for each transaction type.\n",
    "\n",
    "While this was the first attempt to train a model to recognize the activity of a known harvester, the techniques proved quite successful. The approach successfully distinguished 85% of the activity of the harvester with 0 false positives.\n",
    "\n",
    "The rest of this document describes and implements the experiment, closing with some suggested next steps.\n",
    "\n",
    "## Training Data\n",
    "The training dataset consists of several hours of raw transaction logs containing activity from all users, with the full collection of harvesting activity from the LiquidTension harvester across two years. All LiquidTension(LT) activity is labeled as 'BadActor' = 1, while all other traffic is assumed to be innocent and labeled as 'BadActor' = 0. Since LiquidTension is currently our only easily-identified single harvester, we need his full range of activity to have a BadActor sessions in proportion to innocent sessions for training to work properly.\n",
    "\n",
    "The training set includes the following files:\n",
    "\n",
    "|File       |Contents                             |Rows|\n",
    "------------|-------------------------------------|----|\n",
    "|may1.tsv|raw transactions|119474|\n",
    "|may2.tsv|raw transactions|43608|\n",
    "|may3.tsv|raw transactions|30844|\n",
    "|lt-only.tsv|raw transactions for a known attacker|61917|\n",
    "\n",
    "In all, we have 193926 transactions from \"innocent\" sessions and 61917 LT sessions. Approximately 32% of transactions are labeled BadActor = 1, giving us a reasonable proportion in both classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm up\n",
    "Import standard libraries and prepare the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sagemaker session, role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# S3 bucket name\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download\n",
    "Retrieve the datafiles from the project's designated S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "b = s3.Bucket('sagemaker-mlai-harvesting')\n",
    "\n",
    "# b.download_file( 'data/MLAI_ParsedDataSet.tsv', 'data/data.tsv')\n",
    "b.download_file( \"data/MinimalLogs/Minimal_May01.rpt\", 'data/may1.tsv')\n",
    "b.download_file( \"data/MinimalLogs/Minimal_May02.rpt\", 'data/may2.tsv')\n",
    "b.download_file( \"data/MinimalLogs/Minimal_May03.rpt\", 'data/may3.tsv')\n",
    "b.download_file( \"data/MinimalLogs/Minimal_OnlyLT.rpt\", 'data/lt-only.tsv')\n",
    "\n",
    "\n",
    "may1 = pd.read_csv('data/may1.tsv',sep='\\t')\n",
    "may2 = pd.read_csv('data/may2.tsv',sep='\\t')\n",
    "may3 = pd.read_csv('data/may3.tsv',sep='\\t')\n",
    "lt = pd.read_csv('data/lt-only.tsv',sep='\\t')\n",
    "\n",
    "bad_col='BadActor'\n",
    "sess_col='SessionNo'\n",
    "txn_col='Act'\n",
    "\n",
    "txn = may1.append([may2, may3, lt])\n",
    "txn[txn[bad_col]==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data conversion and feature engineering\n",
    "In real life, a session consists of a series of rows of transactions of different types, and each transaction type records a variable number of additional metadata attributes describing a logged event, for a total of over 30 columns of extracted data. In addition, our tagging process has given each row a BadActor label.\n",
    "\n",
    "|sessionno|txn id|BadActor|parm1|parm2|...|\n",
    "|---------|------|--------|-----|-----|---|\n",
    "|1240|111|0|query string|...|...|\n",
    "|1240|112|0|meta|...|...|\n",
    "|2993|301|1|meta|...|...|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Innocent' log entries\n",
    "txns = pd.DataFrame(np.sort(txn['Act'].unique()))\n",
    "\n",
    "# Harvesting log entries\n",
    "lt_txns = pd.DataFrame(np.sort(lt['Act'].unique()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We drop most of this information, including the temporal sequence of the log entries, and convert each session into a single row of data. Almost all of the columns go away, replaced by counts of transaction types in the session.\n",
    "\n",
    "|sessionno|BadActor|111|112|113|...|301|302|...|\n",
    "|---------|--------|---|---|---|---|---|---|---|\n",
    "|1240|0|1|1|0|...|0|0|...|\n",
    "|2993|1|0|0|0|...|1|0|...|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_txns( txn_log ):\n",
    "    txn_narrow = txn_log[[sess_col, txn_col,bad_col]]\n",
    "    txn_pivot = pd.pivot_table(txn_narrow, index=[sess_col,bad_col], columns = [txn_col],aggfunc=[np.size]).fillna(0)\n",
    "    txn_pivot.columns = txn_pivot.columns.droplevel(0)           # the pivot table has a two-level index\n",
    "    txn_flat = txn_pivot.rename_axis(None, axis=1).reset_index() # these two lines get rid of it so we have a simple table\n",
    "    return txn_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SessionNo</th>\n",
       "      <th>BadActor</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>...</th>\n",
       "      <th>403</th>\n",
       "      <th>404</th>\n",
       "      <th>406</th>\n",
       "      <th>407</th>\n",
       "      <th>410</th>\n",
       "      <th>411</th>\n",
       "      <th>511</th>\n",
       "      <th>513</th>\n",
       "      <th>601</th>\n",
       "      <th>607</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2147481927</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2147360137</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2147317281</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2147002735</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2146953899</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-2146926264</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-2146915841</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-2146723372</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-2146089473</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-2145757832</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    SessionNo  BadActor  111  112  114   115  116  117  118  119  ...  403  \\\n",
       "0 -2147481927         0  3.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "1 -2147360137         1  3.0  3.0  0.0   0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "2 -2147317281         0  3.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "3 -2147002735         0  3.0  0.0  0.0   6.0  3.0  0.0  0.0  0.0  ...  0.0   \n",
       "4 -2146953899         0  0.0  3.0  0.0  60.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "5 -2146926264         0  3.0  0.0  0.0   3.0  0.0  3.0  0.0  0.0  ...  0.0   \n",
       "6 -2146915841         0  3.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "7 -2146723372         0  3.0  0.0  0.0   0.0  0.0  3.0  0.0  0.0  ...  0.0   \n",
       "8 -2146089473         0  3.0  0.0  0.0   3.0  3.0  3.0  0.0  0.0  ...  0.0   \n",
       "9 -2145757832         0  3.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "\n",
       "   404  406  407  410  411  511  513  601  607  \n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "5  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "6  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "7  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "8  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "9  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[10 rows x 38 columns]"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten_txns( txn ).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat = flatten_txns( txn )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Producing pools of training and testing data\n",
    "\n",
    "We will divide the combined good and bad data pools as follows:\n",
    "- a training set that the model iterates over during the learning process\n",
    "- a test set that is used to evaluate the model during training\n",
    "- a validation set that is kept separate to test the model after training is complete. We need separate test and validate pools in order to make sure that we're overfitting the model to a single set of test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_frame( df, train_frac ):\n",
    "    l = len(df)\n",
    "    test_frac = (1-train_frac)/2\n",
    "    tr = int(train_frac * l)\n",
    "    te = int(tr + test_frac * l)\n",
    "    \n",
    "    train = df[:tr]\n",
    "    test = df[tr:te]\n",
    "    val = df[te:]\n",
    "    return [train, test, val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_split( flat, bad_split=.8 ):\n",
    "    bad = flat[flat[bad_col]==1]\n",
    "    good = flat[flat[bad_col]==0]\n",
    "    \n",
    "    bads = split_frame(bad, bad_split)\n",
    "    goods = split_frame(good, bad_split)\n",
    "    \n",
    "    dfs = []\n",
    "    for i in range(3):\n",
    "        # Dropping the session # because we don't want to train on it.\n",
    "        # Also leaves our label - BadActor - in the 0 column, as XGBoost requires for CSV\n",
    "        df = bads[i].append(goods[i]).drop(sess_col,axis=1).sample(frac=1)\n",
    "        dfs.append( df )\n",
    "    \n",
    "    return dfs\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the data and upload to S3\n",
    "Break the set into train, test, and validation collections and output CSV's.\n",
    "As Sagemaker requires, leave out row indices and column headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘out’: File exists\n",
      "Uploading out/train.csv to sagemaker-mlai-harvesting\n",
      "None\n",
      "Uploading out/test.csv to sagemaker-mlai-harvesting\n",
      "None\n",
      "Uploading out/validate.csv to sagemaker-mlai-harvesting\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "dfs = train_split(flat, .8)\n",
    "\n",
    "!mkdir out\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "bucket = \"sagemaker-mlai-harvesting\"\n",
    "\n",
    "for i, df in enumerate(dfs):\n",
    "    files = [\"train\",\"test\",\"validate\"]\n",
    "    file = \"out/{}.csv\".format(files[i])\n",
    "    df.to_csv(path_or_buf= file, header=False, index=False  )\n",
    "\n",
    "    print(\"Uploading {} to {}\".format(file, bucket))\n",
    "\n",
    "    response = s3_client.upload_file(file, bucket, file)\n",
    "    print(response)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare and train a model\n",
    "Boilerplate code mostly copied from Amazon sample code at https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/xgboost_abalone/xgboost_abalone.ipynb, with ample room for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training job harvesting-xgboost-binary-classification2019-06-12-17-23-43\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "Completed\n",
      "CPU times: user 163 ms, sys: 0 ns, total: 163 ms\n",
      "Wall time: 3min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "region = 'us-east-1'\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container = get_image_uri(region, 'xgboost')\n",
    "\n",
    "\n",
    "from time import gmtime, strftime\n",
    "\n",
    "job_name = 'harvesting-xgboost-binary-classification' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(\"Training job\", job_name)\n",
    "\n",
    "#Ensure that the training and validation data folders generated above are reflected in the \"InputDataConfig\" parameter below.\n",
    "\n",
    "create_training_params = \\\n",
    "{\n",
    "    \"AlgorithmSpecification\": {\n",
    "        \"TrainingImage\": container,\n",
    "        \"TrainingInputMode\": \"File\"\n",
    "    },\n",
    "    \"RoleArn\": role,\n",
    "    \"OutputDataConfig\": {\n",
    "        \"S3OutputPath\": os.path.join(\"s3://\",bucket, \"out\", \"xgb-class\") \n",
    "    },\n",
    "    \"ResourceConfig\": {\n",
    "        \"InstanceCount\": 1,\n",
    "        \"InstanceType\": \"ml.m4.4xlarge\",\n",
    "        \"VolumeSizeInGB\": 5\n",
    "    },\n",
    "    \"TrainingJobName\": job_name,\n",
    "    \"HyperParameters\": {\n",
    "        \"max_depth\":\"5\",\n",
    "        \"eta\":\"0.2\",\n",
    "        \"gamma\":\"4\",\n",
    "        \"min_child_weight\":\"6\",\n",
    "        \"subsample\":\"0.7\",\n",
    "        \"silent\":\"0\",\n",
    "        \"objective\":\"binary:logistic\",\n",
    "        \"num_round\":\"50\"\n",
    "    },\n",
    "    \"StoppingCondition\": {\n",
    "        \"MaxRuntimeInSeconds\": 3600\n",
    "    },\n",
    "    \"InputDataConfig\": [\n",
    "        {\n",
    "            \"ChannelName\": \"train\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": os.path.join( \"s3://\", bucket, \"out\" ), \n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"text/csv\",\n",
    "            \"CompressionType\": \"None\"\n",
    "        },\n",
    "        {\n",
    "            \"ChannelName\": \"validation\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": os.path.join( \"s3://\", bucket, \"out\" ),\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"text/csv\",\n",
    "            \"CompressionType\": \"None\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "client = boto3.client('sagemaker', region_name=region)\n",
    "client.create_training_job(**create_training_params)\n",
    "\n",
    "import time\n",
    "\n",
    "status = client.describe_training_job(TrainingJobName=job_name)['TrainingJobStatus']\n",
    "print(status)\n",
    "while status !='Completed' and status!='Failed':\n",
    "    time.sleep(60)\n",
    "    status = client.describe_training_job(TrainingJobName=job_name)['TrainingJobStatus']\n",
    "    print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "harvesting-xgboost-binary-cl-2019-06-12-17-23-43-model\n",
      "s3://sagemaker-mlai-harvesting/out/xgb-class/harvesting-xgboost-binary-classification2019-06-12-17-23-43/output/model.tar.gz\n",
      "arn:aws:sagemaker:us-east-1:872344130825:model/harvesting-xgboost-binary-cl-2019-06-12-17-23-43-model\n",
      "CPU times: user 15 ms, sys: 4.03 ms, total: 19.1 ms\n",
      "Wall time: 335 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import boto3\n",
    "from time import gmtime, strftime\n",
    "\n",
    "model_name=\"harvesting-xgboost-binary-cl-2019-06-12-17-23-43\"+ '-model'\n",
    "print(model_name)\n",
    "\n",
    "info = client.describe_training_job(TrainingJobName=job_name)\n",
    "model_data = info['ModelArtifacts']['S3ModelArtifacts']\n",
    "print(model_data)\n",
    "\n",
    "primary_container = {\n",
    "    'Image': container,\n",
    "    'ModelDataUrl': model_data\n",
    "}\n",
    "\n",
    "create_model_response = client.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    PrimaryContainer = primary_container)\n",
    "\n",
    "print(create_model_response['ModelArn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harvest-XGBoostEndpointConfig-2019-06-12-17-30-04\n",
      "Endpoint Config Arn: arn:aws:sagemaker:us-east-1:872344130825:endpoint-config/harvest-xgboostendpointconfig-2019-06-12-17-30-04\n"
     ]
    }
   ],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "endpoint_config_name = 'Harvest-XGBoostEndpointConfig-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(endpoint_config_name)\n",
    "create_endpoint_config_response = client.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType':'ml.m4.xlarge',\n",
    "        'InitialVariantWeight':1,\n",
    "        'InitialInstanceCount':1,\n",
    "        'ModelName':model_name,\n",
    "        'VariantName':'AllTraffic'}])\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response['EndpointConfigArn'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launch an endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harvest-XGBoostEndpoint-2019-06-12-17-30-34\n",
      "arn:aws:sagemaker:us-east-1:872344130825:endpoint/harvest-xgboostendpoint-2019-06-12-17-30-34\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: InService\n",
      "Arn: arn:aws:sagemaker:us-east-1:872344130825:endpoint/harvest-xgboostendpoint-2019-06-12-17-30-34\n",
      "Status: InService\n",
      "CPU times: user 168 ms, sys: 5.78 ms, total: 174 ms\n",
      "Wall time: 9min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import time\n",
    "\n",
    "endpoint_name = 'Harvest-XGBoostEndpoint-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(endpoint_name)\n",
    "create_endpoint_response = client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name)\n",
    "print(create_endpoint_response['EndpointArn'])\n",
    "\n",
    "resp = client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp['EndpointStatus']\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status=='Creating':\n",
    "    time.sleep(60)\n",
    "    resp = client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp['EndpointStatus']\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp['EndpointArn'])\n",
    "print(\"Status: \" + status)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model\n",
    "Currently, we launch an endpoint to test the model. This endpoint includes a simple web service that takes POST request with rows of or model's X values - columns other than BadActor - and returns a corresponding list of Y values - BadActor predictions.\n",
    "\n",
    "The endpoint approach is most suitable to interactive use, such as possibly using the model to blacklist a harvesting session as soon as it is identified. For offline analysis, this should be reconfigured to run batch transform jobs instead, which are cheaper to run and more streamlined to invoke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_client = boto3.client('runtime.sagemaker', region_name=region)\n",
    "\n",
    "import json\n",
    "from itertools import islice\n",
    "import math\n",
    "import struct\n",
    "\n",
    "!head -10000 out/test.csv > out/single-test.csv\n",
    "\n",
    "file_name = 'out/single-test.csv' #customize to your test file\n",
    "\n",
    "csv = pd.read_csv(file_name, header=None)\n",
    "csv.columns\n",
    "label = csv[0]\n",
    "csv = csv.drop(0,axis=1)\n",
    "\n",
    "single = \"out/single.csv\"\n",
    "\n",
    "csv.to_csv(path_or_buf=single, header=False, index=False)\n",
    "\n",
    "with open(single, 'r') as f:\n",
    "    payload = f.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = runtime_client.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                   ContentType='text/csv', \n",
    "                                   Body=payload)\n",
    "result = response['Body'].read()\n",
    "result = result.decode(\"utf-8\")\n",
    "result = result.split(',')\n",
    "result = [round(float(i)) for i in result]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the confusion metrics\n",
    "\n",
    "A confusion matrix describes the proportions of true and false positives and negatives, together with some derived metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9668187474077147 precision: 1.0 recall 0.8409542743538767\n"
     ]
    }
   ],
   "source": [
    "comp = pd.concat( [label, pd.DataFrame(result)], axis = 1)\n",
    "comp.columns =[\"label\",'prediction']\n",
    "\n",
    "label_positive = comp['label'] == 1\n",
    "predict_positive = comp['prediction'] == 1\n",
    "\n",
    "tp = len( comp[label_positive & predict_positive])\n",
    "fp = len( comp[~label_positive & predict_positive])\n",
    "tn = len( comp[~label_positive & ~predict_positive])\n",
    "fn = len( comp[label_positive & ~predict_positive])\n",
    "\n",
    "m = len(comp)\n",
    "\n",
    "accuracy = (tp+tn)/m\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "\n",
    "print(\"accuracy: {} precision: {} recall {}\".format(accuracy, precision,recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(423, 0, 1908, 80, 2411)"
      ]
     },
     "execution_count": 612,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp,fp,tn,fn, len(comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The very first time we ran the model, we achieved strikingly successful rates of harvesting identification.\n",
    "The most significant number here is the recall of 84%, meaning that we successfully identified 84% of all harvesting sessions by looking only at counts of transaction types.\n",
    "\n",
    "This approach appears promising!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further investigating the data \n",
    "\n",
    "We had additional ideas for modeling the data while staying in this bag-of-transaction technique.\n",
    "1. Try some hyperparameter tuning to seem if the success rates can be trivially improved.\n",
    "1. Enrich the training data set in various ways - add colums to summarize total session time, average time/request, and so on.\n",
    "1. Perform some clustering analysis to try to identify common patterns of behavior other than LT. This may reveal the presence of other kinds of harvesting.\n",
    "\n",
    "## Qualifying the approach\n",
    "Can we use this approach to identify and blacklist harvesting sessions as they occur? Some notes:\n",
    "1. The approach must be resilient to easy efforts to evade. Does the accuracy of the identification drop if the attacker makes minor changes to his workflow?\n",
    "1. How long does it take to identify an attacker in real time? \n",
    "    1. Do we gain certainty soon enough to stop an attacker before he's done what he came to do?\n",
    "    2. Can we tag sessions accurately after the first N log entries, for instance?\n",
    "    \n",
    "## Designing an implemetation\n",
    "Design an architecture for identifying and intercepting harvesting activity in real time. Confirm data sources, manage impact to usage latency, model costs and ROI.\n",
    "\n",
    "In today's world, it would be less effective to perform real-time analysis on AWS, since all of our current content usage is on-prem. The algorithm used here, XGBoost, is performant on commodity hardware, so we may be able to run on standard VMs.\n",
    "\n",
    "In real-time analysis, we will face a stream of events from interleaved sessions. We will have to demultiplex these into individual event streams both for training and for prediction, implying some kind of windowing to capture and send sets of log entries as partial sessions. It's not clear how big the impact of this windowing will be on the accuracy of the models.\n",
    "\n",
    "# Other analytical techniques\n",
    "While this algorithm seems promising, we're throwing away a huge amount of intelligence before we start training, in the name of simplicity. We can evaluate what kind of gains we could achieve through more advanced techniques:\n",
    "- Stateful models like LSTM or CNN\n",
    "- more \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
