{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harvesting @ MLAI Training - Third Round Overview\n",
    "\n",
    "At this point in the series of experiments, we're going to try some divergent techniques. I'd like to try some tactics that will improve the fit of the model to continuous use in the live application flow. Right now, the model trains on retrospective data sets and more or less expects to see full sessions. In this experiment, I'm going to try several approaches.\n",
    "\n",
    "## Roadmap\n",
    "### Feature engineering and ML algorithms\n",
    "\n",
    "1. Instead of counting full sessions in the training datasets, we will only take sequences of a maximum N, to see if there's a useful threshold where we could evaluate sliding windows of session history. I'll compare recall at 5, 10, 15, 20 element session subsequences, offset randomly from session start. \n",
    "    - Where do the metrics drop off? \n",
    "    - Does the approach essentially require full sessions?\n",
    "    - Can we query partial sessions against a model trained on full sessions?\n",
    "    - Can we query partial sessions against a model trained on partial sessions?\n",
    "    - Randomly drop out D% of transactions from every session.\n",
    "1. Instead of using bag-of-txns, I will try a vector that contains the last N sessions by number, indexed from a transaction dictionary. This will force sequence into the model.\n",
    "    - Training on the first N transactions:\n",
    "        - Count N transactions from the beginning of a session.\n",
    "    - Train on N-gram tiles\n",
    "    - Train on N-gram shingles\n",
    "1. Instead of training an XGBoost model, train neural network models that are designed to remember sequences\n",
    "    - LSTM\n",
    "    - Convolutional (?)\n",
    "    \n",
    "### Software engineering\n",
    "To this point, I've mostly worked out of a some-what scattered Jupyter notebook pile of global variables, tuning up a few functions here and there to keep scopes clean. In particular, the Sagemaker code is terrible cut-and paste. As I work through the feature engineering & algorithmic roadmap, I also want to drive toward the following engineering goals:\n",
    "\n",
    "- Establish a consistent data manipulation pipeline that is easily customizable and reentrant, with no global variables\n",
    "- a set of wrapper functions for the Sagemaker API, so that I can have a simple pipeline where training jobs -> models -> batch transformations or endpoints, without giant string constant parameters that are impossible to edit or sight-check\n",
    "- Automatically engage a hyperparameter tuning job as desired without changing how the model is implemented or trained.\n",
    "- Easily training many models at once\n",
    "- Easily launch many batch transform jobs/endpoints at once.\n",
    "- Easily display the results of the many jobs at once.\n",
    "- Particular feature engineering processes glide smoothly from experimental tinkering to code that can be put into production, without throwing everything away. Some of this just falls out of doing everything else better.\n",
    "\n",
    "# Training Data\n",
    "The training dataset consists of several hours of raw transaction logs containing activity from all users, with the full collection of harvesting activity from the LiquidTension harvester across two years. All LiquidTension(LT) activity is labeled as 'BadActor' = 1, while all other traffic is assumed to be innocent and labeled as 'BadActor' = 0. Since LiquidTension is currently our only easily-identified single harvester, we need his full range of activity to have a BadActor sessions in proportion to innocent sessions for training to work properly.\n",
    "\n",
    "The training set includes the following files:\n",
    "\n",
    "|File       |Contents                             |Rows|\n",
    "------------|-------------------------------------|----|\n",
    "|may1.tsv|raw transactions|119474|\n",
    "|may2.tsv|raw transactions|43608|\n",
    "|may3.tsv|raw transactions|30844|\n",
    "|lt-only.tsv|raw transactions for a known attacker|61917|\n",
    "\n",
    "In all, we have 193926 transactions from \"innocent\" sessions and 61917 LT sessions. Approximately 32% of transactions are labeled BadActor = 1, giving us a reasonable proportion in both classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence testing phase 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm up\n",
    "Import standard libraries and prepare the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data’: File exists\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "import toolz\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sagemaker session, role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# S3 bucket name\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "# common column names\n",
    "bad_col='BadActor'\n",
    "sess_col='SessionNo'\n",
    "txn_col='Act'\n",
    "logtime_col = 'LogTime'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download\n",
    "Retrieve the datafiles from the project's designated S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "b = s3.Bucket('sagemaker-mlai-harvesting')\n",
    "\n",
    "# b.download_file( 'data/MLAI_ParsedDataSet.tsv', 'data/data.tsv')\n",
    "b.download_file( \"data/MinimalLogs/Minimal_May01.rpt\", 'data/may1.tsv')\n",
    "b.download_file( \"data/MinimalLogs/Minimal_May02.rpt\", 'data/may2.tsv')\n",
    "b.download_file( \"data/MinimalLogs/Minimal_May03.rpt\", 'data/may3.tsv')\n",
    "b.download_file( \"data/MinimalLogs/Minimal_OnlyLT.rpt\", 'data/lt-only.tsv')\n",
    "\n",
    "\n",
    "may1 = pd.read_csv('data/may1.tsv',sep='\\t')\n",
    "may2 = pd.read_csv('data/may2.tsv',sep='\\t')\n",
    "may3 = pd.read_csv('data/may3.tsv',sep='\\t')\n",
    "lt = pd.read_csv('data/lt-only.tsv',sep='\\t')\n",
    "\n",
    "\n",
    "\n",
    "txn = may1.append([may2, may3, lt])\n",
    "txn[logtime_col] = pd.to_datetime(txn[logtime_col])\n",
    "# txn[txn[bad_col]==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data conversion and feature engineering\n",
    "In real life, a session consists of a series of rows of transactions of different types, and each transaction type records a variable number of additional metadata attributes describing a logged event, for a total of over 30 columns of extracted data. In addition, our tagging process has given each row a BadActor label.\n",
    "\n",
    "|sessionno|txn id|BadActor|parm1|parm2|...|\n",
    "|---------|------|--------|-----|-----|---|\n",
    "|1240|111|0|query string|...|...|\n",
    "|1240|112|0|meta|...|...|\n",
    "|2993|301|1|meta|...|...|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Innocent' log entries\n",
    "txns = pd.DataFrame(np.sort(txn['Act'].unique()))\n",
    "\n",
    "# Harvesting log entries\n",
    "lt_txns = pd.DataFrame(np.sort(lt['Act'].unique()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We drop most of this information, including the temporal sequence of the log entries, and convert each session into a single row of data. Almost all of the columns go away, replaced by counts of transaction types in the session.\n",
    "\n",
    "|sessionno|BadActor|111|112|113|...|301|302|...|\n",
    "|---------|--------|---|---|---|---|---|---|---|\n",
    "|1240|0|1|1|0|...|0|0|...|\n",
    "|2993|1|0|0|0|...|1|0|...|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Truncating sessions\n",
    "Before we flatten the sessions, we're going to truncate them. This may be a better match for the real world, in which at best we will be able to scan sliding windows of transactions with a scaling resumption that we may not scan every event.\n",
    "\n",
    "We'll try several approaches at once:\n",
    "- dropping out D% of transactions from every session\n",
    "- taking only the first N sessions from every session\n",
    "- taking on the last N transactions from every session\n",
    "- taking N consecutive transactions from the middle of every session. \n",
    "- choosing N transactions as above, but dropping every session without at least N transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas.core.groupby.generic.DataFrameGroupBy object at 0x7f54544f50f0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def drop_column_groups( txn_c ):\n",
    "    txn_c.columns = txn_c.columns.droplevel(0)  \n",
    "    txn_c.rename_axis(None, axis=1).reset_index()\n",
    "    return txn_c\n",
    "\n",
    "def get_session_groups( txn ):\n",
    "    txn_g = txn.groupby(sess_col)\n",
    "    return txn_g\n",
    "\n",
    "txng = get_session_groups(txn)\n",
    "txng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def drop_pct( df, n = .1 ):\n",
    "    return df.sample(frac= 1-n)\n",
    "\n",
    "def first_n( df, n = 5):\n",
    "    return df.head( n )\n",
    "    \n",
    "def last_n( df, n = 5):\n",
    "    return df.tail( n )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grappl( fun, parm ):\n",
    "    return lambda df: df.apply(fun, parm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_txns( txn_log ):\n",
    "    txn_narrow = txn_log[[sess_col, txn_col,bad_col]]\n",
    "    txn_pivot = pd.pivot_table(txn_narrow, index=[sess_col,bad_col], columns = [txn_col],aggfunc=[len]).fillna(0)\n",
    "    txn_pivot.columns = txn_pivot.columns.droplevel(0)           # the pivot table has a two-level index\n",
    "    txn_flat = txn_pivot.rename_axis(None, axis=1).reset_index() # these two lines get rid of it so we have a simple table\n",
    "    return txn_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "flats = [flatten_txns( txn.reset_index() ) for txn in txngs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('drop10', <function __main__.drop_pct(df, n=0.1)>, 0.1),\n",
       " ('first5', <function __main__.first_n(df, n=5)>, 5),\n",
       " ('last5', <function __main__.last_n(df, n=5)>, 5)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs = [['drop10', [drop_pct, .1]], \n",
    "        ['first5', [first_n, 5]],\n",
    "        ['last5', [last_n, 5]]\n",
    "       ]\n",
    "\n",
    "[(name, func, parm) for [name, [func, parm]] in jobs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_jobs(df, jobs):\n",
    "    groups = [[name, grappl(fun, parm)(df)] for [name, [fun, parm]] in jobs]\n",
    "    flats = [[name,flatten_txns( txn.reset_index() )] for [name,txn] in groups]\n",
    "    return flats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38.6 s, sys: 232 ms, total: 38.8 s\n",
      "Wall time: 38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "gs = prep_jobs(txng, jobs )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['drop10',\n",
       "         SessionNo  BadActor  111  112  114   115  116  117  118  119  ...  \\\n",
       " 0     -2147481927         0  1.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...   \n",
       " 1     -2147360137         1  1.0  1.0  0.0   0.0  0.0  0.0  0.0  0.0  ...   \n",
       " 2     -2147317281         0  1.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...   \n",
       " 3     -2147002735         0  1.0  0.0  0.0   2.0  1.0  0.0  0.0  0.0  ...   \n",
       " 4     -2146953899         0  0.0  1.0  0.0  19.0  0.0  0.0  0.0  0.0  ...   \n",
       " 5     -2146926264         0  1.0  0.0  0.0   1.0  0.0  1.0  0.0  0.0  ...   \n",
       " 6     -2146915841         0  1.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...   \n",
       " 7     -2146723372         0  1.0  0.0  0.0   0.0  0.0  1.0  0.0  0.0  ...   \n",
       " 8     -2146089473         0  1.0  0.0  0.0   1.0  1.0  1.0  0.0  0.0  ...   \n",
       " 9     -2145757832         0  1.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...   \n",
       " 10    -2145629827         1  1.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...   \n",
       " 11    -2145548627         1  1.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...   \n",
       " 12    -2145508864         1  1.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...   \n",
       " 13    -2145260428         0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...   \n",
       " 14    -2145083328         0  1.0  0.0  0.0   3.0  1.0  0.0  0.0  0.0  ...   \n",
       " 15    -2145021172         0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...   \n",
       " 16    -2144988764         0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...   \n",
       " 17    -2144649032         0  1.0  0.0  0.0   2.0  0.0  1.0  0.0  0.0  ...   \n",
       " 18    -2144379411         1  1.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...   \n",
       " 19    -2143861029         0  1.0  0.0  0.0   1.0  0.0  0.0  0.0  0.0  ...   \n",
       " 20    -2143836085         1  1.0  0.0  0.0   1.0  0.0  0.0  0.0  0.0  ...   \n",
       " 21    -2143707174         0  0.0  0.0  0.0   1.0  0.0  0.0  0.0  0.0  ...   \n",
       " 22    -2143402782         1  1.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...   \n",
       " 23    -2143252973         0  1.0  0.0  0.0   1.0  0.0  0.0  0.0  0.0  ...   \n",
       " 24    -2143133687         0  0.0  0.0  0.0   3.0  0.0  1.0  0.0  0.0  ...   \n",
       " 25    -2142836932         0  1.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...   \n",
       " 26    -2142756670         0  0.0  0.0  0.0  12.0  0.0  0.0  1.0  0.0  ...   \n",
       " 27    -2142673878         0  1.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...   \n",
       " 28    -2142367262         0  1.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...   \n",
       " 29    -2142141513         0  1.0  0.0  0.0   1.0  0.0  1.0  0.0  0.0  ...   \n",
       " ...           ...       ...  ...  ...  ...   ...  ...  ...  ...  ...  ...   \n",
       " 24084  2141185865         0  1.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...   \n",
       " 24085  2141204299         0  1.0  0.0  0.0   5.0  5.0  0.0  0.0  0.0  ...   \n",
       " 24086  2141316668         0  1.0  0.0  0.0   0.0  0.0  1.0  0.0  0.0  ...   \n",
       " 24087  2141435832         0  1.0  0.0  0.0   1.0  0.0  1.0  0.0  0.0  ...   \n",
       " 24088  2141604269         0  1.0  0.0  0.0  51.0  0.0  0.0  0.0  0.0  ...   \n",
       " 24089  2141659155         1  1.0  0.0  0.0   0.0  0.0  1.0  0.0  0.0  ...   \n",
       " 24090  2141967358         0  1.0  0.0  0.0   0.0  0.0  1.0  0.0  0.0  ...   \n",
       " 24091  2142314243         0  1.0  0.0  0.0   6.0  1.0  0.0  0.0  0.0  ...   \n",
       " 24092  2142662072         0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...   \n",
       " 24093  2142883874         0  1.0  0.0  0.0   1.0  0.0  1.0  0.0  0.0  ...   \n",
       " 24094  2142982769         0  1.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...   \n",
       " 24095  2143261596         1  1.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...   \n",
       " 24096  2143465070         1  1.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...   \n",
       " 24097  2143997720         0  0.0  0.0  0.0  15.0  1.0  0.0  0.0  0.0  ...   \n",
       " 24098  2144657533         0  1.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...   \n",
       " 24099  2144734527         0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...   \n",
       " 24100  2144806869         0  0.0  0.0  0.0   2.0  1.0  1.0  0.0  0.0  ...   \n",
       " 24101  2144873476         0  1.0  0.0  0.0   1.0  0.0  1.0  0.0  0.0  ...   \n",
       " 24102  2145093193         0  1.0  0.0  0.0   2.0  0.0  0.0  0.0  0.0  ...   \n",
       " 24103  2145203170         0  1.0  0.0  0.0   2.0  0.0  0.0  0.0  0.0  ...   \n",
       " 24104  2145285907         0  1.0  0.0  0.0   2.0  0.0  1.0  0.0  0.0  ...   \n",
       " 24105  2145339713         0  1.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...   \n",
       " 24106  2145535802         0  1.0  0.0  0.0   1.0  0.0  0.0  0.0  0.0  ...   \n",
       " 24107  2146229003         0  1.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...   \n",
       " 24108  2146286657         1  1.0  1.0  0.0   0.0  0.0  0.0  0.0  0.0  ...   \n",
       " 24109  2146314735         0  1.0  0.0  0.0   1.0  1.0  1.0  0.0  0.0  ...   \n",
       " 24110  2146596552         0  1.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...   \n",
       " 24111  2146743041         0  1.0  0.0  0.0   1.0  0.0  0.0  0.0  0.0  ...   \n",
       " 24112  2147184577         1  1.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...   \n",
       " 24113  2147318729         0  1.0  0.0  0.0   0.0  0.0  1.0  0.0  0.0  ...   \n",
       " \n",
       "        403  404  406  407  410  411  511  513  601  607  \n",
       " 0      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 1      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 2      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 3      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 4      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 5      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 6      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 7      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 8      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 9      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 10     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 11     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 12     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 13     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 14     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 15     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 16     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 17     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 18     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 19     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 20     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 21     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 22     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 23     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 24     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 25     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 26     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 27     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 28     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 29     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " ...    ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
       " 24084  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 24085  0.0  0.0  0.0  0.0  1.0  1.0  0.0  0.0  0.0  0.0  \n",
       " 24086  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 24087  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 24088  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 24089  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 24090  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 24091  0.0  0.0  0.0  0.0  2.0  1.0  0.0  0.0  0.0  0.0  \n",
       " 24092  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 24093  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 24094  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 24095  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 24096  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 24097  0.0  0.0  0.0  0.0  4.0  4.0  0.0  0.0  0.0  0.0  \n",
       " 24098  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 24099  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 24100  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 24101  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 24102  0.0  0.0  0.0  0.0  1.0  1.0  0.0  0.0  0.0  0.0  \n",
       " 24103  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 24104  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 24105  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 24106  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  \n",
       " 24107  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 24108  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 24109  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 24110  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 24111  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 24112  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 24113  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " \n",
       " [24114 rows x 38 columns]]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txngs= gs\n",
    "[(name,len(df)) for [name,df] in txngs]\n",
    "gs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Producing pools of training and testing data\n",
    "\n",
    "In order to support simultaneous execution of multiple jobs, this notebook introduces a new scheme for piping data through to models.\n",
    "\n",
    "The normal flow runs as follows:\n",
    "\n",
    "input data(s3) -> df's on notebook instance -> train.csv, test.csv, validate.csv on notebook instance -> s3/out/ -> Sagemaker instances\n",
    "\n",
    "Hardcoding these filenames is fine for playing around in a notebook, but it limits us to one job at a time.\n",
    "\n",
    "In this approach, every job has a base Name. This name will carry through from S3 into the Sagemaker instances.\n",
    "Training data files will reside in `<s3bucket>/<key>/out/Name`.\n",
    "\n",
    "As before, in each `Name` subfolder, we will divide the combined good and bad data pools as follows:\n",
    "- a training set that the model iterates over during the learning process\n",
    "- a test set that is used to evaluate the model during training\n",
    "- a validation set that is kept separate to test the model after training is complete. We need separate test and validate pools in order to make sure that we're not overfitting the model to a single set of test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameSplitter:\n",
    "    def __init__(self, y_col, ignore_cols):\n",
    "        self.y_col = y_col\n",
    "        self.ignore_cols = ignore_cols\n",
    "\n",
    "    def reformat( self, df ):\n",
    "        y = df[self.y_col]\n",
    "        csv = pd.concat( [y, df.drop(self.y_col,axis=1).drop(self.ignore_cols,axis=1)], axis=1)\n",
    "        return csv\n",
    "\n",
    "    def map_sets( fun, dfs ):\n",
    "        '''\n",
    "        fun - function that operates on a df\n",
    "        dfs - [[name1, df1], [name2,df2],...]\n",
    "        \n",
    "        returns - [result1, result2,...]\n",
    "        '''\n",
    "        \n",
    "        return [[name,fun(df)] for [name, df] in dfs]\n",
    "    \n",
    "    def to_csv( self, df, file, sepr=','):\n",
    "        '''\n",
    "        Write a CSV that meets Sagemaker's requirements for training instances:\n",
    "            - y columns are at [0]\n",
    "            - non-x, non-y columns are dropped so they don't affect training\n",
    "        '''\n",
    "        df.to_csv( path_or_buf=file, sep=sepr,index=False, header=False)\n",
    "    \n",
    "    def split_frame( self, df, train_frac ):\n",
    "        '''\n",
    "        df - DataFrame (or array) containing rows of data for training, testing, and validation\n",
    "        train_frac - Decimal representing how much of the set should be used for training.\n",
    "        After reserving train_frac * len(df) rows for training, divide the remainder in half for testing and validation \n",
    "        populations.\n",
    "        returns - list of dataframes as [training, testing, validation]\n",
    "        '''\n",
    "        l = len(df)\n",
    "        test_frac = (1-train_frac)/2\n",
    "        tr = int(train_frac * l)\n",
    "        te = int(tr + test_frac * l)\n",
    "\n",
    "        train = df[:tr]\n",
    "        test = df[tr:te]\n",
    "        val = df[te:]\n",
    "        return [train, test, val]\n",
    "    \n",
    "    def train_split( self, df, y_split=.8 ):\n",
    "        '''\n",
    "        df - dataframe of training data with one labeled column\n",
    "        y_col - column containing labels 1 or 0\n",
    "        Given a dataset with rows in two classes, distinguished by a column y_col with values 1 or 0,\n",
    "        split the dataset into [train, test, validate] sets containing values from both classes.       \n",
    "        '''\n",
    "        y1 = df[df[self.y_col]==1]\n",
    "        y0 = df[df[self.y_col]==0]\n",
    "\n",
    "        y1 = split_frame(y1, y_split)\n",
    "        y0 = split_frame(y0, y_split)\n",
    "\n",
    "        dfs = []\n",
    "        for i in range(3):\n",
    "            # Dropping the session # because we don't want to train on it.\n",
    "            # Also leaves our label - BadActor - in the 0 column, as XGBoost requires for CSV\n",
    "            dfi = y1[i].append(y0[i]).sample(frac=1)\n",
    "            dfs.append( dfi )\n",
    "    \n",
    "        return dfs\n",
    "    \n",
    "#     def write_splits( splits, path, sepr = ',', ext='csv'):\n",
    "#         '''\n",
    "#         splits - list of three dfs to be written as [train.ext, test.ext, validate.ext]\n",
    "#         path - path to write files to. Directory need not exist\n",
    "#         sepr - separator char for output files - typically , or \\t\n",
    "#         ext - file extension for output files, typically csv or txv\n",
    "#         Write train, test, validate delimited files to specified subfolder. Ensure that the subfolder exists.\n",
    "#         returns - filenames with full paths\n",
    "#         '''\n",
    "        \n",
    "#         filenames = [n + ext for n in ['train.', 'test.', 'validation.']]\n",
    "        \n",
    "#         #TODO - make the dir, write the files\n",
    "        \n",
    "        \n",
    "#         return split_files\n",
    "        \n",
    "    \n",
    "    def train_splits( self, dfs, split=0.4, sepr=',', ext='csv'):\n",
    "        '''\n",
    "        dfs - list of named dfs as [name, df] pairs\n",
    "        sepr - separator char for output files - typically , or \\t\n",
    "        ext - file extension for output files, typically csv or txv\n",
    "        Split a list of named dfs into train, test, validate csv's organized into subdirectories by the df names\n",
    "        returns - list of output filenames in [train, test, validate] triples, in same order as source files in dfs\n",
    "        '''\n",
    "        files = []\n",
    "        files = [self.train_split (df, split) for [name, df] in dfs]\n",
    "#         files = [write_splits (df, name, sepr, ext) for [name, df] in dfs]\n",
    "        \n",
    "        return files\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = FrameSplitter( bad_col, [sess_col])\n",
    "txn_narrow = FrameSplitter.map_sets( csv.reformat, gs )\n",
    "trains = csv.train_splits(txn_narrow)\n",
    "csv.to_csv( trains[0][0], \"test.csv\")\n",
    "# trains[0][0].to_csv(sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_frame( df, train_frac ):\n",
    "    l = len(df)\n",
    "    test_frac = (1-train_frac)/2\n",
    "    tr = int(train_frac * l)\n",
    "    te = int(tr + test_frac * l)\n",
    "    \n",
    "    train = df[:tr]\n",
    "    test = df[tr:te]\n",
    "    val = df[te:]\n",
    "    return [train, test, val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_split( flat, bad_split=.8 ):\n",
    "    bad = flat[flat[bad_col]==1]\n",
    "    good = flat[flat[bad_col]==0]\n",
    "    \n",
    "    bads = split_frame(bad, bad_split)\n",
    "    goods = split_frame(good, bad_split)\n",
    "    \n",
    "    dfs = []\n",
    "    for i in range(3):\n",
    "        # Dropping the session # because we don't want to train on it.\n",
    "        # Also leaves our label - BadActor - in the 0 column, as XGBoost requires for CSV\n",
    "        df = bads[i].append(goods[i]).drop(sess_col,axis=1).sample(frac=1)\n",
    "        dfs.append( df )\n",
    "    \n",
    "    return dfs\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the data and upload to S3\n",
    "Break the set into train, test, and validation collections and output CSV's.\n",
    "As Sagemaker requires, leave out row indices and column headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs = train_split(base, .4)\n",
    "# train_splits = [(name, train_split(df, .4)) for [name, df] in txngs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SessionNo</th>\n",
       "      <th>level_1</th>\n",
       "      <th>LogTime</th>\n",
       "      <th>CustID</th>\n",
       "      <th>GroupID</th>\n",
       "      <th>ProfID</th>\n",
       "      <th>Act</th>\n",
       "      <th>BadActor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2147481927</td>\n",
       "      <td>78744</td>\n",
       "      <td>2019-05-01 22:09:42</td>\n",
       "      <td>s9003448</td>\n",
       "      <td>main</td>\n",
       "      <td>eds</td>\n",
       "      <td>111</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2147360137</td>\n",
       "      <td>59038</td>\n",
       "      <td>2019-04-23 00:41:52</td>\n",
       "      <td>s8986718</td>\n",
       "      <td>main</td>\n",
       "      <td>iconnweb</td>\n",
       "      <td>112</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2147360137</td>\n",
       "      <td>59039</td>\n",
       "      <td>2019-04-23 00:41:52</td>\n",
       "      <td>s8986718</td>\n",
       "      <td>main</td>\n",
       "      <td>iconnweb</td>\n",
       "      <td>311</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2147360137</td>\n",
       "      <td>59037</td>\n",
       "      <td>2019-04-23 00:41:39</td>\n",
       "      <td>s8986718</td>\n",
       "      <td>main</td>\n",
       "      <td>iconnweb</td>\n",
       "      <td>111</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2147317281</td>\n",
       "      <td>118033</td>\n",
       "      <td>2019-05-01 23:55:06</td>\n",
       "      <td>s8989685</td>\n",
       "      <td>main</td>\n",
       "      <td>edsapi</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-2147317281</td>\n",
       "      <td>118446</td>\n",
       "      <td>2019-05-01 23:56:29</td>\n",
       "      <td>s8989685</td>\n",
       "      <td>main</td>\n",
       "      <td>edsapi</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-2147317281</td>\n",
       "      <td>117986</td>\n",
       "      <td>2019-05-01 23:54:56</td>\n",
       "      <td>s8989685</td>\n",
       "      <td>main</td>\n",
       "      <td>edsapi</td>\n",
       "      <td>111</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-2147317281</td>\n",
       "      <td>118004</td>\n",
       "      <td>2019-05-01 23:54:59</td>\n",
       "      <td>s8989685</td>\n",
       "      <td>main</td>\n",
       "      <td>edsapi</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-2147002735</td>\n",
       "      <td>28950</td>\n",
       "      <td>2019-05-04 19:52:22</td>\n",
       "      <td>s8989984</td>\n",
       "      <td>main</td>\n",
       "      <td>ehost</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-2147002735</td>\n",
       "      <td>29635</td>\n",
       "      <td>2019-05-04 19:54:52</td>\n",
       "      <td>s8989984</td>\n",
       "      <td>main</td>\n",
       "      <td>ehost</td>\n",
       "      <td>116</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-2147002735</td>\n",
       "      <td>29706</td>\n",
       "      <td>2019-05-04 19:55:05</td>\n",
       "      <td>s8989984</td>\n",
       "      <td>main</td>\n",
       "      <td>ehost</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-2147002735</td>\n",
       "      <td>29182</td>\n",
       "      <td>2019-05-04 19:53:26</td>\n",
       "      <td>s8989984</td>\n",
       "      <td>main</td>\n",
       "      <td>ehost</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-2147002735</td>\n",
       "      <td>29136</td>\n",
       "      <td>2019-05-04 19:53:17</td>\n",
       "      <td>s8989984</td>\n",
       "      <td>main</td>\n",
       "      <td>ehost</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-2146953899</td>\n",
       "      <td>13977</td>\n",
       "      <td>2019-05-01 19:33:28</td>\n",
       "      <td>s8875834</td>\n",
       "      <td>main</td>\n",
       "      <td>novplus</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-2146953899</td>\n",
       "      <td>19155</td>\n",
       "      <td>2019-05-01 19:45:46</td>\n",
       "      <td>s8875834</td>\n",
       "      <td>main</td>\n",
       "      <td>novplus</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-2146953899</td>\n",
       "      <td>9564</td>\n",
       "      <td>2019-05-01 19:23:22</td>\n",
       "      <td>s8875834</td>\n",
       "      <td>main</td>\n",
       "      <td>novplus</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-2146953899</td>\n",
       "      <td>6125</td>\n",
       "      <td>2019-05-01 19:15:18</td>\n",
       "      <td>s8875834</td>\n",
       "      <td>main</td>\n",
       "      <td>novplus</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-2146953899</td>\n",
       "      <td>5887</td>\n",
       "      <td>2019-05-01 19:14:39</td>\n",
       "      <td>s8875834</td>\n",
       "      <td>main</td>\n",
       "      <td>novplus</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-2146953899</td>\n",
       "      <td>21599</td>\n",
       "      <td>2019-05-01 19:52:40</td>\n",
       "      <td>s8875834</td>\n",
       "      <td>main</td>\n",
       "      <td>novplus</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-2146953899</td>\n",
       "      <td>13591</td>\n",
       "      <td>2019-05-01 19:32:33</td>\n",
       "      <td>s8875834</td>\n",
       "      <td>main</td>\n",
       "      <td>novplus</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-2146953899</td>\n",
       "      <td>14338</td>\n",
       "      <td>2019-05-01 19:34:16</td>\n",
       "      <td>s8875834</td>\n",
       "      <td>main</td>\n",
       "      <td>novplus</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-2146953899</td>\n",
       "      <td>15015</td>\n",
       "      <td>2019-05-01 19:35:43</td>\n",
       "      <td>s8875834</td>\n",
       "      <td>main</td>\n",
       "      <td>novplus</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-2146953899</td>\n",
       "      <td>6087</td>\n",
       "      <td>2019-05-01 19:15:12</td>\n",
       "      <td>s8875834</td>\n",
       "      <td>main</td>\n",
       "      <td>novplus</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-2146953899</td>\n",
       "      <td>38238</td>\n",
       "      <td>2019-05-01 20:35:15</td>\n",
       "      <td>s8875834</td>\n",
       "      <td>main</td>\n",
       "      <td>novplus</td>\n",
       "      <td>112</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-2146953899</td>\n",
       "      <td>24503</td>\n",
       "      <td>2019-05-01 19:59:54</td>\n",
       "      <td>s8875834</td>\n",
       "      <td>main</td>\n",
       "      <td>novplus</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-2146953899</td>\n",
       "      <td>864</td>\n",
       "      <td>2019-05-01 19:02:12</td>\n",
       "      <td>s8875834</td>\n",
       "      <td>main</td>\n",
       "      <td>novplus</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-2146953899</td>\n",
       "      <td>22332</td>\n",
       "      <td>2019-05-01 19:54:29</td>\n",
       "      <td>s8875834</td>\n",
       "      <td>main</td>\n",
       "      <td>novplus</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-2146953899</td>\n",
       "      <td>14244</td>\n",
       "      <td>2019-05-01 19:34:05</td>\n",
       "      <td>s8875834</td>\n",
       "      <td>main</td>\n",
       "      <td>novplus</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-2146953899</td>\n",
       "      <td>4215</td>\n",
       "      <td>2019-05-01 19:10:15</td>\n",
       "      <td>s8875834</td>\n",
       "      <td>main</td>\n",
       "      <td>novplus</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-2146953899</td>\n",
       "      <td>22164</td>\n",
       "      <td>2019-05-01 19:54:07</td>\n",
       "      <td>s8875834</td>\n",
       "      <td>main</td>\n",
       "      <td>novplus</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232553</th>\n",
       "      <td>2145203170</td>\n",
       "      <td>108365</td>\n",
       "      <td>2019-05-01 23:21:33</td>\n",
       "      <td>s8990848</td>\n",
       "      <td>main</td>\n",
       "      <td>ehost</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232554</th>\n",
       "      <td>2145285907</td>\n",
       "      <td>113497</td>\n",
       "      <td>2019-05-01 23:39:23</td>\n",
       "      <td>s8884531</td>\n",
       "      <td>Main</td>\n",
       "      <td>eds</td>\n",
       "      <td>117</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232555</th>\n",
       "      <td>2145285907</td>\n",
       "      <td>113509</td>\n",
       "      <td>2019-05-01 23:39:24</td>\n",
       "      <td>s8884531</td>\n",
       "      <td>Main</td>\n",
       "      <td>eds</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232556</th>\n",
       "      <td>2145285907</td>\n",
       "      <td>113500</td>\n",
       "      <td>2019-05-01 23:39:23</td>\n",
       "      <td>s8884531</td>\n",
       "      <td>Main</td>\n",
       "      <td>eds</td>\n",
       "      <td>111</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232557</th>\n",
       "      <td>2145285907</td>\n",
       "      <td>113511</td>\n",
       "      <td>2019-05-01 23:39:24</td>\n",
       "      <td>s8884531</td>\n",
       "      <td>Main</td>\n",
       "      <td>eds</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232558</th>\n",
       "      <td>2145339713</td>\n",
       "      <td>81262</td>\n",
       "      <td>2019-05-01 22:15:29</td>\n",
       "      <td>s9003448</td>\n",
       "      <td>main</td>\n",
       "      <td>eds</td>\n",
       "      <td>111</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232559</th>\n",
       "      <td>2145535802</td>\n",
       "      <td>30646</td>\n",
       "      <td>2019-05-04 19:59:01</td>\n",
       "      <td>s9000228</td>\n",
       "      <td>main</td>\n",
       "      <td>ehost</td>\n",
       "      <td>411</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232560</th>\n",
       "      <td>2145535802</td>\n",
       "      <td>30774</td>\n",
       "      <td>2019-05-04 19:59:34</td>\n",
       "      <td>s9000228</td>\n",
       "      <td>main</td>\n",
       "      <td>ehost</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232561</th>\n",
       "      <td>2145535802</td>\n",
       "      <td>30648</td>\n",
       "      <td>2019-05-04 19:59:01</td>\n",
       "      <td>s9000228</td>\n",
       "      <td>main</td>\n",
       "      <td>ehost</td>\n",
       "      <td>410</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232562</th>\n",
       "      <td>2145535802</td>\n",
       "      <td>30647</td>\n",
       "      <td>2019-05-04 19:59:01</td>\n",
       "      <td>s9000228</td>\n",
       "      <td>main</td>\n",
       "      <td>ehost</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232563</th>\n",
       "      <td>2145535802</td>\n",
       "      <td>30702</td>\n",
       "      <td>2019-05-04 19:59:16</td>\n",
       "      <td>s9000228</td>\n",
       "      <td>main</td>\n",
       "      <td>ehost</td>\n",
       "      <td>123</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232564</th>\n",
       "      <td>2146229003</td>\n",
       "      <td>30688</td>\n",
       "      <td>2019-05-04 19:59:12</td>\n",
       "      <td>s8875834</td>\n",
       "      <td>main</td>\n",
       "      <td>ehost</td>\n",
       "      <td>111</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232565</th>\n",
       "      <td>2146286657</td>\n",
       "      <td>43642</td>\n",
       "      <td>2018-10-29 00:39:00</td>\n",
       "      <td>s5842106</td>\n",
       "      <td>main</td>\n",
       "      <td>ehost</td>\n",
       "      <td>111</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232566</th>\n",
       "      <td>2146286657</td>\n",
       "      <td>43645</td>\n",
       "      <td>2018-10-29 00:39:59</td>\n",
       "      <td>s5842106</td>\n",
       "      <td>main</td>\n",
       "      <td>ehost</td>\n",
       "      <td>121</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232567</th>\n",
       "      <td>2146286657</td>\n",
       "      <td>43647</td>\n",
       "      <td>2018-10-29 00:41:28</td>\n",
       "      <td>s5842106</td>\n",
       "      <td>main</td>\n",
       "      <td>ehost</td>\n",
       "      <td>112</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232568</th>\n",
       "      <td>2146286657</td>\n",
       "      <td>43643</td>\n",
       "      <td>2018-10-29 00:39:08</td>\n",
       "      <td>s5842106</td>\n",
       "      <td>main</td>\n",
       "      <td>ehost</td>\n",
       "      <td>311</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232569</th>\n",
       "      <td>2146314735</td>\n",
       "      <td>8330</td>\n",
       "      <td>2019-05-04 18:32:56</td>\n",
       "      <td>s8989685</td>\n",
       "      <td>main</td>\n",
       "      <td>eds</td>\n",
       "      <td>111</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232570</th>\n",
       "      <td>2146314735</td>\n",
       "      <td>8328</td>\n",
       "      <td>2019-05-04 18:32:56</td>\n",
       "      <td>s8989685</td>\n",
       "      <td>main</td>\n",
       "      <td>eds</td>\n",
       "      <td>117</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232571</th>\n",
       "      <td>2146314735</td>\n",
       "      <td>8352</td>\n",
       "      <td>2019-05-04 18:33:02</td>\n",
       "      <td>s8989685</td>\n",
       "      <td>main</td>\n",
       "      <td>eds</td>\n",
       "      <td>116</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232572</th>\n",
       "      <td>2146314735</td>\n",
       "      <td>8334</td>\n",
       "      <td>2019-05-04 18:32:57</td>\n",
       "      <td>s8989685</td>\n",
       "      <td>main</td>\n",
       "      <td>eds</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232573</th>\n",
       "      <td>2146596552</td>\n",
       "      <td>31382</td>\n",
       "      <td>2019-05-02 23:25:58</td>\n",
       "      <td>s8989984</td>\n",
       "      <td>Test</td>\n",
       "      <td>eitws</td>\n",
       "      <td>111</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232574</th>\n",
       "      <td>2146743041</td>\n",
       "      <td>55053</td>\n",
       "      <td>2019-05-01 21:14:43</td>\n",
       "      <td>s8875834</td>\n",
       "      <td>main</td>\n",
       "      <td>ehost</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232575</th>\n",
       "      <td>2146743041</td>\n",
       "      <td>55003</td>\n",
       "      <td>2019-05-01 21:14:34</td>\n",
       "      <td>s8875834</td>\n",
       "      <td>main</td>\n",
       "      <td>ehost</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232576</th>\n",
       "      <td>2146743041</td>\n",
       "      <td>54841</td>\n",
       "      <td>2019-05-01 21:14:15</td>\n",
       "      <td>s8875834</td>\n",
       "      <td>main</td>\n",
       "      <td>ehost</td>\n",
       "      <td>111</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232577</th>\n",
       "      <td>2147184577</td>\n",
       "      <td>58744</td>\n",
       "      <td>2019-04-21 22:28:41</td>\n",
       "      <td>s8447892</td>\n",
       "      <td>main</td>\n",
       "      <td>ehost</td>\n",
       "      <td>311</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232578</th>\n",
       "      <td>2147184577</td>\n",
       "      <td>58743</td>\n",
       "      <td>2019-04-21 22:28:09</td>\n",
       "      <td>s8447892</td>\n",
       "      <td>main</td>\n",
       "      <td>ehost</td>\n",
       "      <td>111</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232579</th>\n",
       "      <td>2147184577</td>\n",
       "      <td>58745</td>\n",
       "      <td>2019-04-21 22:28:57</td>\n",
       "      <td>s8447892</td>\n",
       "      <td>main</td>\n",
       "      <td>ehost</td>\n",
       "      <td>121</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232580</th>\n",
       "      <td>2147318729</td>\n",
       "      <td>12977</td>\n",
       "      <td>2019-05-04 18:48:13</td>\n",
       "      <td>s8989685</td>\n",
       "      <td>main</td>\n",
       "      <td>ehost</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232581</th>\n",
       "      <td>2147318729</td>\n",
       "      <td>12980</td>\n",
       "      <td>2019-05-04 18:48:13</td>\n",
       "      <td>s8989685</td>\n",
       "      <td>main</td>\n",
       "      <td>ehost</td>\n",
       "      <td>111</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232582</th>\n",
       "      <td>2147318729</td>\n",
       "      <td>12971</td>\n",
       "      <td>2019-05-04 18:48:13</td>\n",
       "      <td>s8989685</td>\n",
       "      <td>main</td>\n",
       "      <td>ehost</td>\n",
       "      <td>117</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>232583 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         SessionNo  level_1             LogTime    CustID GroupID    ProfID  \\\n",
       "0      -2147481927    78744 2019-05-01 22:09:42  s9003448    main       eds   \n",
       "1      -2147360137    59038 2019-04-23 00:41:52  s8986718    main  iconnweb   \n",
       "2      -2147360137    59039 2019-04-23 00:41:52  s8986718    main  iconnweb   \n",
       "3      -2147360137    59037 2019-04-23 00:41:39  s8986718    main  iconnweb   \n",
       "4      -2147317281   118033 2019-05-01 23:55:06  s8989685    main    edsapi   \n",
       "5      -2147317281   118446 2019-05-01 23:56:29  s8989685    main    edsapi   \n",
       "6      -2147317281   117986 2019-05-01 23:54:56  s8989685    main    edsapi   \n",
       "7      -2147317281   118004 2019-05-01 23:54:59  s8989685    main    edsapi   \n",
       "8      -2147002735    28950 2019-05-04 19:52:22  s8989984    main     ehost   \n",
       "9      -2147002735    29635 2019-05-04 19:54:52  s8989984    main     ehost   \n",
       "10     -2147002735    29706 2019-05-04 19:55:05  s8989984    main     ehost   \n",
       "11     -2147002735    29182 2019-05-04 19:53:26  s8989984    main     ehost   \n",
       "12     -2147002735    29136 2019-05-04 19:53:17  s8989984    main     ehost   \n",
       "13     -2146953899    13977 2019-05-01 19:33:28  s8875834    main   novplus   \n",
       "14     -2146953899    19155 2019-05-01 19:45:46  s8875834    main   novplus   \n",
       "15     -2146953899     9564 2019-05-01 19:23:22  s8875834    main   novplus   \n",
       "16     -2146953899     6125 2019-05-01 19:15:18  s8875834    main   novplus   \n",
       "17     -2146953899     5887 2019-05-01 19:14:39  s8875834    main   novplus   \n",
       "18     -2146953899    21599 2019-05-01 19:52:40  s8875834    main   novplus   \n",
       "19     -2146953899    13591 2019-05-01 19:32:33  s8875834    main   novplus   \n",
       "20     -2146953899    14338 2019-05-01 19:34:16  s8875834    main   novplus   \n",
       "21     -2146953899    15015 2019-05-01 19:35:43  s8875834    main   novplus   \n",
       "22     -2146953899     6087 2019-05-01 19:15:12  s8875834    main   novplus   \n",
       "23     -2146953899    38238 2019-05-01 20:35:15  s8875834    main   novplus   \n",
       "24     -2146953899    24503 2019-05-01 19:59:54  s8875834    main   novplus   \n",
       "25     -2146953899      864 2019-05-01 19:02:12  s8875834    main   novplus   \n",
       "26     -2146953899    22332 2019-05-01 19:54:29  s8875834    main   novplus   \n",
       "27     -2146953899    14244 2019-05-01 19:34:05  s8875834    main   novplus   \n",
       "28     -2146953899     4215 2019-05-01 19:10:15  s8875834    main   novplus   \n",
       "29     -2146953899    22164 2019-05-01 19:54:07  s8875834    main   novplus   \n",
       "...            ...      ...                 ...       ...     ...       ...   \n",
       "232553  2145203170   108365 2019-05-01 23:21:33  s8990848    main     ehost   \n",
       "232554  2145285907   113497 2019-05-01 23:39:23  s8884531    Main       eds   \n",
       "232555  2145285907   113509 2019-05-01 23:39:24  s8884531    Main       eds   \n",
       "232556  2145285907   113500 2019-05-01 23:39:23  s8884531    Main       eds   \n",
       "232557  2145285907   113511 2019-05-01 23:39:24  s8884531    Main       eds   \n",
       "232558  2145339713    81262 2019-05-01 22:15:29  s9003448    main       eds   \n",
       "232559  2145535802    30646 2019-05-04 19:59:01  s9000228    main     ehost   \n",
       "232560  2145535802    30774 2019-05-04 19:59:34  s9000228    main     ehost   \n",
       "232561  2145535802    30648 2019-05-04 19:59:01  s9000228    main     ehost   \n",
       "232562  2145535802    30647 2019-05-04 19:59:01  s9000228    main     ehost   \n",
       "232563  2145535802    30702 2019-05-04 19:59:16  s9000228    main     ehost   \n",
       "232564  2146229003    30688 2019-05-04 19:59:12  s8875834    main     ehost   \n",
       "232565  2146286657    43642 2018-10-29 00:39:00  s5842106    main     ehost   \n",
       "232566  2146286657    43645 2018-10-29 00:39:59  s5842106    main     ehost   \n",
       "232567  2146286657    43647 2018-10-29 00:41:28  s5842106    main     ehost   \n",
       "232568  2146286657    43643 2018-10-29 00:39:08  s5842106    main     ehost   \n",
       "232569  2146314735     8330 2019-05-04 18:32:56  s8989685    main       eds   \n",
       "232570  2146314735     8328 2019-05-04 18:32:56  s8989685    main       eds   \n",
       "232571  2146314735     8352 2019-05-04 18:33:02  s8989685    main       eds   \n",
       "232572  2146314735     8334 2019-05-04 18:32:57  s8989685    main       eds   \n",
       "232573  2146596552    31382 2019-05-02 23:25:58  s8989984    Test     eitws   \n",
       "232574  2146743041    55053 2019-05-01 21:14:43  s8875834    main     ehost   \n",
       "232575  2146743041    55003 2019-05-01 21:14:34  s8875834    main     ehost   \n",
       "232576  2146743041    54841 2019-05-01 21:14:15  s8875834    main     ehost   \n",
       "232577  2147184577    58744 2019-04-21 22:28:41  s8447892    main     ehost   \n",
       "232578  2147184577    58743 2019-04-21 22:28:09  s8447892    main     ehost   \n",
       "232579  2147184577    58745 2019-04-21 22:28:57  s8447892    main     ehost   \n",
       "232580  2147318729    12977 2019-05-04 18:48:13  s8989685    main     ehost   \n",
       "232581  2147318729    12980 2019-05-04 18:48:13  s8989685    main     ehost   \n",
       "232582  2147318729    12971 2019-05-04 18:48:13  s8989685    main     ehost   \n",
       "\n",
       "        Act  BadActor  \n",
       "0       111         0  \n",
       "1       112         1  \n",
       "2       311         1  \n",
       "3       111         1  \n",
       "4       121         0  \n",
       "5       121         0  \n",
       "6       111         0  \n",
       "7       121         0  \n",
       "8       121         0  \n",
       "9       116         0  \n",
       "10      115         0  \n",
       "11      115         0  \n",
       "12      121         0  \n",
       "13      121         0  \n",
       "14      121         0  \n",
       "15      115         0  \n",
       "16      115         0  \n",
       "17      115         0  \n",
       "18      121         0  \n",
       "19      115         0  \n",
       "20      115         0  \n",
       "21      121         0  \n",
       "22      115         0  \n",
       "23      112         0  \n",
       "24      115         0  \n",
       "25      115         0  \n",
       "26      115         0  \n",
       "27      115         0  \n",
       "28      115         0  \n",
       "29      121         0  \n",
       "...     ...       ...  \n",
       "232553  115         0  \n",
       "232554  117         0  \n",
       "232555  115         0  \n",
       "232556  111         0  \n",
       "232557  115         0  \n",
       "232558  111         0  \n",
       "232559  411         0  \n",
       "232560  115         0  \n",
       "232561  410         0  \n",
       "232562  121         0  \n",
       "232563  123         0  \n",
       "232564  111         0  \n",
       "232565  111         1  \n",
       "232566  121         1  \n",
       "232567  112         1  \n",
       "232568  311         1  \n",
       "232569  111         0  \n",
       "232570  117         0  \n",
       "232571  116         0  \n",
       "232572  115         0  \n",
       "232573  111         0  \n",
       "232574  115         0  \n",
       "232575  121         0  \n",
       "232576  111         0  \n",
       "232577  311         1  \n",
       "232578  111         1  \n",
       "232579  121         1  \n",
       "232580  121         0  \n",
       "232581  111         0  \n",
       "232582  117         0  \n",
       "\n",
       "[232583 rows x 8 columns]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[name,df]=txngs[0]\n",
    "\n",
    "df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading out/train.csv to sagemaker-mlai-harvesting\n",
      "None\n",
      "Uploading out/test.csv to sagemaker-mlai-harvesting\n",
      "None\n",
      "Uploading out/validate.csv to sagemaker-mlai-harvesting\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "!mkdir out\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "bucket = \"sagemaker-mlai-harvesting\"\n",
    "\n",
    "for i, df in enumerate(dfs):\n",
    "    files = [\"train\",\"test\",\"validate\"]\n",
    "    file = \"out/{}.csv\".format(files[i])\n",
    "    df.to_csv(path_or_buf= file, header=False, index=False  )\n",
    "\n",
    "    print(\"Uploading {} to {}\".format(file, bucket))\n",
    "\n",
    "    response = s3_client.upload_file(file, bucket, file)\n",
    "    print(response)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare and train a model\n",
    "Boilerplate code mostly copied from Amazon sample code at https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/xgboost_abalone/xgboost_abalone.ipynb, with ample room for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training job harvesting-xgboost-binary-classification2019-06-21-19-33-25\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "Completed\n",
      "CPU times: user 77.3 ms, sys: 4.05 ms, total: 81.4 ms\n",
      "Wall time: 3min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "region = 'us-east-1'\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container = get_image_uri(region, 'xgboost')\n",
    "\n",
    "\n",
    "from time import gmtime, strftime\n",
    "\n",
    "job_name = 'harvesting-xgboost-binary-classification' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(\"Training job\", job_name)\n",
    "\n",
    "#Ensure that the training and validation data folders generated above are reflected in the \"InputDataConfig\" parameter below.\n",
    "\n",
    "create_training_params = \\\n",
    "{\n",
    "    \"AlgorithmSpecification\": {\n",
    "        \"TrainingImage\": container,\n",
    "        \"TrainingInputMode\": \"File\"\n",
    "    },\n",
    "    \"RoleArn\": role,\n",
    "    \"OutputDataConfig\": {\n",
    "        \"S3OutputPath\": os.path.join(\"s3://\",bucket, \"out\", \"xgb-class\") \n",
    "    },\n",
    "    \"ResourceConfig\": {\n",
    "        \"InstanceCount\": 1,\n",
    "        \"InstanceType\": \"ml.m4.4xlarge\",\n",
    "        \"VolumeSizeInGB\": 5\n",
    "    },\n",
    "    \"TrainingJobName\": job_name,\n",
    "    \"HyperParameters\": {\n",
    "        \"max_depth\":\"5\",\n",
    "        \"eta\":\"0.2\",\n",
    "        \"gamma\":\"4\",\n",
    "        \"min_child_weight\":\"6\",\n",
    "        \"subsample\":\"0.7\",\n",
    "        \"silent\":\"0\",\n",
    "        \"objective\":\"binary:logistic\",\n",
    "        \"num_round\":\"50\"\n",
    "    },\n",
    "    \"StoppingCondition\": {\n",
    "        \"MaxRuntimeInSeconds\": 3600\n",
    "    },\n",
    "    \"InputDataConfig\": [\n",
    "        {\n",
    "            \"ChannelName\": \"train\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": \"s3://sagemaker-mlai-harvesting/out/train.csv\" , \n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"text/csv\",\n",
    "            \"CompressionType\": \"None\"\n",
    "        },\n",
    "        {\n",
    "            \"ChannelName\": \"validation\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": \"s3://sagemaker-mlai-harvesting/out/validate.csv\" ,\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"text/csv\",\n",
    "            \"CompressionType\": \"None\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "client = boto3.client('sagemaker', region_name=region)\n",
    "client.create_training_job(**create_training_params)\n",
    "\n",
    "import time\n",
    "\n",
    "status = client.describe_training_job(TrainingJobName=job_name)['TrainingJobStatus']\n",
    "print(status)\n",
    "while status !='Completed' and status!='Failed':\n",
    "    time.sleep(60)\n",
    "    status = client.describe_training_job(TrainingJobName=job_name)['TrainingJobStatus']\n",
    "    print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "harvesting-xgboost-binary-class2019-06-21-19-33-25-model\n",
      "s3://sagemaker-mlai-harvesting/out/xgb-class/harvesting-xgboost-binary-classification2019-06-21-19-33-25/output/model.tar.gz\n"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "An error occurred (ValidationException) when calling the CreateModel operation: Cannot create already existing model \"arn:aws:sagemaker:us-east-1:872344130825:model/harvesting-xgboost-binary-class2019-06-21-19-33-25-model\".",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    356\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    659\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (ValidationException) when calling the CreateModel operation: Cannot create already existing model \"arn:aws:sagemaker:us-east-1:872344130825:model/harvesting-xgboost-binary-class2019-06-21-19-33-25-model\"."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import boto3\n",
    "from time import gmtime, strftime\n",
    "\n",
    "model_name=\"harvesting-xgboost-binary-class2019-06-21-19-33-25\"+ '-model'\n",
    "print(model_name)\n",
    "\n",
    "info = client.describe_training_job(TrainingJobName=job_name)\n",
    "model_data = info['ModelArtifacts']['S3ModelArtifacts']\n",
    "print(model_data)\n",
    "\n",
    "primary_container = {\n",
    "    'Image': container,\n",
    "    'ModelDataUrl': model_data\n",
    "}\n",
    "\n",
    "create_model_response = client.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    PrimaryContainer = primary_container)\n",
    "\n",
    "print(create_model_response['ModelArn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harvest-XGBoostEndpointConfig-2019-06-21-19-39-54\n",
      "Endpoint Config Arn: arn:aws:sagemaker:us-east-1:872344130825:endpoint-config/harvest-xgboostendpointconfig-2019-06-21-19-39-54\n"
     ]
    }
   ],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "endpoint_config_name = 'Harvest-XGBoostEndpointConfig-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(endpoint_config_name)\n",
    "create_endpoint_config_response = client.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType':'ml.m4.xlarge',\n",
    "        'InitialVariantWeight':1,\n",
    "        'InitialInstanceCount':1,\n",
    "        'ModelName':model_name,\n",
    "        'VariantName':'AllTraffic'}])\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response['EndpointConfigArn'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launch an endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harvest-XGBoostEndpoint-2019-06-21-19-40-24\n",
      "arn:aws:sagemaker:us-east-1:872344130825:endpoint/harvest-xgboostendpoint-2019-06-21-19-40-24\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: InService\n",
      "Arn: arn:aws:sagemaker:us-east-1:872344130825:endpoint/harvest-xgboostendpoint-2019-06-21-19-40-24\n",
      "Status: InService\n",
      "CPU times: user 142 ms, sys: 283 µs, total: 142 ms\n",
      "Wall time: 8min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import time\n",
    "\n",
    "endpoint_name = 'Harvest-XGBoostEndpoint-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(endpoint_name)\n",
    "create_endpoint_response = client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name)\n",
    "print(create_endpoint_response['EndpointArn'])\n",
    "\n",
    "resp = client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp['EndpointStatus']\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status=='Creating':\n",
    "    time.sleep(60)\n",
    "    resp = client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp['EndpointStatus']\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp['EndpointArn'])\n",
    "print(\"Status: \" + status)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model\n",
    "Currently, we launch an endpoint to test the model. This endpoint includes a simple web service that takes POST request with rows of or model's X values - columns other than BadActor - and returns a corresponding list of Y values - BadActor predictions.\n",
    "\n",
    "The endpoint approach is most suitable to interactive use, such as possibly using the model to blacklist a harvesting session as soon as it is identified. For offline analysis, this should be reconfigured to run batch transform jobs instead, which are cheaper to run and more streamlined to invoke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_client = boto3.client('runtime.sagemaker', region_name=region)\n",
    "\n",
    "import json\n",
    "from itertools import islice\n",
    "import math\n",
    "import struct\n",
    "\n",
    "!head -10000 out/test.csv > out/single-test.csv\n",
    "\n",
    "file_name = 'out/single-test.csv' \n",
    "\n",
    "# file_name = \"out/may8.csv\"\n",
    "\n",
    "\n",
    "csv = pd.read_csv(file_name, header=None)\n",
    "csv.columns\n",
    "label = csv[0]\n",
    "csv = csv.drop(0,axis=1)\n",
    "\n",
    "single = \"out/single.csv\"\n",
    "\n",
    "csv.to_csv(path_or_buf=single, header=False, index=False)\n",
    "\n",
    "with open(single, 'r') as f:\n",
    "    payload = f.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = runtime_client.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                   ContentType='text/csv', \n",
    "                                   Body=payload)\n",
    "result = response['Body'].read()\n",
    "result = result.decode(\"utf-8\")\n",
    "result = result.split(',')\n",
    "result = [round(float(i)) for i in result]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the confusion metrics\n",
    "\n",
    "A confusion matrix describes the proportions of true and false positives and negatives, together with some derived metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.968754320475598 precision: 0.9665454545454546 recall 0.8807157057654076\n"
     ]
    }
   ],
   "source": [
    "comp = pd.concat( [label, pd.DataFrame(result)], axis = 1)\n",
    "comp.columns =[\"label\",'prediction']\n",
    "\n",
    "label_positive = comp['label'] == 1\n",
    "predict_positive = comp['prediction'] == 1\n",
    "\n",
    "tp = len( comp[label_positive & predict_positive])\n",
    "fp = len( comp[~label_positive & predict_positive])\n",
    "tn = len( comp[~label_positive & ~predict_positive])\n",
    "fn = len( comp[label_positive & ~predict_positive])\n",
    "m = len(comp)\n",
    "\n",
    "accuracy = (tp+tn)/m\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "\n",
    "print(\"accuracy: {} precision: {} recall {}\".format(accuracy, precision,recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1329, 46, 5678, 180, 7233)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp,fp,tn,fn, len(comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V1 results\n",
    "accuracy: 0.9657853810264385 precision: 1.0 recall 0.8360655737704918\n",
    "\n",
    "|tp|fp|tn|fn|len(comp)|\n",
    "|--|--|--|--|---------|\n",
    "|1683| 0| 7632| 330|9645|\n",
    "\n",
    "The very first time we ran the model, we achieved strikingly successful rates of harvesting identification.\n",
    "The most significant number here is the recall of 84%, meaning that we successfully identified 84% of all harvesting sessions by looking only at counts of transaction types.\n",
    "\n",
    "\n",
    "# V2 results, with txns + wait stats:\n",
    "accuracy: 0.968754320475598 precision: 0.9665454545454546 recall 0.8807157057654076\n",
    "\n",
    "|tp|fp|tn|fn|len(comp)|\n",
    "|--|--|--|--|---------|\n",
    "|1329| 46| 5678| 180|7233|\n",
    "\n",
    "In the second run, we see an increased recall, at the price of a small number of false positives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further investigating the data \n",
    "\n",
    "We had additional ideas for modeling the data while staying in this bag-of-transaction technique.\n",
    "1. Try some hyperparameter tuning to seem if the success rates can be trivially improved.\n",
    "1. Enrich the training data set in various ways - add colums to summarize total session time, average time/request, and so on.\n",
    "1. Perform some clustering analysis to try to identify common patterns of behavior other than LT. This may reveal the presence of other kinds of harvesting.\n",
    "\n",
    "## Qualifying the approach\n",
    "Can we use this approach to identify and blacklist harvesting sessions as they occur? Some notes:\n",
    "1. The approach must be resilient to easy efforts to evade. Does the accuracy of the identification drop if the attacker makes minor changes to his workflow?\n",
    "1. How long does it take to identify an attacker in real time? \n",
    "    1. Do we gain certainty soon enough to stop an attacker before he's done what he came to do?\n",
    "    2. Can we tag sessions accurately after the first N log entries, for instance?\n",
    "    \n",
    "## Designing an implemetation\n",
    "Design an architecture for identifying and intercepting harvesting activity in real time. Confirm data sources, manage impact to usage latency, model costs and ROI.\n",
    "\n",
    "In today's world, it would be less effective to perform real-time analysis on AWS, since all of our current content usage is on-prem. The algorithm used here, XGBoost, is performant on commodity hardware, so we may be able to run on standard VMs.\n",
    "\n",
    "In real-time analysis, we will face a stream of events from interleaved sessions. We will have to demultiplex these into individual event streams both for training and for prediction, implying some kind of windowing to capture and send sets of log entries as partial sessions. It's not clear how big the impact of this windowing will be on the accuracy of the models.\n",
    "\n",
    "# Other analytical techniques\n",
    "While this algorithm seems promising, we're throwing away a huge amount of intelligence before we start training, in the name of simplicity. We can evaluate what kind of gains we could achieve through more advanced techniques:\n",
    "- Stateful models like LSTM or CNN\n",
    "- more \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
