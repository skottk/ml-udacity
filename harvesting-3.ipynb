{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harvesting @ MLAI Training - Third Round Overview\n",
    "\n",
    "At this point in the series of experiments, we're going to try some divergent techniques. I'd like to try some tactics that will improve the fit of the model to continuous use in the live application flow. Right now, the model trains on retrospective data sets and more or less expects to see full sessions. In this experiment, I'm going to try several approaches.\n",
    "\n",
    "## Roadmap\n",
    "### Feature engineering and ML algorithms\n",
    "\n",
    "1. Instead of counting full sessions in the training datasets, we will only take sequences of a maximum N, to see if there's a useful threshold where we could evaluate sliding windows of session history. I'll compare recall at 5, 10, 15, 20 element session subsequences, offset randomly from session start. \n",
    "    - Where do the metrics drop off? \n",
    "    - Does the approach essentially require full sessions?\n",
    "    - Can we query partial sessions against a model trained on full sessions?\n",
    "    - Can we query partial sessions against a model trained on partial sessions?\n",
    "    - Randomly drop out D% of transactions from every session.\n",
    "1. Instead of using bag-of-txns, I will try a vector that contains the last N sessions by number, indexed from a transaction dictionary. This will force sequence into the model.\n",
    "    - Training on the first N transactions:\n",
    "        - Count N transactions from the beginning of a session.\n",
    "    - Train on N-gram tiles\n",
    "    - Train on N-gram shingles\n",
    "1. Instead of training an XGBoost model, train neural network models that are designed to remember sequences\n",
    "    - LSTM\n",
    "    - Convolutional (?)\n",
    "    \n",
    "### Software engineering\n",
    "To this point, I've mostly worked out of a some-what scattered Jupyter notebook pile of global variables, tuning up a few functions here and there to keep scopes clean. In particular, the Sagemaker code is terrible cut-and paste. As I work through the feature engineering & algorithmic roadmap, I also want to drive toward the following engineering goals:\n",
    "\n",
    "- Establish a consistent data manipulation pipeline that is easily customizable and reentrant, with no global variables\n",
    "- a set of wrapper functions for the Sagemaker API, so that I can have a simple pipeline where training jobs -> models -> batch transformations or endpoints, without giant string constant parameters that are impossible to edit or sight-check\n",
    "- Automatically engage a hyperparameter tuning job as desired without changing how the model is implemented or trained.\n",
    "- Easily training many models at once\n",
    "- Easily launch many batch transform jobs/endpoints at once.\n",
    "- Easily display the results of the many jobs at once.\n",
    "- Particular feature engineering processes glide smoothly from experimental tinkering to code that can be put into production, without throwing everything away. Some of this just falls out of doing everything else better.\n",
    "\n",
    "# Training Data\n",
    "The training dataset consists of several hours of raw transaction logs containing activity from all users, with the full collection of harvesting activity from the LiquidTension harvester across two years. All LiquidTension(LT) activity is labeled as 'BadActor' = 1, while all other traffic is assumed to be innocent and labeled as 'BadActor' = 0. Since LiquidTension is currently our only easily-identified single harvester, we need his full range of activity to have a BadActor sessions in proportion to innocent sessions for training to work properly.\n",
    "\n",
    "The training set includes the following files:\n",
    "\n",
    "|File       |Contents                             |Rows|\n",
    "------------|-------------------------------------|----|\n",
    "|may1.tsv|raw transactions|119474|\n",
    "|may2.tsv|raw transactions|43608|\n",
    "|may3.tsv|raw transactions|30844|\n",
    "|lt-only.tsv|raw transactions for a known attacker|61917|\n",
    "\n",
    "In all, we have 193926 transactions from \"innocent\" sessions and 61917 LT sessions. Approximately 32% of transactions are labeled BadActor = 1, giving us a reasonable proportion in both classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence testing phase 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm up\n",
    "Import standard libraries and prepare the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data’: File exists\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "import toolz\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sagemaker session, role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# S3 bucket name\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "# common column names\n",
    "bad_col='BadActor'\n",
    "sess_col='SessionNo'\n",
    "txn_col='Act'\n",
    "logtime_col = 'LogTime'\n",
    "\n",
    "# paths\n",
    "csv_path = \"out\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download\n",
    "Retrieve the datafiles from the project's designated S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "b = s3.Bucket('sagemaker-mlai-harvesting')\n",
    "\n",
    "# b.download_file( 'data/MLAI_ParsedDataSet.tsv', 'data/data.tsv')\n",
    "b.download_file( \"data/MinimalLogs/Minimal_May01.rpt\", 'data/may1.tsv')\n",
    "b.download_file( \"data/MinimalLogs/Minimal_May02.rpt\", 'data/may2.tsv')\n",
    "b.download_file( \"data/MinimalLogs/Minimal_May03.rpt\", 'data/may3.tsv')\n",
    "b.download_file( \"data/MinimalLogs/Minimal_OnlyLT.rpt\", 'data/lt-only.tsv')\n",
    "\n",
    "\n",
    "may1 = pd.read_csv('data/may1.tsv',sep='\\t')\n",
    "may2 = pd.read_csv('data/may2.tsv',sep='\\t')\n",
    "may3 = pd.read_csv('data/may3.tsv',sep='\\t')\n",
    "lt = pd.read_csv('data/lt-only.tsv',sep='\\t')\n",
    "\n",
    "\n",
    "\n",
    "txn = may1.append([may2, may3, lt])\n",
    "txn[logtime_col] = pd.to_datetime(txn[logtime_col])\n",
    "# txn[txn[bad_col]==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data conversion and feature engineering\n",
    "In real life, a session consists of a series of rows of transactions of different types, and each transaction type records a variable number of additional metadata attributes describing a logged event, for a total of over 30 columns of extracted data. In addition, our tagging process has given each row a BadActor label.\n",
    "\n",
    "|sessionno|txn id|BadActor|parm1|parm2|...|\n",
    "|---------|------|--------|-----|-----|---|\n",
    "|1240|111|0|query string|...|...|\n",
    "|1240|112|0|meta|...|...|\n",
    "|2993|301|1|meta|...|...|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Innocent' log entries\n",
    "txns = pd.DataFrame(np.sort(txn['Act'].unique()))\n",
    "\n",
    "# Harvesting log entries\n",
    "lt_txns = pd.DataFrame(np.sort(lt['Act'].unique()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We drop most of this information, including the temporal sequence of the log entries, and convert each session into a single row of data. Almost all of the columns go away, replaced by counts of transaction types in the session.\n",
    "\n",
    "|sessionno|BadActor|111|112|113|...|301|302|...|\n",
    "|---------|--------|---|---|---|---|---|---|---|\n",
    "|1240|0|1|1|0|...|0|0|...|\n",
    "|2993|1|0|0|0|...|1|0|...|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Truncating sessions\n",
    "Before we flatten the sessions, we're going to truncate them. This may be a better match for the real world, in which at best we will be able to scan sliding windows of transactions with a scaling resumption that we may not scan every event.\n",
    "\n",
    "We'll try several approaches at once:\n",
    "- dropping out D% of transactions from every session\n",
    "- taking only the first N sessions from every session\n",
    "- taking on the last N transactions from every session\n",
    "- taking N consecutive transactions from the middle of every session. \n",
    "- choosing N transactions as above, but dropping every session without at least N transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas.core.groupby.generic.DataFrameGroupBy object at 0x7fd6ba1db5c0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def drop_column_groups( txn_c ):\n",
    "    txn_c.columns = txn_c.columns.droplevel(0)  \n",
    "    txn_c.rename_axis(None, axis=1).reset_index()\n",
    "    return txn_c\n",
    "\n",
    "def get_session_groups( txn ):\n",
    "    txn_g = txn.groupby(sess_col)\n",
    "    return txn_g\n",
    "\n",
    "txng = get_session_groups(txn)\n",
    "txng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
      "Wall time: 7.63 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def drop_pct( df, n = .1 ):\n",
    "    return df.sample(frac= 1-n)\n",
    "\n",
    "def first_n( df, n = 5):\n",
    "    return df.head( n )\n",
    "    \n",
    "def last_n( df, n = 5):\n",
    "    return df.tail( n )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grappl( fun, parm ):\n",
    "    return lambda df: df.apply(fun, parm).drop(sess_col, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_txns( txn_log ):\n",
    "    txn_narrow = txn_log[[sess_col, txn_col,bad_col]]\n",
    "    txn_pivot = pd.pivot_table(txn_narrow, index=[sess_col,bad_col], columns = [txn_col],aggfunc=[len]).fillna(0)\n",
    "    txn_pivot.columns = txn_pivot.columns.droplevel(0)           # the pivot table has a two-level index\n",
    "    txn_flat = txn_pivot.rename_axis(None, axis=1).reset_index() # these two lines get rid of it so we have a simple table\n",
    "    return txn_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_groups( txn_log ):\n",
    "    txn_narrow = txn_log[[txn_col,bad_col]]\n",
    "    txn_pivot = pd.pivot_table(txn_narrow, index=[sess_col,bad_col], columns = [txn_col],aggfunc=[len]).fillna(0)\n",
    "    txn_pivot.columns = txn_pivot.columns.droplevel(0)           # the pivot table has a two-level index\n",
    "    txn_flat = txn_pivot.rename_axis(None, axis=1).reset_index() # these two lines get rid of it so we have a simple table\n",
    "    return txn_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('drop10', <function __main__.drop_pct(df, n=0.1)>, 0.1),\n",
       " ('first5', <function __main__.first_n(df, n=5)>, 5),\n",
       " ('last5', <function __main__.last_n(df, n=5)>, 5)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs = [['drop10', [drop_pct, .1]], \n",
    "        ['first5', [first_n, 5]],\n",
    "        ['last5', [last_n, 5]]\n",
    "       ]\n",
    "\n",
    "[(name, func, parm) for [name, [func, parm]] in jobs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_jobs(df, jobs):\n",
    "    groups = [[name, grappl(fun, parm)(df)] for [name, [fun, parm]] in jobs]\n",
    "\n",
    "    flats = [[name,flatten_groups( txn )] for [name,txn] in groups] #.reset_index()\n",
    "    return flats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 40s, sys: 535 ms, total: 1min 41s\n",
      "Wall time: 1min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "gs = prep_jobs(txng, jobs )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('drop10', 24114), ('first5', 24112), ('last5', 24112)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txngs= gs\n",
    "[(name,len(df)) for [name,df] in txngs]\n",
    "# gs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Producing pools of training and testing data\n",
    "\n",
    "In order to support simultaneous execution of multiple jobs, this notebook introduces a new scheme for piping data through to models.\n",
    "\n",
    "The normal flow runs as follows:\n",
    "\n",
    "input data(s3) -> df's on notebook instance -> train.csv, test.csv, validate.csv on notebook instance -> s3/out/ -> Sagemaker instances\n",
    "\n",
    "Hardcoding these filenames is fine for playing around in a notebook, but it limits us to one job at a time.\n",
    "\n",
    "In this approach, every job has a base Name. This name will carry through from S3 into the Sagemaker instances.\n",
    "Training data files will reside in `<s3bucket>/<key>/out/Name`.\n",
    "\n",
    "As before, in each `Name` subfolder, we will divide the combined good and bad data pools as follows:\n",
    "- a training set that the model iterates over during the learning process\n",
    "- a test set that is used to evaluate the model during training\n",
    "- a validation set that is kept separate to test the model after training is complete. We need separate test and validate pools in order to make sure that we're not overfitting the model to a single set of test data.\n",
    "\n",
    "All of these functions are in the FrameSplitter class. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'lib.JupHelper.JupHelper' from '/home/ec2-user/SageMaker/mlai-harvesting/lib/JupHelper/JupHelper.py'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import lib.JupHelper.JupHelper as jh\n",
    "reload(jh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib.JupHelper.JupHelper as jh\n",
    "\n",
    "csv = jh.FrameSplitter( bad_col, [sess_col]) # FrameSplitter only holds onto the definition of the y_col and the x_cols - everything else is passed in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv.make_all_csvs( gs )           # Drives the whole conversion process - look in the class for other helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "csvs = csv.get_all_csv_names(gs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload to S3 and move into SageMaker\n",
    "\n",
    "Move all of the current csv's up into S3 for SageMaker, then start configuring jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "class SageHelper:\n",
    "    def __init__(self, bucket):\n",
    "        self.bucket = bucket\n",
    "        self.s3_client = boto3.client('s3')\n",
    "    \n",
    "    def s3_upload(self, file, s3_path=\"\", verbose = True):\n",
    "        target = os.path.join(s3_path, file) \n",
    "        if verbose:\n",
    "            print( \"Uploading {} to {}://{}\".format(file, bucket, target))\n",
    "        response = self.s3_client.upload_file( file, bucket, target )\n",
    "        if verbose:\n",
    "            print( response )\n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading out/drop10_train.csv to sagemaker-us-east-1-872344130825://out/drop10_train.csv\n",
      "None\n",
      "Uploading out/drop10_test.csv to sagemaker-us-east-1-872344130825://out/drop10_test.csv\n",
      "None\n",
      "Uploading out/drop10_validate.csv to sagemaker-us-east-1-872344130825://out/drop10_validate.csv\n",
      "None\n",
      "Uploading out/first5_train.csv to sagemaker-us-east-1-872344130825://out/first5_train.csv\n",
      "None\n",
      "Uploading out/first5_test.csv to sagemaker-us-east-1-872344130825://out/first5_test.csv\n",
      "None\n",
      "Uploading out/first5_validate.csv to sagemaker-us-east-1-872344130825://out/first5_validate.csv\n",
      "None\n",
      "Uploading out/last5_train.csv to sagemaker-us-east-1-872344130825://out/last5_train.csv\n",
      "None\n",
      "Uploading out/last5_test.csv to sagemaker-us-east-1-872344130825://out/last5_test.csv\n",
      "None\n",
      "Uploading out/last5_validate.csv to sagemaker-us-east-1-872344130825://out/last5_validate.csv\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "sh = SageHelper( bucket )\n",
    "for file in csvs:\n",
    "    sh.s3_upload( file )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare and train a model\n",
    "Boilerplate code mostly copied from Amazon sample code at https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/xgboost_abalone/xgboost_abalone.ipynb, with ample room for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "region = 'us-east-1'\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container = get_image_uri(region, 'xgboost')\n",
    "\n",
    "\n",
    "from time import gmtime, strftime\n",
    "\n",
    "job_name = 'harvesting-xgboost-binary-classification' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(\"Training job\", job_name)\n",
    "\n",
    "#Ensure that the training and validation data folders generated above are reflected in the \"InputDataConfig\" parameter below.\n",
    "\n",
    "create_training_params = \\\n",
    "{\n",
    "    \"AlgorithmSpecification\": {\n",
    "        \"TrainingImage\": container,\n",
    "        \"TrainingInputMode\": \"File\"\n",
    "    },\n",
    "    \"RoleArn\": role,\n",
    "    \"OutputDataConfig\": {\n",
    "        \"S3OutputPath\": os.path.join(\"s3://\",bucket, \"out\", \"xgb-class\") \n",
    "    },\n",
    "    \"ResourceConfig\": {\n",
    "        \"InstanceCount\": 1,\n",
    "        \"InstanceType\": \"ml.m4.4xlarge\",\n",
    "        \"VolumeSizeInGB\": 5\n",
    "    },\n",
    "    \"TrainingJobName\": job_name,\n",
    "    \"HyperParameters\": {\n",
    "        \"max_depth\":\"5\",\n",
    "        \"eta\":\"0.2\",\n",
    "        \"gamma\":\"4\",\n",
    "        \"min_child_weight\":\"6\",\n",
    "        \"subsample\":\"0.7\",\n",
    "        \"silent\":\"0\",\n",
    "        \"objective\":\"binary:logistic\",\n",
    "        \"num_round\":\"50\"\n",
    "    },\n",
    "    \"StoppingCondition\": {\n",
    "        \"MaxRuntimeInSeconds\": 3600\n",
    "    },\n",
    "    \"InputDataConfig\": [\n",
    "        {\n",
    "            \"ChannelName\": \"train\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": \"s3://sagemaker-mlai-harvesting/out/train.csv\" , \n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"text/csv\",\n",
    "            \"CompressionType\": \"None\"\n",
    "        },\n",
    "        {\n",
    "            \"ChannelName\": \"validation\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": \"s3://sagemaker-mlai-harvesting/out/validate.csv\" ,\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"text/csv\",\n",
    "            \"CompressionType\": \"None\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "client = boto3.client('sagemaker', region_name=region)\n",
    "client.create_training_job(**create_training_params)\n",
    "\n",
    "import time\n",
    "\n",
    "status = client.describe_training_job(TrainingJobName=job_name)['TrainingJobStatus']\n",
    "print(status)\n",
    "while status !='Completed' and status!='Failed':\n",
    "    time.sleep(60)\n",
    "    status = client.describe_training_job(TrainingJobName=job_name)['TrainingJobStatus']\n",
    "    print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import boto3\n",
    "from time import gmtime, strftime\n",
    "\n",
    "model_name=\"harvesting-xgboost-binary-class2019-06-21-19-33-25\"+ '-model'\n",
    "print(model_name)\n",
    "\n",
    "info = client.describe_training_job(TrainingJobName=job_name)\n",
    "model_data = info['ModelArtifacts']['S3ModelArtifacts']\n",
    "print(model_data)\n",
    "\n",
    "primary_container = {\n",
    "    'Image': container,\n",
    "    'ModelDataUrl': model_data\n",
    "}\n",
    "\n",
    "create_model_response = client.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    PrimaryContainer = primary_container)\n",
    "\n",
    "print(create_model_response['ModelArn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "endpoint_config_name = 'Harvest-XGBoostEndpointConfig-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(endpoint_config_name)\n",
    "create_endpoint_config_response = client.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType':'ml.m4.xlarge',\n",
    "        'InitialVariantWeight':1,\n",
    "        'InitialInstanceCount':1,\n",
    "        'ModelName':model_name,\n",
    "        'VariantName':'AllTraffic'}])\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response['EndpointConfigArn'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launch an endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "\n",
    "endpoint_name = 'Harvest-XGBoostEndpoint-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(endpoint_name)\n",
    "create_endpoint_response = client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name)\n",
    "print(create_endpoint_response['EndpointArn'])\n",
    "\n",
    "resp = client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp['EndpointStatus']\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status=='Creating':\n",
    "    time.sleep(60)\n",
    "    resp = client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp['EndpointStatus']\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp['EndpointArn'])\n",
    "print(\"Status: \" + status)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model\n",
    "Currently, we launch an endpoint to test the model. This endpoint includes a simple web service that takes POST request with rows of or model's X values - columns other than BadActor - and returns a corresponding list of Y values - BadActor predictions.\n",
    "\n",
    "The endpoint approach is most suitable to interactive use, such as possibly using the model to blacklist a harvesting session as soon as it is identified. For offline analysis, this should be reconfigured to run batch transform jobs instead, which are cheaper to run and more streamlined to invoke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_client = boto3.client('runtime.sagemaker', region_name=region)\n",
    "\n",
    "import json\n",
    "from itertools import islice\n",
    "import math\n",
    "import struct\n",
    "\n",
    "!head -10000 out/test.csv > out/single-test.csv\n",
    "\n",
    "file_name = 'out/single-test.csv' \n",
    "\n",
    "# file_name = \"out/may8.csv\"\n",
    "\n",
    "\n",
    "csv = pd.read_csv(file_name, header=None)\n",
    "csv.columns\n",
    "label = csv[0]\n",
    "csv = csv.drop(0,axis=1)\n",
    "\n",
    "single = \"out/single.csv\"\n",
    "\n",
    "csv.to_csv(path_or_buf=single, header=False, index=False)\n",
    "\n",
    "with open(single, 'r') as f:\n",
    "    payload = f.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = runtime_client.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                   ContentType='text/csv', \n",
    "                                   Body=payload)\n",
    "result = response['Body'].read()\n",
    "result = result.decode(\"utf-8\")\n",
    "result = result.split(',')\n",
    "result = [round(float(i)) for i in result]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the confusion metrics\n",
    "\n",
    "A confusion matrix describes the proportions of true and false positives and negatives, together with some derived metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = pd.concat( [label, pd.DataFrame(result)], axis = 1)\n",
    "comp.columns =[\"label\",'prediction']\n",
    "\n",
    "label_positive = comp['label'] == 1\n",
    "predict_positive = comp['prediction'] == 1\n",
    "\n",
    "tp = len( comp[label_positive & predict_positive])\n",
    "fp = len( comp[~label_positive & predict_positive])\n",
    "tn = len( comp[~label_positive & ~predict_positive])\n",
    "fn = len( comp[label_positive & ~predict_positive])\n",
    "m = len(comp)\n",
    "\n",
    "accuracy = (tp+tn)/m\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "\n",
    "print(\"accuracy: {} precision: {} recall {}\".format(accuracy, precision,recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp,fp,tn,fn, len(comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V1 results\n",
    "accuracy: 0.9657853810264385 precision: 1.0 recall 0.8360655737704918\n",
    "\n",
    "|tp|fp|tn|fn|len(comp)|\n",
    "|--|--|--|--|---------|\n",
    "|1683| 0| 7632| 330|9645|\n",
    "\n",
    "The very first time we ran the model, we achieved strikingly successful rates of harvesting identification.\n",
    "The most significant number here is the recall of 84%, meaning that we successfully identified 84% of all harvesting sessions by looking only at counts of transaction types.\n",
    "\n",
    "\n",
    "# V2 results, with txns + wait stats:\n",
    "accuracy: 0.968754320475598 precision: 0.9665454545454546 recall 0.8807157057654076\n",
    "\n",
    "|tp|fp|tn|fn|len(comp)|\n",
    "|--|--|--|--|---------|\n",
    "|1329| 46| 5678| 180|7233|\n",
    "\n",
    "In the second run, we see an increased recall, at the price of a small number of false positives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further investigating the data \n",
    "\n",
    "We had additional ideas for modeling the data while staying in this bag-of-transaction technique.\n",
    "1. Try some hyperparameter tuning to seem if the success rates can be trivially improved.\n",
    "1. Enrich the training data set in various ways - add colums to summarize total session time, average time/request, and so on.\n",
    "1. Perform some clustering analysis to try to identify common patterns of behavior other than LT. This may reveal the presence of other kinds of harvesting.\n",
    "\n",
    "## Qualifying the approach\n",
    "Can we use this approach to identify and blacklist harvesting sessions as they occur? Some notes:\n",
    "1. The approach must be resilient to easy efforts to evade. Does the accuracy of the identification drop if the attacker makes minor changes to his workflow?\n",
    "1. How long does it take to identify an attacker in real time? \n",
    "    1. Do we gain certainty soon enough to stop an attacker before he's done what he came to do?\n",
    "    2. Can we tag sessions accurately after the first N log entries, for instance?\n",
    "    \n",
    "## Designing an implemetation\n",
    "Design an architecture for identifying and intercepting harvesting activity in real time. Confirm data sources, manage impact to usage latency, model costs and ROI.\n",
    "\n",
    "In today's world, it would be less effective to perform real-time analysis on AWS, since all of our current content usage is on-prem. The algorithm used here, XGBoost, is performant on commodity hardware, so we may be able to run on standard VMs.\n",
    "\n",
    "In real-time analysis, we will face a stream of events from interleaved sessions. We will have to demultiplex these into individual event streams both for training and for prediction, implying some kind of windowing to capture and send sets of log entries as partial sessions. It's not clear how big the impact of this windowing will be on the accuracy of the models.\n",
    "\n",
    "# Other analytical techniques\n",
    "While this algorithm seems promising, we're throwing away a huge amount of intelligence before we start training, in the name of simplicity. We can evaluate what kind of gains we could achieve through more advanced techniques:\n",
    "- Stateful models like LSTM or CNN\n",
    "- more \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
