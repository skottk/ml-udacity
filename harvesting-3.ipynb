{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harvesting @ MLAI Training - Third Round Overview\n",
    "\n",
    "At this point in the series of experiments, we're going to try some divergent techniques. I'd like to try some tactics that will improve the fit of the model to continuous use in the live application flow. Right now, the model trains on retrospective data sets and more or less expects to see full sessions. In this experiment, I'm going to try several approaches.\n",
    "\n",
    "## Roadmap\n",
    "### Feature engineering and ML algorithms\n",
    "\n",
    "1. Instead of counting full sessions in the training datasets, we will only take sequences of a maximum N, to see if there's a useful threshold where we could evaluate sliding windows of session history. I'll compare recall at 5, 10, 15, 20 element session subsequences, offset randomly from session start. \n",
    "    - Where do the metrics drop off? \n",
    "    - Does the approach essentially require full sessions?\n",
    "    - Can we query partial sessions against a model trained on full sessions?\n",
    "    - Can we query partial sessions against a model trained on partial sessions?\n",
    "    - Randomly drop out D% of transactions from every session.\n",
    "1. Instead of using bag-of-txns, I will try a vector that contains the last N sessions by number, indexed from a transaction dictionary. This will force sequence into the model.\n",
    "    - Training on the first N transactions:\n",
    "        - Count N transactions from the beginning of a session.\n",
    "    - Train on N-gram tiles\n",
    "    - Train on N-gram shingles\n",
    "1. Instead of training an XGBoost model, train neural network models that are designed to remember sequences\n",
    "    - LSTM\n",
    "    - Convolutional (?)\n",
    "    \n",
    "### Software engineering\n",
    "To this point, I've mostly worked out of a some-what scattered Jupyter notebook pile of global variables, tuning up a few functions here and there to keep scopes clean. In particular, the Sagemaker code is terrible cut-and paste. As I work through the feature engineering & algorithmic roadmap, I also want to drive toward the following engineering goals:\n",
    "\n",
    "- Establish a consistent data manipulation pipeline that is easily customizable and reentrant, with no global variables\n",
    "- a set of wrapper functions for the Sagemaker API, so that I can have a simple pipeline where training jobs -> models -> batch transformations or endpoints, without giant string constant parameters that are impossible to edit or sight-check\n",
    "- Automatically engage a hyperparameter tuning job as desired without changing how the model is implemented or trained.\n",
    "- Easily training many models at once\n",
    "- Easily launch many batch transform jobs/endpoints at once.\n",
    "- Easily display the results of the many jobs at once.\n",
    "- Particular feature engineering processes glide smoothly from experimental tinkering to code that can be put into production, without throwing everything away. Some of this just falls out of doing everything else better.\n",
    "\n",
    "# Training Data\n",
    "The training dataset consists of several hours of raw transaction logs containing activity from all users, with the full collection of harvesting activity from the LiquidTension harvester across two years. All LiquidTension(LT) activity is labeled as 'BadActor' = 1, while all other traffic is assumed to be innocent and labeled as 'BadActor' = 0. Since LiquidTension is currently our only easily-identified single harvester, we need his full range of activity to have a BadActor sessions in proportion to innocent sessions for training to work properly.\n",
    "\n",
    "The training set includes the following files:\n",
    "\n",
    "|File       |Contents                             |Rows|\n",
    "------------|-------------------------------------|----|\n",
    "|may1.tsv|raw transactions|119474|\n",
    "|may2.tsv|raw transactions|43608|\n",
    "|may3.tsv|raw transactions|30844|\n",
    "|lt-only.tsv|raw transactions for a known attacker|61917|\n",
    "\n",
    "In all, we have 193926 transactions from \"innocent\" sessions and 61917 LT sessions. Approximately 32% of transactions are labeled BadActor = 1, giving us a reasonable proportion in both classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence testing phase 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm up\n",
    "Import standard libraries and prepare the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data’: File exists\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "import toolz\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 bucket name\n",
    "bucket = 'sagemaker-mlai-harvesting'\n",
    "\n",
    "# common column names\n",
    "bad_col='BadActor'\n",
    "sess_col='SessionNo'\n",
    "txn_col='Act'\n",
    "logtime_col = 'LogTime'\n",
    "\n",
    "# paths\n",
    "csv_path = \"out\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download\n",
    "Retrieve the datafiles from the project's designated S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "b = s3.Bucket('sagemaker-mlai-harvesting')\n",
    "\n",
    "# b.download_file( 'data/MLAI_ParsedDataSet.tsv', 'data/data.tsv')\n",
    "b.download_file( \"data/MinimalLogs/Minimal_May01.rpt\", 'data/may1.tsv')\n",
    "b.download_file( \"data/MinimalLogs/Minimal_May02.rpt\", 'data/may2.tsv')\n",
    "b.download_file( \"data/MinimalLogs/Minimal_May03.rpt\", 'data/may3.tsv')\n",
    "b.download_file( \"data/MinimalLogs/Minimal_OnlyLT.rpt\", 'data/lt-only.tsv')\n",
    "\n",
    "\n",
    "may1 = pd.read_csv('data/may1.tsv',sep='\\t')\n",
    "may2 = pd.read_csv('data/may2.tsv',sep='\\t')\n",
    "may3 = pd.read_csv('data/may3.tsv',sep='\\t')\n",
    "lt = pd.read_csv('data/lt-only.tsv',sep='\\t')\n",
    "\n",
    "\n",
    "\n",
    "txn = may1.append([may2, may3, lt])\n",
    "txn[logtime_col] = pd.to_datetime(txn[logtime_col])\n",
    "# txn[txn[bad_col]==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data conversion and feature engineering\n",
    "In real life, a session consists of a series of rows of transactions of different types, and each transaction type records a variable number of additional metadata attributes describing a logged event, for a total of over 30 columns of extracted data. In addition, our tagging process has given each row a BadActor label.\n",
    "\n",
    "|sessionno|txn id|BadActor|parm1|parm2|...|\n",
    "|---------|------|--------|-----|-----|---|\n",
    "|1240|111|0|query string|...|...|\n",
    "|1240|112|0|meta|...|...|\n",
    "|2993|301|1|meta|...|...|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Innocent' log entries\n",
    "txns = pd.DataFrame(np.sort(txn['Act'].unique()))\n",
    "\n",
    "# Harvesting log entries\n",
    "lt_txns = pd.DataFrame(np.sort(lt['Act'].unique()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We drop most of this information, including the temporal sequence of the log entries, and convert each session into a single row of data. Almost all of the columns go away, replaced by counts of transaction types in the session.\n",
    "\n",
    "|sessionno|BadActor|111|112|113|...|301|302|...|\n",
    "|---------|--------|---|---|---|---|---|---|---|\n",
    "|1240|0|1|1|0|...|0|0|...|\n",
    "|2993|1|0|0|0|...|1|0|...|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Truncating sessions\n",
    "Before we flatten the sessions, we're going to truncate them. This may be a better match for the real world, in which at best we will be able to scan sliding windows of transactions with a scaling resumption that we may not scan every event.\n",
    "\n",
    "We'll try several approaches at once:\n",
    "- dropping out D% of transactions from every session\n",
    "- taking only the first N sessions from every session\n",
    "- taking on the last N transactions from every session\n",
    "- taking N consecutive transactions from the middle of every session. \n",
    "- choosing N transactions as above, but dropping every session without at least N transactions.\n",
    "\n",
    "# DOOF!\n",
    "\n",
    "## **Truncating the sessions reduces the total population of txn types, which reduces the number of columns in the output datasets. Need to make sure we force the same columns across all experiments, or we can't test when we get to the end.**\n",
    "\n",
    "## It also ought to be a lot easier to run the whole thing from beginning to end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas.core.groupby.generic.DataFrameGroupBy object at 0x7fd6ba1db5c0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def drop_column_groups( txn_c ):\n",
    "    txn_c.columns = txn_c.columns.droplevel(0)  \n",
    "    txn_c.rename_axis(None, axis=1).reset_index()\n",
    "    return txn_c\n",
    "\n",
    "def get_session_groups( txn ):\n",
    "    txn_g = txn.groupby(sess_col)\n",
    "    return txn_g\n",
    "\n",
    "txng = get_session_groups(txn)\n",
    "txng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
      "Wall time: 7.63 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def drop_pct( df, n = .1 ):\n",
    "    return df.sample(frac= 1-n)\n",
    "\n",
    "def first_n( df, n = 5):\n",
    "    return df.head( n )\n",
    "    \n",
    "def last_n( df, n = 5):\n",
    "    return df.tail( n )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grappl( fun, parm ):\n",
    "    return lambda df: df.apply(fun, parm).drop(sess_col, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_txns( txn_log ):\n",
    "    '''\n",
    "    On a flat list of sessions, run a pivot table on transaction type counts by session, and eliminate extraneous columns.\n",
    "    Flatten the pivot table and simplify the index.\n",
    "    '''\n",
    "    txn_narrow = txn_log[[sess_col, txn_col,bad_col]]\n",
    "    txn_pivot = pd.pivot_table(txn_narrow, index=[sess_col,bad_col], columns = [txn_col],aggfunc=[len]).fillna(0)\n",
    "    txn_pivot.columns = txn_pivot.columns.droplevel(0)           # the pivot table has a two-level index\n",
    "    txn_flat = txn_pivot.rename_axis(None, axis=1).reset_index() # these two lines get rid of it so we have a simple table\n",
    "    return txn_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_groups( txn_log ):\n",
    "    '''\n",
    "    On a set of session groups, run a pivot table on transaction type counts by session, and eliminate extraneous columns.\n",
    "    Flatten all groups into one table and simplify the index.\n",
    "    '''\n",
    "    txn_narrow = txn_log[[txn_col,bad_col]] # for groups, don't need to drop the session column because it's already an index column.\n",
    "    txn_pivot = pd.pivot_table(txn_narrow, index=[sess_col,bad_col], columns = [txn_col],aggfunc=[len]).fillna(0)\n",
    "    txn_pivot.columns = txn_pivot.columns.droplevel(0)           # the pivot table has a two-level index\n",
    "    txn_flat = txn_pivot.rename_axis(None, axis=1).reset_index() # these two lines get rid of it so we have a simple table\n",
    "    return txn_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('drop10', <function __main__.drop_pct(df, n=0.1)>, 0.1),\n",
       " ('first5', <function __main__.first_n(df, n=5)>, 5),\n",
       " ('last5', <function __main__.last_n(df, n=5)>, 5)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs = [['drop10', [drop_pct, .1]], \n",
    "        ['first5', [first_n, 5]],\n",
    "        ['last5', [last_n, 5]]\n",
    "       ]\n",
    "job_names = [job[0] for job in jobs]\n",
    "[(name, func, parm) for [name, [func, parm]] in jobs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_jobs(df, jobs):\n",
    "    groups = [[name, grappl(fun, parm)(df)] for [name, [fun, parm]] in jobs]\n",
    "\n",
    "    flats = [[name,flatten_groups( txn )] for [name,txn] in groups] #.reset_index()\n",
    "    return flats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 40s, sys: 535 ms, total: 1min 41s\n",
      "Wall time: 1min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "gs = prep_jobs(txng, jobs )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('drop10', 24114, 38), ('first5', 24112, 35), ('last5', 24112, 37)]"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txngs= gs\n",
    "[(name,len(df), len(df.columns)) for [name,df] in txngs]\n",
    "# gs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Producing pools of training and testing data\n",
    "\n",
    "In order to support simultaneous execution of multiple jobs, this notebook introduces a new scheme for piping data through to models.\n",
    "\n",
    "The normal flow runs as follows:\n",
    "\n",
    "input data(s3) -> df's on notebook instance -> train.csv, test.csv, validate.csv on notebook instance -> s3/out/ -> Sagemaker instances\n",
    "\n",
    "Hardcoding these filenames is fine for playing around in a notebook, but it limits us to one job at a time.\n",
    "\n",
    "In this approach, every job has a base Name. This name will carry through from S3 into the Sagemaker instances.\n",
    "Training data files will reside in `<s3bucket>/<key>/out/Name`.\n",
    "\n",
    "As before, in each `Name` subfolder, we will divide the combined good and bad data pools as follows:\n",
    "- a training set that the model iterates over during the learning process\n",
    "- a test set that is used to evaluate the model during training\n",
    "- a validation set that is kept separate to test the model after training is complete. We need separate test and validate pools in order to make sure that we're not overfitting the model to a single set of test data.\n",
    "\n",
    "All of these functions are in the FrameSplitter class. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'lib.JupHelper.JupHelper' from '/home/ec2-user/SageMaker/mlai-harvesting/lib/JupHelper/JupHelper.py'>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import lib.JupHelper.JupHelper as jh\n",
    "reload(jh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib.JupHelper.JupHelper as jh\n",
    "\n",
    "csv = jh.FrameSplitter( bad_col, [sess_col]) # FrameSplitter only holds onto the definition of the y_col and the x_cols - everything else is passed in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_csvs = csv.make_all_csvs( gs )           # Drives the whole conversion process - look in the class for other helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvs = csv.get_all_csv_names(gs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload to S3 and move into SageMaker\n",
    "\n",
    "Move all of the current csv's up into S3 for SageMaker, then start configuring jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'lib.JupHelper.JupHelper' from '/home/ec2-user/SageMaker/mlai-harvesting/lib/JupHelper/JupHelper.py'>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import lib.JupHelper.JupHelper as jh\n",
    "reload(jh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "import boto3\n",
    "from time import gmtime, strftime\n",
    "import time\n",
    "\n",
    "class XgbHyper:\n",
    "    def __init__(self):\n",
    "        self.params = {\n",
    "            \"max_depth\":\"5\",\n",
    "            \"eta\":\"0.2\",\n",
    "            \"gamma\":\"4\",\n",
    "            \"min_child_weight\":\"6\",\n",
    "            \"subsample\":\"0.7\",\n",
    "            \"silent\":\"0\",\n",
    "            \"objective\":\"binary:logistic\",\n",
    "            \"num_round\":\"50\"\n",
    "        }\n",
    "\n",
    "class TrainRunner:\n",
    "    def __init__( self, prefix, region='us-east-1' ):\n",
    "        '''\n",
    "        Wrapper object to run multiple \n",
    "        '''\n",
    "        self.prefix = prefix\n",
    "        self.region = region\n",
    "\n",
    "        self.running_jobs = []\n",
    "        self.models = []\n",
    "        self.sg_client = boto3.client('sagemaker', region_name=region )\n",
    "        # sagemaker session, role\n",
    "        self.sagemaker_session = sagemaker.Session()\n",
    "        self.role = sagemaker.get_execution_role()\n",
    "    \n",
    "    def start_jobs(self, jobs, bucket ):\n",
    "        for (name, csvs) in jobs:\n",
    "            job_name = self.make_job_name( name )\n",
    "            print( job_name )\n",
    "            self.start_training_job( bucket, job_name, csvs )\n",
    "            self.running_jobs.append([name,job_name])\n",
    "        return self.running_jobs\n",
    "    \n",
    "    def clear_jobs(self):\n",
    "        self.running_jobs = []\n",
    "        self.models = []\n",
    "        \n",
    "        #delete endpoints\n",
    "        #delete configs\n",
    "    \n",
    "    def start_training_job(self, bucket, job_name, s3_inputs, image='xgboost', instance = [1, \"ml.m4.4xlarge\", 5 ], hyper=XgbHyper(), verbose=True):\n",
    "        self.image = image\n",
    "        self.s3_input = s3_inputs\n",
    "        self.instance = instance\n",
    "        self.hyper = hyper\n",
    "        self.bucket = bucket\n",
    "\n",
    "        self.container = get_image_uri( self.region, 'xgboost' )\n",
    "        job_config = self.make_job_config( job_name, s3_inputs[0], s3_inputs[1], s3_inputs[2])\n",
    "        res = self.launch_training_job( job_config )\n",
    "        if verbose:\n",
    "            print( \"Started {}. Response: {}\".format( job_name, res))\n",
    "            \n",
    "        return res\n",
    "    \n",
    "    def make_s3_url(self, file):\n",
    "        return \"s3://{}/{}\".format(self.bucket, file)\n",
    "    \n",
    "    def get_job_status(self, job):\n",
    "        [name, job_name] = job\n",
    "        return self.sg_client.describe_training_job(TrainingJobName=job_name)['TrainingJobStatus']\n",
    "    \n",
    "    def check_jobs_still_running( self ):\n",
    "        still_running = False\n",
    "        for job in self.running_jobs:\n",
    "            status = get_job_status( job )\n",
    "            if status !='Completed' and status !='Failed':\n",
    "                still_running = True\n",
    "                break\n",
    "        \n",
    "        return still_running\n",
    "    \n",
    "    def map_jobs( self, func ):\n",
    "        for job in self.running_jobs:\n",
    "            func( job )\n",
    "    \n",
    "    def print_job_status( self, job ):\n",
    "        [name, job_name] = job\n",
    "        \n",
    "        print(\"{}: {}\".format(job, self.get_job_status(job) )  )\n",
    "        \n",
    "    def trace_jobs( self ):\n",
    "        self.map_jobs( self.print_job_status )\n",
    "                    \n",
    "    def make_job_name( self, job_name ):\n",
    "        name = \"{}-{}-{}\".format( self.prefix, job_name, strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime()))\n",
    "        return name\n",
    "            \n",
    "    def launch_training_job( self, job_config ):\n",
    "        self.sg_client.create_training_job( **job_config )\n",
    "    \n",
    "    def wait_for_jobs( self ):\n",
    "        while True:\n",
    "            print(\"Checking job statuses:\")\n",
    "            self.trace_jobs()\n",
    "            if not self.check_jobs_still_running():\n",
    "                break\n",
    "            time.sleep(15)\n",
    "\n",
    "    def get_model_name(self, name):\n",
    "        return name + \"-model\"\n",
    "    \n",
    "    def get_endpoint_config_name(self, name):\n",
    "        return name + \"-config\"\n",
    "     \n",
    "    def get_endpoint_name(self, name):\n",
    "        return name + \"-endpoint\"\n",
    "\n",
    "    def create_models( self ):\n",
    "        for [name, job_name] in self.running_jobs:\n",
    "            info = self.sg_client.describe_training_job(TrainingJobName=job_name)\n",
    "            model_data = info['ModelArtifacts']['S3ModelArtifacts']\n",
    "            primary_container = {\n",
    "                'Image': self.container,\n",
    "                'ModelDataUrl': model_data\n",
    "            }\n",
    "            res = self.sg_client.create_model(\n",
    "                ModelName = self. get_model_name(job_name),\n",
    "                ExecutionRoleArn = role,\n",
    "                PrimaryContainer = primary_container)\n",
    "            print(res['ModelArn'])\n",
    "            self.models.append( res )\n",
    "            \n",
    "    def create_endpoint_configs(self, job_name ):\n",
    "        endpoint_config_name = self.get_endpoint_config_name(job_name)\n",
    "        model_name = self.get_model_name( job_name )\n",
    "        print(endpoint_config_name)\n",
    "        res = self.sg_client.create_endpoint_config(\n",
    "            EndpointConfigName = endpoint_config_name,\n",
    "            ProductionVariants=[{\n",
    "                'InstanceType':'ml.m4.xlarge',\n",
    "                'InitialVariantWeight':1,\n",
    "                'InitialInstanceCount':1,\n",
    "                'ModelName':model_name,\n",
    "                'VariantName':'AllTraffic'}])\n",
    "        print(\"Endpoint Config Arn: \" + res['EndpointConfigArn'])\n",
    "        \n",
    "    def create_endpoints(self):\n",
    "        for [name, job_name] in self.running_jobs:\n",
    "            self.create_endpoint_configs(job_name)\n",
    "            endpoint_name = self.get_endpoint_name(job_name)\n",
    "            print(endpoint_name)\n",
    "            res = self.sg_client.create_endpoint(\n",
    "                EndpointName=endpoint_name,\n",
    "                EndpointConfigName=self.get_endpoint_config_name( job_name ))\n",
    "            print(res['EndpointArn'])\n",
    "\n",
    "    def wait_for_endpoints(self):\n",
    "        still_creating = False\n",
    "            \n",
    "        while still_creating:\n",
    "            still_creating = False\n",
    "\n",
    "            print(\"Checking endpoint statuses:\")\n",
    "\n",
    "            for [name, job_name] in self.running_jobs:\n",
    "                resp = self.sg_client.describe_endpoint(EndpointName=get_endpoint_name( job_name ))\n",
    "                status = resp['EndpointStatus']\n",
    "                if status == 'Creating':\n",
    "                    still_creating = True\n",
    "                else:\n",
    "                    print(\"Arn: \" + resp['EndpointArn'])\n",
    "                \n",
    "                print( \"{}: {}\".format( job_name, status))\n",
    "                    \n",
    "            time.sleep(30)\n",
    "        \n",
    "        print( \"All endpoints created.\")\n",
    "        \n",
    "    def test_model(self, job_name, csv):\n",
    "        '''\n",
    "        job_name - name of a job that has run all the way through to an endpoint\n",
    "        csv - a csv with the same y_col and x_col structure as the training data\n",
    "        returns - a single-column dataframe with the predictions from the model.\n",
    "        '''\n",
    "        endpoint_name = self.get_endpoint_name( job_name )\n",
    "        with open(csv, 'r') as f:\n",
    "            payload = f.read().strip()\n",
    "        response = self.sg_client.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                   ContentType='text/csv', \n",
    "                                   Body=payload)\n",
    "        result = response['Body'].read()\n",
    "        result = result.decode(\"utf-8\")\n",
    "        result = result.split(',')\n",
    "        result = [round(float(i)) for i in result]\n",
    "        return pd.DataFrame( result )\n",
    "            \n",
    "    def make_job_config(self, job_name, train, test, val ):\n",
    "        return {\n",
    "        \"AlgorithmSpecification\": {\n",
    "            \"TrainingImage\": self.container,\n",
    "            \"TrainingInputMode\": \"File\"\n",
    "        },\n",
    "        \"RoleArn\": self.role,\n",
    "        \"OutputDataConfig\": {\n",
    "            \"S3OutputPath\": os.path.join(\"s3://\", self.bucket, \"out\", \"xgb-class\") \n",
    "        },\n",
    "        \"ResourceConfig\": {\n",
    "            \"InstanceCount\": self.instance[0],\n",
    "            \"InstanceType\": self.instance[1],\n",
    "            \"VolumeSizeInGB\": self.instance[2]\n",
    "        },\n",
    "        \"TrainingJobName\": job_name,\n",
    "        \"HyperParameters\": self.hyper.params,        \n",
    "        \"StoppingCondition\": {\n",
    "            \"MaxRuntimeInSeconds\": 3600\n",
    "        },\n",
    "        \"InputDataConfig\": [\n",
    "            {\n",
    "                \"ChannelName\": \"train\",\n",
    "                \"DataSource\": {\n",
    "                    \"S3DataSource\": {\n",
    "                        \"S3DataType\": \"S3Prefix\",\n",
    "                        \"S3Uri\": self.make_s3_url( train ), # \"s3://sagemaker-mlai-harvesting/out/train.csv\" , \n",
    "                        \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                    }\n",
    "                },\n",
    "                \"ContentType\": \"text/csv\",\n",
    "                \"CompressionType\": \"None\"\n",
    "            },\n",
    "            {\n",
    "                \"ChannelName\": \"test\",\n",
    "                \"DataSource\": {\n",
    "                    \"S3DataSource\": {\n",
    "                        \"S3DataType\": \"S3Prefix\",\n",
    "                        \"S3Uri\": self.make_s3_url( test ), # \"s3://sagemaker-mlai-harvesting/out/test.csv\" , \n",
    "                        \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                    }\n",
    "                },\n",
    "                \"ContentType\": \"text/csv\",\n",
    "                \"CompressionType\": \"None\"\n",
    "            },\n",
    "            {\n",
    "                \"ChannelName\": \"validation\",\n",
    "                \"DataSource\": {\n",
    "                    \"S3DataSource\": {\n",
    "                        \"S3DataType\": \"S3Prefix\",\n",
    "                        \"S3Uri\": self.make_s3_url( val ), # \"s3://sagemaker-mlai-harvesting/out/validate.csv\" ,\n",
    "                        \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                    }\n",
    "                },\n",
    "                \"ContentType\": \"text/csv\",\n",
    "                \"CompressionType\": \"None\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "class SageHelper:\n",
    "    def __init__(self, bucket, image, region='us-east-1'):\n",
    "        self.bucket = bucket\n",
    "        self.image = image\n",
    "        self.container = get_image_uri(region, self.image)\n",
    "        self.s3_client = boto3.client('s3')\n",
    "    \n",
    "    def s3_upload(self, file, s3_path=\"\", verbose = True):\n",
    "        target = os.path.join(s3_path, file) \n",
    "        if verbose:\n",
    "            print( \"Uploading {} to s3://{}/{}\".format(file, bucket, target))\n",
    "        response = self.s3_client.upload_file( file, bucket, target )\n",
    "        if verbose:\n",
    "            print( response )\n",
    "        return response\n",
    "    \n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "sh = SageHelper( bucket, 'xgboost' )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for (job, csvs ) in job_csvs:\n",
    "    print( \"Uploading files for {}\".format(job))\n",
    "    for file in csvs:\n",
    "        sh.s3_upload( file )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare and train a model\n",
    "Boilerplate code mostly copied from Amazon sample code at https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/xgboost_abalone/xgboost_abalone.ipynb, with ample room for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr= TrainRunner(\"harvest-xgb-bc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "harvest-xgb-bc-drop10-2019-07-09-19-53-25\n",
      "Started harvest-xgb-bc-drop10-2019-07-09-19-53-25. Response: None\n",
      "harvest-xgb-bc-first5-2019-07-09-19-53-25\n",
      "Started harvest-xgb-bc-first5-2019-07-09-19-53-25. Response: None\n",
      "harvest-xgb-bc-last5-2019-07-09-19-53-25\n",
      "Started harvest-xgb-bc-last5-2019-07-09-19-53-25. Response: None\n",
      "Checking job statuses:\n",
      "['drop10', 'harvest-xgb-bc-drop10-2019-07-09-19-53-25']: InProgress\n",
      "['first5', 'harvest-xgb-bc-first5-2019-07-09-19-53-25']: InProgress\n",
      "['last5', 'harvest-xgb-bc-last5-2019-07-09-19-53-25']: InProgress\n",
      "Checking job statuses:\n",
      "['drop10', 'harvest-xgb-bc-drop10-2019-07-09-19-53-25']: InProgress\n",
      "['first5', 'harvest-xgb-bc-first5-2019-07-09-19-53-25']: InProgress\n",
      "['last5', 'harvest-xgb-bc-last5-2019-07-09-19-53-25']: InProgress\n",
      "Checking job statuses:\n",
      "['drop10', 'harvest-xgb-bc-drop10-2019-07-09-19-53-25']: InProgress\n",
      "['first5', 'harvest-xgb-bc-first5-2019-07-09-19-53-25']: InProgress\n",
      "['last5', 'harvest-xgb-bc-last5-2019-07-09-19-53-25']: InProgress\n",
      "Checking job statuses:\n",
      "['drop10', 'harvest-xgb-bc-drop10-2019-07-09-19-53-25']: InProgress\n",
      "['first5', 'harvest-xgb-bc-first5-2019-07-09-19-53-25']: InProgress\n",
      "['last5', 'harvest-xgb-bc-last5-2019-07-09-19-53-25']: InProgress\n",
      "Checking job statuses:\n",
      "['drop10', 'harvest-xgb-bc-drop10-2019-07-09-19-53-25']: InProgress\n",
      "['first5', 'harvest-xgb-bc-first5-2019-07-09-19-53-25']: InProgress\n",
      "['last5', 'harvest-xgb-bc-last5-2019-07-09-19-53-25']: InProgress\n",
      "Checking job statuses:\n",
      "['drop10', 'harvest-xgb-bc-drop10-2019-07-09-19-53-25']: InProgress\n",
      "['first5', 'harvest-xgb-bc-first5-2019-07-09-19-53-25']: InProgress\n",
      "['last5', 'harvest-xgb-bc-last5-2019-07-09-19-53-25']: InProgress\n",
      "Checking job statuses:\n",
      "['drop10', 'harvest-xgb-bc-drop10-2019-07-09-19-53-25']: InProgress\n",
      "['first5', 'harvest-xgb-bc-first5-2019-07-09-19-53-25']: InProgress\n",
      "['last5', 'harvest-xgb-bc-last5-2019-07-09-19-53-25']: InProgress\n",
      "Checking job statuses:\n",
      "['drop10', 'harvest-xgb-bc-drop10-2019-07-09-19-53-25']: InProgress\n",
      "['first5', 'harvest-xgb-bc-first5-2019-07-09-19-53-25']: InProgress\n",
      "['last5', 'harvest-xgb-bc-last5-2019-07-09-19-53-25']: InProgress\n",
      "Checking job statuses:\n",
      "['drop10', 'harvest-xgb-bc-drop10-2019-07-09-19-53-25']: InProgress\n",
      "['first5', 'harvest-xgb-bc-first5-2019-07-09-19-53-25']: InProgress\n",
      "['last5', 'harvest-xgb-bc-last5-2019-07-09-19-53-25']: InProgress\n",
      "Checking job statuses:\n",
      "['drop10', 'harvest-xgb-bc-drop10-2019-07-09-19-53-25']: InProgress\n",
      "['first5', 'harvest-xgb-bc-first5-2019-07-09-19-53-25']: InProgress\n",
      "['last5', 'harvest-xgb-bc-last5-2019-07-09-19-53-25']: InProgress\n",
      "Checking job statuses:\n",
      "['drop10', 'harvest-xgb-bc-drop10-2019-07-09-19-53-25']: InProgress\n",
      "['first5', 'harvest-xgb-bc-first5-2019-07-09-19-53-25']: Completed\n",
      "['last5', 'harvest-xgb-bc-last5-2019-07-09-19-53-25']: InProgress\n",
      "Checking job statuses:\n",
      "['drop10', 'harvest-xgb-bc-drop10-2019-07-09-19-53-25']: Completed\n",
      "['first5', 'harvest-xgb-bc-first5-2019-07-09-19-53-25']: Completed\n",
      "['last5', 'harvest-xgb-bc-last5-2019-07-09-19-53-25']: InProgress\n",
      "Checking job statuses:\n",
      "['drop10', 'harvest-xgb-bc-drop10-2019-07-09-19-53-25']: Completed\n",
      "['first5', 'harvest-xgb-bc-first5-2019-07-09-19-53-25']: Completed\n",
      "['last5', 'harvest-xgb-bc-last5-2019-07-09-19-53-25']: InProgress\n",
      "Checking job statuses:\n",
      "['drop10', 'harvest-xgb-bc-drop10-2019-07-09-19-53-25']: Completed\n",
      "['first5', 'harvest-xgb-bc-first5-2019-07-09-19-53-25']: Completed\n",
      "['last5', 'harvest-xgb-bc-last5-2019-07-09-19-53-25']: InProgress\n",
      "Checking job statuses:\n",
      "['drop10', 'harvest-xgb-bc-drop10-2019-07-09-19-53-25']: Completed\n",
      "['first5', 'harvest-xgb-bc-first5-2019-07-09-19-53-25']: Completed\n",
      "['last5', 'harvest-xgb-bc-last5-2019-07-09-19-53-25']: InProgress\n",
      "Checking job statuses:\n",
      "['drop10', 'harvest-xgb-bc-drop10-2019-07-09-19-53-25']: Completed\n",
      "['first5', 'harvest-xgb-bc-first5-2019-07-09-19-53-25']: Completed\n",
      "['last5', 'harvest-xgb-bc-last5-2019-07-09-19-53-25']: Completed\n",
      "CPU times: user 501 ms, sys: 29.6 ms, total: 531 ms\n",
      "Wall time: 3min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "running_jobs = tr.start_jobs(job_csvs, bucket)\n",
    "\n",
    "tr.wait_for_jobs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launching endpoints for trained models.\n",
    "\n",
    "In a straight-through pipeline, launch endpoints, run tests, collect output, and shut endpoints down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:sagemaker:us-east-1:872344130825:model/harvest-xgb-bc-drop10-2019-07-09-19-53-25-model\n",
      "arn:aws:sagemaker:us-east-1:872344130825:model/harvest-xgb-bc-first5-2019-07-09-19-53-25-model\n",
      "arn:aws:sagemaker:us-east-1:872344130825:model/harvest-xgb-bc-last5-2019-07-09-19-53-25-model\n"
     ]
    }
   ],
   "source": [
    "tr.create_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "harvest-xgb-bc-drop10-2019-07-09-19-53-25-config\n",
      "Endpoint Config Arn: arn:aws:sagemaker:us-east-1:872344130825:endpoint-config/harvest-xgb-bc-drop10-2019-07-09-19-53-25-config\n",
      "harvest-xgb-bc-drop10-2019-07-09-19-53-25-endpoint\n",
      "arn:aws:sagemaker:us-east-1:872344130825:endpoint/harvest-xgb-bc-drop10-2019-07-09-19-53-25-endpoint\n",
      "harvest-xgb-bc-first5-2019-07-09-19-53-25-config\n",
      "Endpoint Config Arn: arn:aws:sagemaker:us-east-1:872344130825:endpoint-config/harvest-xgb-bc-first5-2019-07-09-19-53-25-config\n",
      "harvest-xgb-bc-first5-2019-07-09-19-53-25-endpoint\n",
      "arn:aws:sagemaker:us-east-1:872344130825:endpoint/harvest-xgb-bc-first5-2019-07-09-19-53-25-endpoint\n",
      "harvest-xgb-bc-last5-2019-07-09-19-53-25-config\n",
      "Endpoint Config Arn: arn:aws:sagemaker:us-east-1:872344130825:endpoint-config/harvest-xgb-bc-last5-2019-07-09-19-53-25-config\n",
      "harvest-xgb-bc-last5-2019-07-09-19-53-25-endpoint\n",
      "arn:aws:sagemaker:us-east-1:872344130825:endpoint/harvest-xgb-bc-last5-2019-07-09-19-53-25-endpoint\n"
     ]
    }
   ],
   "source": [
    "tr.create_endpoints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All endpoints created.\n"
     ]
    }
   ],
   "source": [
    "tr.wait_for_endpoints()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model\n",
    "Currently, we launch an endpoint to test the model. This endpoint includes a simple web service that takes POST request with rows of or model's X values - columns other than BadActor - and returns a corresponding list of Y values - BadActor predictions.\n",
    "\n",
    "The endpoint approach is most suitable to interactive use, such as possibly using the model to blacklist a harvesting session as soon as it is identified. For offline analysis, this should be reconfigured to run batch transform jobs instead, which are cheaper to run and more streamlined to invoke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = 'us-east-1'\n",
    "sg_client = boto3.client('runtime.sagemaker', region_name=region)\n",
    "\n",
    "import json\n",
    "from itertools import islice\n",
    "import math\n",
    "import struct\n",
    "\n",
    "!head -10000 out/test.csv > out/single-test.csv\n",
    "\n",
    "file_name = 'out/single-test.csv' \n",
    "\n",
    "# file_name = \"out/may8.csv\"\n",
    "\n",
    "csv = pd.read_csv(file_name, header=None)\n",
    "csv.columns\n",
    "label = csv[0]\n",
    "csv = csv.drop(0,axis=1)\n",
    "\n",
    "single = \"out/single.csv\"\n",
    "\n",
    "csv.to_csv(path_or_buf=single, header=False, index=False)\n",
    "\n",
    "with open(single, 'r') as f:\n",
    "    payload = f.read().strip()\n",
    "    \n",
    "# csv\n",
    "# drop10 = pd.read_csv(\"out/drop10_train.csv\", header=None)\n",
    "# drop10, csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop10 = pd.read_csv(\"out/drop10_train.csv\", header = None)\n",
    "first5 = pd.read_csv(\"out/first5_train.csv\", header = None)\n",
    "last5 = pd.read_csv(\"out/last5_train.csv\", header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37, 34, 36, 36)"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(drop10.columns),len(first5.columns),len(last5.columns),len(csv.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['drop10', 'harvest-xgb-bc-drop10-2019-07-09-19-53-25'],\n",
       " ['first5', 'harvest-xgb-bc-first5-2019-07-09-19-53-25'],\n",
       " ['last5', 'harvest-xgb-bc-last5-2019-07-09-19-53-25']]"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr.running_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = 'harvest-xgb-bc-drop10-2019-07-09-19-53-25-endpoint'\n",
    "endpoints = [get_endpoint_name(ep) for ep in tr.running_jobs]\n",
    "\n",
    "response = sg_client.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                   ContentType='text/csv', \n",
    "                                   Body=payload)\n",
    "result = response['Body'].read()\n",
    "result = result.decode(\"utf-8\")\n",
    "result = result.split(',')\n",
    "result = [round(float(i)) for i in result]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(job_name, csv):\n",
    "    '''\n",
    "    job_name - name of a job that has run all the way through to an endpoint\n",
    "    csv - a csv with the same y_col and x_col structure as the training data\n",
    "    returns - a single-column dataframe with the predictions from the model.\n",
    "    '''\n",
    "    endpoint_name = tr.get_endpoint_name( job_name )\n",
    "    with open(csv, 'r') as f:\n",
    "        payload = f.read().strip()\n",
    "    response = sg_client.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                               ContentType='text/csv', \n",
    "                               Body=payload)\n",
    "    result = response['Body'].read()\n",
    "    result = result.decode(\"utf-8\")\n",
    "    result = result.split(',')\n",
    "    result = [round(float(i)) for i in result]\n",
    "    return pd.DataFrame( result )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name=tr.running_jobs[0][1]\n",
    "result = test_model( job_name, single)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the confusion metrics\n",
    "\n",
    "A confusion matrix describes the proportions of true and false positives and negatives, together with some derived metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalHelper:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def compute_confusion(self, reference, test):\n",
    "        '''\n",
    "        reference - single-column dataframe of expected y-values - labels\n",
    "        test - single-column dataframe of computed y-values for comparison\n",
    "        '''\n",
    "        comp = pd.concat( [reference, test], axis = 1)\n",
    "        comp.columns =[\"label\",'prediction']\n",
    "        label_positive = comp['label'] == 1\n",
    "        predict_positive = comp['prediction'] == 1\n",
    "        tp = len( comp[label_positive & predict_positive])\n",
    "        fp = len( comp[~label_positive & predict_positive])\n",
    "        tn = len( comp[~label_positive & ~predict_positive])\n",
    "        fn = len( comp[label_positive & ~predict_positive])\n",
    "        m = len(comp)\n",
    "\n",
    "        accuracy = (tp+tn)/m\n",
    "        precision = tp/(tp+fp)\n",
    "        recall = tp/(tp+fn)\n",
    "\n",
    "        print(\"accuracy: {} precision: {} recall {}\".format(accuracy, precision,recall))\n",
    "        return (accuracy, precision, recall, tp,fp,tn,fn)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9654743390357698 precision: 0.998220640569395 recall 0.8360655737704918\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9654743390357698, 0.998220640569395, 0.8360655737704918, 1683, 3, 7629, 330)"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eh = EvalHelper()\n",
    "eh.compute_confusion(label, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
