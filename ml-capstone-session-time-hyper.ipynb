{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Udacity ML Engineering Nanodegree - Capstone Project\n",
    "\n",
    "# Identifying attacker application sessions through supervised learning over known attacker sessions\n",
    "## Project Domain\n",
    "This project bears on the domain of security. I work for a company that publishes expensive content on the web, making it available to subscribers only. We frequently discover cases in which attackers have stolen credentials from our customers and use them to perform unauthorized content downloads. Detecting such activity quickly, without human analysis, has always been difficult given the large amounts of data involved.\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "Given a selection of log data containing records that describe different usage activities, determine whether a given session is likely to describe unauthorized content access. This project will use known attacker data as labeled examples supporting a supervised learning approach. I will use a supervised learning approach on labeled input data that includes both attacker sessions and “innocent” sessions, hoping to produce a model that can predict a high percentage of attack sessions with very low false positives. A successful proof of concept would identify 70% of attacker sessions with no more than 5% false positives.\n",
    "\n",
    "## Datasets and Inputs\n",
    "\n",
    "- known-attacker.txt–afilecontaininglogentriescollectedfromsessions manually identified as belonging to known attackers. Covering a span of around two years, this file contains ~60K attacker events.\n",
    "\n",
    "- mixed1-3.txt – files containing an unfiltered selection of time intervals known to contain attacker activities. These files contain a total of ~200K events, of which less than 0.5% are attacker events.\n",
    "\n",
    "The events in each file are recorded as tab-separated rows in the following columns:\n",
    "\n",
    "SessionNo LogTime CustID GroupID ProfID Act BadActor\n",
    "\n",
    "The ‘SessionNo’ column groups activities into sets of consecutive user actions, each of which is identified with an ‘Act’ column that identified what the user did, e.g., logged in, performed a search, downloaded content. The CustID, GroupID, and ProfID columns identify unique customer organizations, and are not expected to be useful in the learning exercise per se.\n",
    "\n",
    "Each file has been labeled manually with a notation whether each event belongs to an attacker session. The ‘BadActor’ column indicates whether the given event belongs to an attacker session.\n",
    "\n",
    "Given the low ratio of attacker to innocent events in the mixed files, I expect it will be necessary to augment the data to improve training results. [2016, Buczakak and Guven] suggests dropping negative rows or duplicating positive rows; I plan to augment training data using the long history of attacker-only rows in known-attacker.txt.\n",
    "\n",
    "## Solution Statement\n",
    "\n",
    "I will train a binary classifier model to distinguish attacker sessions from normal usage. My hypothesis is that attacker activity is distinctly different from normal usage in at least the following ways:\n",
    "• An attacker logs in and begins downloading one ebook after another. A normal user will perform topic, title or author searches, read a few pages, and search again, browsing several pieces of content before choosing any one work to download.\n",
    "• An attacker tends not to pause for long intervals between content accesses. A normal user will read at least part of a work before moving on to the next.\n",
    "• An attacker’s session will often not last as long as a normal research session. After completing an attack, the attacker leaves immediately, while a researcher will do homework or study for hours.\n",
    "These three distinctions suggest several approaches for feature engineering:\n",
    "• Our logs distinguish around 40 distinct actions that users take when performing\n",
    "research. Derive features that represent these actions across the course of a session.\n",
    "• A straightforward method to model user think-time is to calculate some simple session metrics. Average think-time and (possibly) standard deviation of think-time seem likely\n",
    "to capture a large fraction of the difference between attack and research.\n",
    "• Session length should be easy to calculate and populate as a feature of a session.\n",
    "My overall goal is to produce a model that will allow me to evaluate past log files looking for evidence of attacks, so that this information can be used to create rules in our perimeter security devices.\n",
    "\n",
    "## Prior research\n",
    "As far as I can see, the problem of identifying attackers from logs of activities at the business- use-case level has not been thoroughly researched. I did find numerous papers on analysis of net flows and HTTP logs that can be read for useful parallel techniques.\n",
    "\n",
    "Pietraszek, Tadeusz and Axel Tanner. “Data mining and machine learning - Towards reducing false positives in intrusion detection.” Inf. Sec. Techn. Report 10 (2005): 169-183. Uses machine learning to identify candidate alerts from an IDS for human labeling. Labels are used in supervised learning to refine selection of alerts.\n",
    "\n",
    "Buczak, Anna L. and Erhan Guven. “A Survey of Data Mining and Machine Learning Methods for Cyber Security Intrusion Detection.” IEEE Communications Surveys & Tutorials 18 (2016): 1153-1176. In the domain of IP net flow analysis, establishes terminology and reviews a broad selection of techniques for modeling attacks based on collections of internet packets.\n",
    "\n",
    "Sperotto, Anna et al. “An Overview of IP Flow-Based Intrusion Detection.” IEEE Communications Surveys & Tutorials 12 (2010): 343-356. In-depth discussion of net flow analysis, distinguishing parts of the net-flow modeling problem rather than how to analyze collections of successfully-captured packets.\n",
    "\n",
    "Moh, Melody et al. “Detecting Web Attacks Using Multi-stage Log Analysis.” 2016 IEEE 6th International Conference on Advanced Computing (IACC) (2016): 733-738. Overview of an approach for managing high volumes of HTTP logs and analyzing for presence of SQL injection attackes. Uses Bayes net classification in WEKA, produces an enriched analyst workstation environment in Kibana.\n",
    "\n",
    "## Benchmark Model & Evaluation Metrics\n",
    "The total volume of known attacker traffic is extremely low, less than .05% for a given victim. My approach to computing a baseline is to assume that *P(attack)* for a given row is 0. I will compare the confusion matrix for this baseline to the one for predictions from my ML model.\n",
    "\n",
    "## Project Design\n",
    "I will execute the following plan:\n",
    "\n",
    "1. Import the data into Jupyter & SageMaker in order to study it in place 2. Choose an approach for feature engineering:\n",
    "\n",
    "    a. Can I engineer a session row that contains enough information to produce a useful result in one of the algorithms I’ve already used, like XGBoost?\n",
    "\n",
    "    b. Do I need to use LSTM or convolution or some other learning algorithm with a memory that can learn sequences?\n",
    "\n",
    "3. Produce a repeatable process that can convert our raw log file into a dataset that my chosen algorithm can process\n",
    "\n",
    "4. Train a model, and execute the model against labelled data in batch transform mode 5. Use the result to calculate true and false positives and negatives. Given the small population of positive results – attacker session are a small fraction of total traffic – precision and recall are the most important metrics.\n",
    "\n",
    "    a. High precision means that my false positive rate is low. It’s critical not to misidentify innocent traffic as malicious.\n",
    "\n",
    "    b. High recall is the next most important metric. I need to identify as great a fraction of actual attack traffic as possible.\n",
    "\n",
    "The training set includes the following files:\n",
    "\n",
    "|File       |Rows                             |Contents|\n",
    "------------|-------------------------------------|----|\n",
    "|mixed-1.txt|119474|raw transactions|\n",
    "|mixed-2.txt|43608|raw transactions|\n",
    "|mixed-3.txt|30844|raw transactions|\n",
    "|known-attacker.txt|61917|raw transactions for a known attacker|\n",
    "\n",
    "All files include a 'BadActor' column that labels a transaction as belonging to a known attacker or not. The 'mixed-' files consist of whole hours of activity in which there is an attack, while known-attacker.txt contains all attacks for the known attacker over the last two years. In all, we have 193926 transactions from \"innocent\" sessions and 61917 LT transactions. Approximately 32% of transactions are labeled BadActor = 1, giving us a reasonable proportion in both classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm up\n",
    "Import standard libraries and prepare the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data’: File exists\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sagemaker session, role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# S3 bucket name\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "# common column names\n",
    "bad_col='BadActor'\n",
    "sess_col='SessionNo'\n",
    "txn_col='Act'\n",
    "logtime_col = 'LogTime'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download\n",
    "Retrieve the datafiles from the project's designated S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: data/mixed-1.txt Rows: 119474\n",
      "File: data/mixed-2.txt Rows: 43608\n",
      "File: data/mixed-3.txt Rows: 30844\n",
      "File: known Rows: 533\n"
     ]
    }
   ],
   "source": [
    "mixed = []\n",
    "for i in range(3):\n",
    "    name = 'data/mixed-{}.txt'.format(i+1)\n",
    "    m = pd.read_csv(name,sep='\\t')\n",
    "    m[logtime_col] = pd.to_datetime(m[logtime_col])\n",
    "    m[txn_col]= m[txn_col].astype(str)\n",
    "    print( \"File: {} Rows: {}\".format( name, len(m)))\n",
    "    mixed.append(m)\n",
    "    \n",
    "# Load data for known attacker\n",
    "name = 'data/known-attacker.txt'\n",
    "# known = pd.read_csv(name ,sep='\\t')\n",
    "\n",
    "mixed = pd.concat(mixed)\n",
    "known = mixed[mixed[bad_col]==1]\n",
    "# known = pd.concat([known]*200)\n",
    "print( f\"File: known Rows: {len(known)}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total mixed transaction rows: 193926\n"
     ]
    }
   ],
   "source": [
    "# known[txn_col]= known[txn_col].astype(str)\n",
    "# print( \"File: {} Rows: {}\".format( name, len(known)))\n",
    "\n",
    "# Load data for extra day of test data - need to have full set of transaction types\n",
    "name = 'data/single.txt'\n",
    "single = pd.read_csv(name ,sep='\\t')\n",
    "single[txn_col]= single[txn_col].astype(str)\n",
    "\n",
    "# txn = pd.concat([mixed,known])\n",
    "txn = pd.concat([mixed])\n",
    "print( \"Total mixed transaction rows: {}\".format(len(txn)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data conversion and feature engineering\n",
    "In real life, a session consists of a series of rows of transactions of different types, and each transaction type records a variable number of additional metadata attributes describing a logged event, for a total of over 30 columns of extracted data. In addition, our tagging process has given each row a BadActor label.\n",
    "\n",
    "|sessionno|txn id|BadActor|parm1|parm2|...|\n",
    "|---------|------|--------|-----|-----|---|\n",
    "|1240|111|0|query string|...|...|\n",
    "|1240|112|0|meta|...|...|\n",
    "|2993|301|1|meta|...|...|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['111', '112', '114', '115', '116', '117', '118', '119', '121',\n",
       "       '123', '124', '125', '126', '127', '135', '201', '215', '216',\n",
       "       '217', '219', '312', '315', '316', '317', '401', '402', '403',\n",
       "       '406', '407', '410', '411', '511', '513', '601', '607'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'Innocent' log entries\n",
    "txns = pd.DataFrame(np.sort(txn['Act'].unique()))\n",
    "\n",
    "# Harvesting log entries\n",
    "known_txns = pd.DataFrame(np.sort(known['Act'].unique()))\n",
    "\n",
    "all_txns=pd.concat([txn,single])\n",
    "\n",
    "all_txn_types = np.sort(all_txns[txn_col].unique())\n",
    "all_txn_types\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We drop most of this information, including the temporal sequence of the log entries, and convert each session into a single row of data. Almost all of the columns go away, replaced by counts of transaction types in the session.\n",
    "\n",
    "|sessionno|BadActor|111|112|113|...|301|302|...|\n",
    "|---------|--------|---|---|---|---|---|---|---|\n",
    "|1240|0|1|1|0|...|0|0|...|\n",
    "|2993|1|0|0|0|...|1|0|...|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_missing_columns( df_target, source_cols):\n",
    "    '''\n",
    "    If there are columns in the list 'source' that are not in the DataFrame 'target',\n",
    "    add new columns to 'target' that are populated with 0.0.\n",
    "    '''\n",
    "    df = df_target\n",
    "    target_cols = df_target.columns\n",
    "    missing_cols = set(source_cols) - set(target_cols)\n",
    "    \n",
    "    if 0 < len(missing_cols):\n",
    "        print( \"Missing columns: {}\".format(missing_cols)) \n",
    "        new_cols = dict([(col,0.0) for col in missing_cols])\n",
    "        df = df_target.assign(**new_cols)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def flatten_txns( txn_log ):\n",
    "    txn_narrow = txn_log[[sess_col, txn_col,bad_col]]\n",
    "    txn_pivot = pd.pivot_table(txn_narrow, index=[sess_col,bad_col], columns = [txn_col],aggfunc=[len]).fillna(0)\n",
    "    txn_pivot.columns = txn_pivot.columns.droplevel(0)           # the pivot table has a two-level index\n",
    "    txn_flat = txn_pivot.rename_axis(None, axis=1).reset_index() # these two lines get rid of it so we have a simple table\n",
    "    \n",
    "    \n",
    "    txn_full = add_missing_columns(txn_flat, all_txn_types)\n",
    "    txn_xs = txn_full.drop(columns=[sess_col,bad_col])\n",
    "    txn_xs = txn_xs.reindex(sorted(txn_xs.columns), axis=1)\n",
    "\n",
    "    txn_sort = pd.concat([txn_full[[sess_col,bad_col]], txn_xs],sort=True,axis=1)\n",
    "    return txn_sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute event statistics - \"wait times\"\n",
    "We expect that the behaviors shown in event traces of innocent research sessions will differ from those in harvesting sessions. Researchers will exhibit variable waits between events, time for thought, reading, or checking citations; harvesters will search for a work and then leave. \n",
    "\n",
    "With this in mind, we compute statistics for time intervals between events in sessions, as well as metrics for sessions overall, and add features to the set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_wait_times(txns):\n",
    "    # sort by session and logtime\n",
    "    txng = txns.set_index(['SessionNo','LogTime']).sort_index() \n",
    "\n",
    "    # remove the index so we can compute on log time values\n",
    "    txng.reset_index(inplace=True)\n",
    "    \n",
    "    # subtract the previous row's logtime from this rows logtime\n",
    "    txng['Wait'] = pd.to_timedelta(txng[logtime_col].diff() ).astype('timedelta64[s]')\n",
    "    \n",
    "    # add the session back to the index so that we can flag session starts\n",
    "    txng = txng.set_index(['SessionNo']).sort_index() \n",
    "    \n",
    "    # if the current session value is different, set the wait time to 0\n",
    "    s = pd.Series(txng.index)\n",
    "    session_starts = (s != s.shift()).values\n",
    "    txng.loc[session_starts,'Wait'] = 0\n",
    "    \n",
    "    return txng\n",
    "\n",
    "\n",
    "def compute_wait_stats(txnw):\n",
    "    txnw_g = txnw.reset_index()[[sess_col,'Wait']]\n",
    "    txnw_g = txnw_g.groupby(sess_col)\n",
    "    txnw_stat = txnw_g.agg([np.mean,np.std,np.sum,np.min,np.max,len]).fillna(0)\n",
    "    txnw_stat.columns = txnw_stat.columns.droplevel(0)  \n",
    "    txnw_stat.rename_axis(None, axis=1).reset_index()\n",
    "    return txnw_stat\n",
    "\n",
    "txnw  = add_wait_times(txn)\n",
    "txn_wait_stats = compute_wait_stats(txnw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_session_stats( txn ):\n",
    "    txng = txn[[sess_col,'LogTime']].groupby(sess_col)\n",
    "    txn_sess = txng.agg([np.min,np.max])\n",
    "    txn_sess.columns = txn_sess.columns.droplevel(0)  \n",
    "    txn_sess.rename_axis(None, axis=1).reset_index()\n",
    "    txn_sess['length'] = pd.to_timedelta((txn_sess['amax'] - txn_sess['amin'])).astype('timedelta64[s]')\n",
    "    return txn_sess\n",
    "\n",
    "txn_session_stats = compute_session_stats( txn )\n",
    "# txn_session_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining timing and session data with transaction data\n",
    "\n",
    "On review, the session data is a simple subset of the wait times data - the total session time is already the 'sum' column, and the min-max times aren't really trainable - we'd need to adjust them to local tz or something.\n",
    "\n",
    "We can conduct many experiments with these additional time-related datasets:\n",
    "\n",
    "1. ~~Train on each one individually to assess its value in a model~~\n",
    "2. ~~Combine them, and see a) whether they improve together and b) which is a better predictor than the other~~\n",
    "3. Only Wait stats is interesting.\n",
    "4. ~~Combine session stats + txn counts~~\n",
    "4. Combine wait stats + txn counts \n",
    "\n",
    "For now, we're just going to use txns and wait stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing columns: {'312'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/preprocessing/data.py:334: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing as skp\n",
    "\n",
    "def prep_txn_df(txn):\n",
    "    '''\n",
    "    Flatten and augment a file of txn data.\n",
    "    Given a df of data in raw transaction format, return a df of data in session format\n",
    "    Relies on a global all_txn_types listing all possible txn_type values for the set.\n",
    "    '''\n",
    "    \n",
    "    # Clean up column types.\n",
    "    txn[logtime_col] = pd.to_datetime(txn[logtime_col])\n",
    "    txn[txn_col]= txn[txn_col].astype(str)\n",
    "\n",
    "    # Flatten\n",
    "    flat = flatten_txns(txn)\n",
    "    \n",
    "    # Add session timings\n",
    "    txnw  = add_wait_times(txn)\n",
    "    txn_wait_stats = compute_wait_stats(txnw)\n",
    "    wide = flat.merge( txn_wait_stats, on=sess_col, suffixes=['base', 'wait'] )\n",
    "\n",
    "    # scale everything into 0..1 range\n",
    "    cols = wide.columns\n",
    "    sessions=wide[sess_col]\n",
    "    min_max_scaler = skp.MinMaxScaler()\n",
    "    wide_minmax = pd.DataFrame(min_max_scaler.fit_transform(wide))\n",
    "    wide_minmax.columns = cols\n",
    "    wide_minmax[sess_col]=sessions\n",
    "    return wide\n",
    "\n",
    "base = prep_txn_df(txn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SessionNo</th>\n",
       "      <th>BadActor</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>...</th>\n",
       "      <th>511</th>\n",
       "      <th>513</th>\n",
       "      <th>601</th>\n",
       "      <th>607</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sum</th>\n",
       "      <th>amin</th>\n",
       "      <th>amax</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2147481927</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2147317281</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.250000</td>\n",
       "      <td>39.936408</td>\n",
       "      <td>93.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2147002735</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.833333</td>\n",
       "      <td>33.017672</td>\n",
       "      <td>185.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2146953899</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>150.891892</td>\n",
       "      <td>360.721098</td>\n",
       "      <td>5583.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2121.0</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2146926264</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.577350</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    SessionNo  BadActor  111  112  114   115  116  117  118  119  ...  511  \\\n",
       "0 -2147481927         0  1.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "1 -2147317281         0  1.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "2 -2147002735         0  1.0  0.0  0.0   2.0  1.0  0.0  0.0  0.0  ...  0.0   \n",
       "3 -2146953899         0  0.0  1.0  0.0  20.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "4 -2146926264         0  1.0  0.0  0.0   1.0  0.0  1.0  0.0  0.0  ...  0.0   \n",
       "\n",
       "   513  601  607        mean         std     sum  amin    amax   len  \n",
       "0  0.0  0.0  0.0    0.000000    0.000000     0.0   0.0     0.0   1.0  \n",
       "1  0.0  0.0  0.0   23.250000   39.936408    93.0   0.0    83.0   4.0  \n",
       "2  0.0  0.0  0.0   30.833333   33.017672   185.0   0.0    86.0   6.0  \n",
       "3  0.0  0.0  0.0  150.891892  360.721098  5583.0   0.0  2121.0  37.0  \n",
       "4  0.0  0.0  0.0    0.333333    0.577350     1.0   0.0     1.0   3.0  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Producing pools of training and testing data\n",
    "\n",
    "We will divide the combined good and bad data pools as follows:\n",
    "- a training set that the model iterates over during the learning process\n",
    "- a test set that is used to evaluate the model during training\n",
    "- a validation set that is kept separate to test the model after training is complete. We need separate test and validate pools in order to make sure that we're overfitting the model to a single set of test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_frame( df, train_frac ):\n",
    "    l = len(df)\n",
    "    test_frac = (1-train_frac)/2\n",
    "    tr = int(train_frac * l)\n",
    "    te = int(tr + test_frac * l)\n",
    "    \n",
    "    train = df[:tr]\n",
    "    test = df[tr:te]\n",
    "    val = df[te:]\n",
    "    return [train, test, val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_split( flat, bad_split=.8 ):\n",
    "    bad = flat[flat[bad_col]==1]\n",
    "    good = flat[flat[bad_col]==0]\n",
    "    \n",
    "    bads = split_frame(bad, bad_split)\n",
    "    goods = split_frame(good, bad_split)\n",
    "    \n",
    "    dfs = []\n",
    "    for i in range(3):\n",
    "        # Dropping the session # because we don't want to train on it.\n",
    "        # Also leaves our label - BadActor - in the 0 column, as XGBoost requires for CSV\n",
    "        df = bads[i].append(goods[i]).drop(sess_col,axis=1).sample(frac=1)\n",
    "        dfs.append( df )\n",
    "    \n",
    "    return dfs\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the data and upload to S3\n",
    "Break the set into train, test, and validation collections and output CSV's.\n",
    "As Sagemaker requires, leave out row indices and column headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘output’: File exists\n",
      "Uploading output/train.csv to sk-mlai-harvesting\n",
      "None\n",
      "Uploading output/test.csv to sk-mlai-harvesting\n",
      "None\n",
      "Uploading output/validate.csv to sk-mlai-harvesting\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "dfs = train_split(base, .4)\n",
    "\n",
    "!mkdir output\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "bucket = \"sk-mlai-harvesting\"\n",
    "\n",
    "for i, df in enumerate(dfs):\n",
    "    files = [\"train\",\"test\",\"validate\"]\n",
    "    file = \"output/{}.csv\".format(files[i])\n",
    "#     df.to_csv(path_or_buf= file, header=False, index=False  )\n",
    "    df.to_csv(path_or_buf= file, index=False , header=None )\n",
    "\n",
    "    print(\"Uploading {} to {}\".format(file, bucket))\n",
    "\n",
    "    response = s3_client.upload_file(file, bucket, file)\n",
    "    print(response)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare and train a model\n",
    "Boilerplate code mostly copied from Amazon sample code at https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/xgboost_abalone/xgboost_abalone.ipynb, with ample room for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training job harvesting-xgboost-binary-class2019-07-20-00-02-00\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "Completed\n"
     ]
    }
   ],
   "source": [
    "### %%time\n",
    "region = 'us-east-1'\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container = get_image_uri(region, 'xgboost')\n",
    "\n",
    "\n",
    "from time import gmtime, strftime\n",
    "\n",
    "job_name = 'harvesting-xgboost-binary-class' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(\"Training job\", job_name)\n",
    "\n",
    "#Ensure that the training and validation data folders generated above are reflected in the \"InputDataConfig\" parameter below.\n",
    "\n",
    "create_training_params = \\\n",
    "{\n",
    "    \"AlgorithmSpecification\": {\n",
    "        \"TrainingImage\": container,\n",
    "        \"TrainingInputMode\": \"File\"\n",
    "    },\n",
    "    \"RoleArn\": role,\n",
    "    \"OutputDataConfig\": {\n",
    "        \"S3OutputPath\": os.path.join(\"s3://\",bucket, \"output\", \"xgb-class\") \n",
    "    },\n",
    "    \"ResourceConfig\": {\n",
    "        \"InstanceCount\": 1,\n",
    "        \"InstanceType\": \"ml.m4.4xlarge\",\n",
    "        \"VolumeSizeInGB\": 5\n",
    "    },\n",
    "    \"TrainingJobName\": job_name,\n",
    "    \"HyperParameters\": {\n",
    "        \"max_depth\":\"8\",\n",
    "        \"eta\":\"0.53\",\n",
    "        \"alpha\":\"1.02\",\n",
    "        \"gamma\":\"4\",\n",
    "        \"rate_drop\":\".3\",\n",
    "        \"min_child_weight\":\"3\",\n",
    "        \"scale_pos_weight\":\"375\",\n",
    "        \"subsample\":\"1\",\n",
    "        \"silent\":\"0\",\n",
    "        \"tweedie_variance_power\":\"1.3\",\n",
    "        \"objective\":\"binary:logistic\",\n",
    "        \"num_round\":\"83\"\n",
    "    },\n",
    "    \"StoppingCondition\": {\n",
    "        \"MaxRuntimeInSeconds\": 3600\n",
    "    },\n",
    "    \"InputDataConfig\": [\n",
    "        {\n",
    "            \"ChannelName\": \"train\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": \"s3://sk-mlai-harvesting/output/train.csv\" , \n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"text/csv\",\n",
    "            \"CompressionType\": \"None\"\n",
    "        },\n",
    "        {\n",
    "            \"ChannelName\": \"validation\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": \"s3://sk-mlai-harvesting/output/validate.csv\" ,\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"text/csv\",\n",
    "            \"CompressionType\": \"None\"\n",
    "        },\n",
    "        {\n",
    "            \"ChannelName\": \"test\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": \"s3://sk-mlai-harvesting/output/test.csv\" ,\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"text/csv\",\n",
    "            \"CompressionType\": \"None\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "client = boto3.client('sagemaker', region_name=region)\n",
    "client.create_training_job(**create_training_params)\n",
    "\n",
    "import time\n",
    "\n",
    "status = client.describe_training_job(TrainingJobName=job_name)['TrainingJobStatus']\n",
    "print(status)\n",
    "while status !='Completed' and status!='Failed':\n",
    "    time.sleep(60)\n",
    "    status = client.describe_training_job(TrainingJobName=job_name)['TrainingJobStatus']\n",
    "    print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "region = 'us-east-1'\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "prefix = \"model/xgb_class\"\n",
    "\n",
    "container = get_image_uri(region, 'xgboost', repo_version='latest')\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.m4.xlarge',\n",
    "                                    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                                    sagemaker_session=sess)\n",
    "\n",
    "xgb.set_hyperparameters(eval_metric='auc',\n",
    "                        objective='binary:logistic',\n",
    "                        num_round=100,\n",
    "                        rate_drop=0.3,\n",
    "                        tweedie_variance_power=1.4)\n",
    "\n",
    "hyperparameter_ranges = {'eta': ContinuousParameter(0, 1),\n",
    "                        'min_child_weight': ContinuousParameter(1, 10),\n",
    "                        'alpha': ContinuousParameter(0, 2),\n",
    "                        'max_depth': IntegerParameter(1, 10),\n",
    "                        'num_round': IntegerParameter(10,100)}\n",
    "\n",
    "objective_metric_name = 'validation:auc'\n",
    "\n",
    "tuner = HyperparameterTuner(xgb,\n",
    "                            objective_metric_name,\n",
    "                            hyperparameter_ranges,\n",
    "                            max_jobs=20,\n",
    "                            max_parallel_jobs=3)\n",
    "\n",
    "s3_input_train = sagemaker.s3_input(s3_data=f's3://{bucket}/output/train.csv', content_type='csv')\n",
    "s3_input_validation = sagemaker.s3_input(s3_data=f's3://{bucket}/output/validate.csv', content_type='csv')\n",
    "\n",
    "tuner.fit({'train': s3_input_train, 'validation': s3_input_validation}, include_cls_metadata=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'InProgress'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boto3.client('sagemaker').describe_hyper_parameter_tuning_job(\n",
    "    HyperParameterTuningJobName=tuner.latest_tuning_job.job_name)['HyperParameterTuningJobStatus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "harvesting-xgboost-binary-class2019-07-20-00-02-00-model\n",
      "s3://sk-mlai-harvesting/output/xgb-class/harvesting-xgboost-binary-class2019-07-20-00-02-00/output/model.tar.gz\n",
      "arn:aws:sagemaker:us-east-1:617644144259:model/harvesting-xgboost-binary-class2019-07-20-00-02-00-model\n",
      "CPU times: user 19.2 ms, sys: 256 µs, total: 19.4 ms\n",
      "Wall time: 290 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import boto3\n",
    "from time import gmtime, strftime\n",
    "\n",
    "model_name=job_name + '-model'\n",
    "print(model_name)\n",
    "\n",
    "info = client.describe_training_job(TrainingJobName=job_name)\n",
    "model_data = info['ModelArtifacts']['S3ModelArtifacts']\n",
    "print(model_data)\n",
    "\n",
    "primary_container = {\n",
    "    'Image': container,\n",
    "    'ModelDataUrl': model_data\n",
    "}\n",
    "\n",
    "create_model_response = client.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    PrimaryContainer = primary_container)\n",
    "\n",
    "print(create_model_response['ModelArn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harvest-XGBoostEndpointConfig-2019-07-20-00-05-54\n",
      "Endpoint Config Arn: arn:aws:sagemaker:us-east-1:617644144259:endpoint-config/harvest-xgboostendpointconfig-2019-07-20-00-05-54\n"
     ]
    }
   ],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "endpoint_config_name = 'Harvest-XGBoostEndpointConfig-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(endpoint_config_name)\n",
    "create_endpoint_config_response = client.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType':'ml.m4.xlarge',\n",
    "        'InitialVariantWeight':1,\n",
    "        'InitialInstanceCount':1,\n",
    "        'ModelName':model_name,\n",
    "        'VariantName':'AllTraffic'}])\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response['EndpointConfigArn'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launch an endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harvest-XGBoostEndpoint-2019-07-20-00-05-56\n",
      "arn:aws:sagemaker:us-east-1:617644144259:endpoint/harvest-xgboostendpoint-2019-07-20-00-05-56\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: InService\n",
      "Arn: arn:aws:sagemaker:us-east-1:617644144259:endpoint/harvest-xgboostendpoint-2019-07-20-00-05-56\n",
      "Status: InService\n",
      "CPU times: user 137 ms, sys: 1.42 ms, total: 138 ms\n",
      "Wall time: 8min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import time\n",
    "\n",
    "endpoint_name = 'Harvest-XGBoostEndpoint-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(endpoint_name)\n",
    "create_endpoint_response = client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name)\n",
    "print(create_endpoint_response['EndpointArn'])\n",
    "\n",
    "resp = client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp['EndpointStatus']\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status=='Creating':\n",
    "    time.sleep(60)\n",
    "    resp = client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp['EndpointStatus']\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp['EndpointArn'])\n",
    "print(\"Status: \" + status)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model\n",
    "Currently, we launch an endpoint to test the model. This endpoint includes a simple web service that takes POST request with rows of or model's X values - columns other than BadActor - and returns a corresponding list of Y values - BadActor predictions.\n",
    "\n",
    "The endpoint approach is most suitable to interactive use, such as possibly using the model to blacklist a harvesting session as soon as it is identified. For offline analysis, this should be reconfigured to run batch transform jobs instead, which are cheaper to run and more streamlined to invoke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing columns: {'601'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/preprocessing/data.py:334: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test file single.txt: 9246 sessions, 10 bad actor sessions\n"
     ]
    }
   ],
   "source": [
    "runtime_client = boto3.client('runtime.sagemaker', region_name=region)\n",
    "single = pd.read_csv('data/single.txt', sep=\"\\t\")\n",
    "wide = pd.concat([single])\n",
    "wide_test = prep_txn_df(wide).drop(sess_col,axis=1)\n",
    "print( f\"Test file single.txt: {len(wide_test)} sessions, {len(wide_test[wide_test[bad_col]==1])} bad actor sessions\")\n",
    "\n",
    "label = wide_test[bad_col]\n",
    "csv = wide_test.drop([bad_col],axis=1)\n",
    "\n",
    "csv.to_csv(path_or_buf=\"output/single.csv\", header=None, index=None)\n",
    "\n",
    "with open(\"output/single.csv\", 'r') as f:\n",
    "    payload = f.read().strip()\n",
    "    \n",
    "# wide_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = runtime_client.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                   ContentType='text/csv', \n",
    "                                   Body=payload)\n",
    "result = response['Body'].read()\n",
    "result = result.decode(\"utf-8\")\n",
    "result = result.split(',')\n",
    "result = [round(float(i)) for i in result]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the confusion metrics\n",
    "\n",
    "A confusion matrix describes the proportions of true and false positives and negatives, together with some derived metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9896171317326411 precision: 0.011363636362345043 recall 0.0999999999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 87, 9149, 9, 9246)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp = pd.concat( [label, pd.DataFrame(result)], axis = 1)\n",
    "comp.columns =[\"label\",'prediction']\n",
    "\n",
    "label_positive = comp['label'] == 1\n",
    "predict_positive = comp['prediction'] == 1\n",
    "\n",
    "tp = len( comp[label_positive & predict_positive])\n",
    "fp = len( comp[~label_positive & predict_positive])\n",
    "tn = len( comp[~label_positive & ~predict_positive])\n",
    "fn = len( comp[label_positive & ~predict_positive])\n",
    "m = len(comp)\n",
    "\n",
    "accuracy = (tp+tn)/m\n",
    "precision = tp/(tp+0.00000001+fp)\n",
    "recall = tp/(tp+0.00000001+fn)\n",
    "\n",
    "print(f\"accuracy: {accuracy} precision: {precision} recall {recall}\")\n",
    "tp,fp,tn,fn, len(comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial results with session timings.\n",
    "accuracy: 0.9657853810264385 precision: 1.0 recall 0.8360655737704918\n",
    "\n",
    "|tp|fp|tn|fn|len(comp)|\n",
    "|--|--|--|--|---------|\n",
    "|1683| 0| 7632| 330|9645|\n",
    "\n",
    "Running the model against the test population - split from the initial training set of 3 days mixed traffic + 2 years attack traffic - I saw extremely promising results. Perfect precision, very high recall.\n",
    "\n",
    "\n",
    "# Results against new data, whole day:\n",
    "accuracy: 0.968754320475598 precision: 0.9665454545454546 recall 0.8807157057654076\n",
    "\n",
    "|tp|fp|tn|fn|len(comp)|\n",
    "|--|--|--|--|---------|\n",
    "|1329| 46| 5678| 180|7233|\n",
    "\n",
    "However, testing against an additional data set with a normal frequency of malicious sessions was extremely disappointing. Essentially no attacker sessions were recognized by the model trained on a dataset augmented with two years of training data.\n",
    "\n",
    "# Possible optimal approach with this algorithm, still inadequate:\n",
    "accuracy: 0.9950248756218906 precision: 0.12499999997395833 recall 0.5999999993999999\n",
    "\n",
    "|tp|fp|tn|fn|len(comp)|\n",
    "|--|--|--|--|---------|\n",
    "|6| 42| 9194| 4|9246|\n",
    "\n",
    "Using tuned hyperparameters, as well as the untunable scale_pos_weight parameter adjusted to reflect the proportion of attack sessions, we see some detection with poor precision. At this point, I'm not sure how else to improve the approach; the low proportion of attack data may make it difficult to infer consistent distinctions in the model.\n",
    "\n",
    "# Final attempt:\n",
    "accuracy: 0.9896171317326411 precision: 0.011363636362345043 recall 0.0999999999\n",
    "\n",
    "|tp|fp|tn|fn|len(comp)|\n",
    "|--|--|--|--|---------|\n",
    "|1| 87| 9149| 9|9246|\n",
    "\n",
    "Building on the tuned hyperparameter version, with all x columns scaled into the range 0..1. Given that the feature columns come from two different domains- transaction counts and session timings - the value ranges are quite different. Maximum session durations range into the hundreds, while session sigmas are much smaller, of course. \n",
    "However, the model fared even worse after normalization. It may be that the existing proportions overweight one set of data beneficially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further investigating the data \n",
    "\n",
    "We had additional ideas for modeling the data while staying in this bag-of-transaction technique.\n",
    "1. Try some hyperparameter tuning to seem if the success rates can be trivially improved.\n",
    "1. Enrich the training data set in various ways - add colums to summarize total session time, average time/request, and so on.\n",
    "1. Perform some clustering analysis to try to identify common patterns of behavior other than LT. This may reveal the presence of other kinds of harvesting.\n",
    "\n",
    "## Qualifying the approach\n",
    "Can we use this approach to identify and blacklist harvesting sessions as they occur? Some notes:\n",
    "1. The approach must be resilient to easy efforts to evade. Does the accuracy of the identification drop if the attacker makes minor changes to his workflow?\n",
    "1. How long does it take to identify an attacker in real time? \n",
    "    1. Do we gain certainty soon enough to stop an attacker before he's done what he came to do?\n",
    "    2. Can we tag sessions accurately after the first N log entries, for instance?\n",
    "    \n",
    "## Designing an implementation\n",
    "Design an architecture for identifying and intercepting harvesting activity in real time. Confirm data sources, manage impact to usage latency, model costs and ROI.\n",
    "\n",
    "In today's world, it would be less effective to perform real-time analysis on AWS, since all of our current content usage is on-prem. The algorithm used here, XGBoost, is performant on commodity hardware, so we may be able to run on standard VMs.\n",
    "\n",
    "In real-time analysis, we will face a stream of events from interleaved sessions. We will have to demultiplex these into individual event streams both for training and for prediction, implying some kind of windowing to capture and send sets of log entries as partial sessions. It's not clear how big the impact of this windowing will be on the accuracy of the models.\n",
    "\n",
    "# Other analytical techniques\n",
    "While this algorithm seems promising, we're throwing away a huge amount of intelligence before we start training, in the name of simplicity. We can evaluate what kind of gains we could achieve through more advanced techniques:\n",
    "- Stateful models like LSTM or CNN\n",
    "- more \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
